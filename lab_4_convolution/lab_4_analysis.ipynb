{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from neunet.neunet import NeuralNet\n",
    "from neunet.layer import *\n",
    "from analysis.classification_error_analyser import ClassificationErrorAnalyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def prepare_df(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n",
    "    df[columns] = df[columns].apply(lambda s: s.apply(lambda x: x / 255))\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_target_to_vector(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n",
    "    target = df[column_name]\n",
    "    new_target = np.zeros((len(df.index), 10))\n",
    "    for label, val in target.items():\n",
    "        new_target[label, int(val)] = 1.0\n",
    "    new_target_df = pd.DataFrame(new_target, columns=[\"is_0\", \"is_1\", \"is_2\", \"is_3\", \"is_4\", \"is_5\", \"is_6\", \"is_7\", \"is_8\", \"is_9\"])\n",
    "    df = df.drop([column_name], axis=1)\n",
    "    return df.join(new_target_df)\n",
    "\n",
    "def show_plots_classification(analyser: ClassificationErrorAnalyser, msg: str):\n",
    "    recall = analyser.recall()\n",
    "    precision = analyser.precision()\n",
    "    accuracy = analyser.accuracy()\n",
    "    f_score = analyser.f_score()\n",
    "\n",
    "    border = 0.5\n",
    "    ind = analyser.get_index_by_border(border)\n",
    "\n",
    "    print(f\"\\n{msg}\\n border = {border}\\n recall = {recall[ind]}\\n precision = {precision[ind]}\\n \"\n",
    "          f\"accuracy = {accuracy[ind]}\\n F-score = {f_score[ind]}\\n\")\n",
    "\n",
    "    tpr = analyser.tpr()\n",
    "    fpr = analyser.fpr()\n",
    "    fnr = analyser.fnr()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    plt.title('test', fontsize=15)\n",
    "    axs[0].plot(fpr, fpr, fpr, tpr)\n",
    "    axs[0].grid(True)\n",
    "    axs[0].set_title(label='ROC', fontsize=10)\n",
    "    axs[1].plot(fpr, fpr, fpr, fnr)\n",
    "    axs[1].grid(True)\n",
    "    axs[1].set_title(label='DET', fontsize=10)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "dataset_train_path = \"MNIST_dataset/mnist_train.csv\"\n",
    "dataset_test_path = \"MNIST_dataset/mnist_test.csv\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "       1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  1x10  ...  is_0  is_1  \\\n0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n1      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   1.0   0.0   \n2      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n3      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   1.0   \n4      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n...    ...  ...  ...  ...  ...  ...  ...  ...  ...   ...  ...   ...   ...   \n59995  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n59996  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n59997  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n59998  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n59999  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n\n       is_2  is_3  is_4  is_5  is_6  is_7  is_8  is_9  \n0       0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0  \n1       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n2       0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0  \n3       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n4       0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0  \n...     ...   ...   ...   ...   ...   ...   ...   ...  \n59995   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0  \n59996   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  \n59997   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0  \n59998   0.0   0.0   0.0   0.0   1.0   0.0   0.0   0.0  \n59999   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0  \n\n[60000 rows x 794 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1x1</th>\n      <th>1x2</th>\n      <th>1x3</th>\n      <th>1x4</th>\n      <th>1x5</th>\n      <th>1x6</th>\n      <th>1x7</th>\n      <th>1x8</th>\n      <th>1x9</th>\n      <th>1x10</th>\n      <th>...</th>\n      <th>is_0</th>\n      <th>is_1</th>\n      <th>is_2</th>\n      <th>is_3</th>\n      <th>is_4</th>\n      <th>is_5</th>\n      <th>is_6</th>\n      <th>is_7</th>\n      <th>is_8</th>\n      <th>is_9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>59995</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>59996</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>59997</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>59998</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>59999</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>60000 rows × 794 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = pd.read_csv(dataset_train_path)\n",
    "learn[learn.columns] = learn[learn.columns].astype(float)\n",
    "columns_list = list(learn.columns[1:])\n",
    "columns_list.append(learn.columns[0])\n",
    "learn = learn[columns_list]\n",
    "learn = prepare_df(learn, columns_list[:-1])\n",
    "learn = convert_target_to_vector(learn, learn.columns[-1])\n",
    "learn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "      1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  1x10  ...  is_0  is_1  \\\n0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   1.0   \n3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   1.0   0.0   \n4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n...   ...  ...  ...  ...  ...  ...  ...  ...  ...   ...  ...   ...   ...   \n9995  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n9996  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n9997  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n9998  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n9999  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n\n      is_2  is_3  is_4  is_5  is_6  is_7  is_8  is_9  \n0      0.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0  \n1      1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n2      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n3      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n4      0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0  \n...    ...   ...   ...   ...   ...   ...   ...   ...  \n9995   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n9996   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  \n9997   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0  \n9998   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0  \n9999   0.0   0.0   0.0   0.0   1.0   0.0   0.0   0.0  \n\n[10000 rows x 794 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1x1</th>\n      <th>1x2</th>\n      <th>1x3</th>\n      <th>1x4</th>\n      <th>1x5</th>\n      <th>1x6</th>\n      <th>1x7</th>\n      <th>1x8</th>\n      <th>1x9</th>\n      <th>1x10</th>\n      <th>...</th>\n      <th>is_0</th>\n      <th>is_1</th>\n      <th>is_2</th>\n      <th>is_3</th>\n      <th>is_4</th>\n      <th>is_5</th>\n      <th>is_6</th>\n      <th>is_7</th>\n      <th>is_8</th>\n      <th>is_9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9995</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>9996</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>9997</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>9998</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>9999</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows × 794 columns</p>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(dataset_test_path)\n",
    "test = test[columns_list]\n",
    "test = prepare_df(test, columns_list[:-1])\n",
    "test = convert_target_to_vector(test, test.columns[-1])\n",
    "test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "hidden_1 = 120\n",
    "hidden_2 = 84\n",
    "#nn = NeuralNet(dir_name='./classification_weights')\n",
    "#\"\"\"\n",
    "nn = NeuralNet(learn_rate=0.0005,  layers=[\n",
    "    ConvLayer((1, 28, 28), dims_filter=(1, 3, 3), num_filters=6, start_weight_multiplier=0.1,\n",
    "              funcs=[\"ReLu\"] * 2304, deviation=0.0),\n",
    "    PoolLayer(\"avg\", (6, 26, 26)),\n",
    "    ConvLayer((6, 13, 13), dims_filter=(6, 4, 4), num_filters=16, start_weight_multiplier=0.1,\n",
    "              funcs=[\"ReLu\"] * 576, deviation=0.0),\n",
    "    PoolLayer(\"avg\", (16, 10, 10)),\n",
    "    DenseLayer(dims=(hidden_1, 400), funcs=[\"sigmoid\"] * hidden_1, deviation=0.5),\n",
    "    DenseLayer(dims=(hidden_2, hidden_1), funcs=[\"sigmoid\"] * hidden_2, deviation=0.5),\n",
    "    DenseLayer(dims=(10, hidden_2), funcs=[\"sigmoid\"], deviation=0.5)\n",
    "])\n",
    "#\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.14671704  0.15321782 -0.2131115   0.17305529 -0.01045012  0.26758282\n",
      " -0.03359595  0.18320523  0.04911209  0.13405561], error = [ 0.85328296 -0.15321782  0.2131115  -0.17305529  0.01045012 -0.26758282\n",
      "  0.03359595 -0.18320523 -0.04911209 -0.13405561]\n",
      "learn time: 63.17895293235779\n",
      "2000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.11677338  0.26763104 -0.32678544  0.26886723 -0.20366316 -0.08316497\n",
      "  0.02828858  0.03773896 -0.29887338  0.21023212], error = [-0.11677338 -0.26763104  0.32678544 -0.26886723  0.20366316  1.08316497\n",
      " -0.02828858 -0.03773896  0.29887338 -0.21023212]\n",
      "learn time: 125.96600413322449\n",
      "3000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.11988384 -0.07443383  0.20534822  0.19407816  0.11182989  0.01801515\n",
      "  0.06593298  0.01282299  0.18711563  0.12226033], error = [-0.11988384  0.07443383 -0.20534822 -0.19407816 -0.11182989 -0.01801515\n",
      " -0.06593298 -0.01282299 -0.18711563  0.87773967]\n",
      "learn time: 188.50992465019226\n",
      "4000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [0.11127918 0.16090086 0.05210131 0.08756572 0.2149228  0.18387135\n",
      " 0.04096289 0.24721746 0.08100136 0.11331494], error = [-0.11127918 -0.16090086 -0.05210131 -0.08756572 -0.2149228  -0.18387135\n",
      " -0.04096289  0.75278254 -0.08100136 -0.11331494]\n",
      "learn time: 251.11340498924255\n",
      "5000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.09588611 -0.04789161  0.34172143  0.10197645  0.14205734  0.02730984\n",
      "  0.15359033  0.15631143  0.1791635   0.06789702], error = [-0.09588611  0.04789161 -0.34172143 -0.10197645 -0.14205734 -0.02730984\n",
      " -0.15359033  0.84368857 -0.1791635  -0.06789702]\n",
      "learn time: 313.7482373714447\n",
      "6000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.1026596  -0.00932432  0.08352921  0.15099825  0.06175742  0.10110297\n",
      "  0.31035476 -0.08869302  0.03507819  0.09034219], error = [-0.1026596   0.00932432 -0.08352921 -0.15099825 -0.06175742 -0.10110297\n",
      "  0.68964524  0.08869302 -0.03507819 -0.09034219]\n",
      "learn time: 376.32734656333923\n",
      "7000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.11255203  0.11261239 -0.02090861  0.12247277 -0.01259957  0.10850071\n",
      "  0.15905629  0.04468328  0.05049759  0.11913129], error = [-0.11255203 -0.11261239  0.02090861 -0.12247277  0.01259957 -0.10850071\n",
      " -0.15905629 -0.04468328  0.94950241 -0.11913129]\n",
      "learn time: 438.9330596923828\n",
      "8000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.10107264 -0.07614123  0.0170968   0.15350298  0.20628801  0.09958331\n",
      "  0.1443027   0.00427654  0.17885152 -0.02948106], error = [ 0.89892736  0.07614123 -0.0170968  -0.15350298 -0.20628801 -0.09958331\n",
      " -0.1443027  -0.00427654 -0.17885152  0.02948106]\n",
      "learn time: 501.53014731407166\n",
      "9000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.08849611 -0.04075661  0.3859417   0.06976195  0.49189492  0.09021422\n",
      "  0.36250157 -0.23619488  0.02599442  0.01272823], error = [-0.08849611  0.04075661 -0.3859417  -0.06976195 -0.49189492 -0.09021422\n",
      "  0.63749843  0.23619488 -0.02599442 -0.01272823]\n",
      "learn time: 564.2106919288635\n",
      "10000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.10157372  0.10373105  0.12891906  0.30177568 -0.03401677  0.1798734\n",
      "  0.0177056   0.23578561  0.0925076  -0.00420735], error = [-0.10157372 -0.10373105 -0.12891906  0.69822432  0.03401677 -0.1798734\n",
      " -0.0177056  -0.23578561 -0.0925076   0.00420735]\n",
      "learn time: 627.2695634365082\n",
      "11000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.08065719  0.04611866 -0.02666917  0.02385937  0.30663756  0.03841364\n",
      "  0.53402859  0.03866861  0.11504119  0.17201273], error = [-0.08065719 -0.04611866  0.02666917 -0.02385937 -0.30663756 -0.03841364\n",
      "  0.46597141 -0.03866861 -0.11504119 -0.17201273]\n",
      "learn time: 690.2229044437408\n",
      "12000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.07072708 -0.01732524  0.11828661  0.19050945  0.03116738  0.11087493\n",
      " -0.17936096  0.33643997 -0.09972949 -0.0315779 ], error = [-0.07072708  0.01732524 -0.11828661 -0.19050945 -0.03116738 -0.11087493\n",
      "  0.17936096  0.66356003  0.09972949  0.0315779 ]\n",
      "learn time: 752.6461246013641\n",
      "13000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.12129616  0.23072369 -0.21119288 -0.08676557  0.13233558  0.07483002\n",
      " -0.06353589  0.44601989 -0.02562475  0.3097332 ], error = [-0.12129616 -0.23072369  0.21119288  0.08676557 -0.13233558 -0.07483002\n",
      "  0.06353589  0.55398011  0.02562475 -0.3097332 ]\n",
      "learn time: 815.0382242202759\n",
      "14000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.12209183  0.38530433  0.01263043 -0.03099955  0.06591547  0.12775029\n",
      "  0.08663746  0.14632787  0.00060513  0.27595897], error = [-1.22091830e-01  6.14695674e-01 -1.26304292e-02  3.09995460e-02\n",
      " -6.59154670e-02 -1.27750291e-01 -8.66374637e-02 -1.46327865e-01\n",
      " -6.05133336e-04 -2.75958965e-01]\n",
      "learn time: 877.3049569129944\n",
      "15000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [0.10413571 0.00757204 0.07008862 0.12993063 0.03839359 0.14936255\n",
      " 0.17997282 0.01006803 0.09500731 0.25951576], error = [-0.10413571 -0.00757204 -0.07008862 -0.12993063 -0.03839359  0.85063745\n",
      " -0.17997282 -0.01006803 -0.09500731 -0.25951576]\n",
      "learn time: 939.7396860122681\n",
      "16000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [0.10058793 0.16524825 0.0295872  0.03676748 0.04623906 0.15324703\n",
      " 0.11626831 0.05386271 0.32081807 0.03509389], error = [-0.10058793 -0.16524825 -0.0295872  -0.03676748 -0.04623906 -0.15324703\n",
      " -0.11626831 -0.05386271  0.67918193 -0.03509389]\n",
      "learn time: 1002.3009259700775\n",
      "17000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.11177398 -0.09408754  0.32023943  0.24909181 -0.06479191  0.12209431\n",
      "  0.16122758 -0.01063238  0.04487408 -0.02882554], error = [-0.11177398  0.09408754 -0.32023943  0.75090819  0.06479191 -0.12209431\n",
      " -0.16122758  0.01063238 -0.04487408  0.02882554]\n",
      "learn time: 1064.6269037723541\n",
      "18000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.11167421 -0.12371073  0.05668334  0.03808146  0.38712852 -0.17202572\n",
      "  0.25812557  0.01930795  0.03610593  0.29314726], error = [-0.11167421  0.12371073 -0.05668334 -0.03808146  0.61287148  0.17202572\n",
      " -0.25812557 -0.01930795 -0.03610593 -0.29314726]\n",
      "learn time: 1126.9241416454315\n",
      "19000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [0.08753024 0.24047484 0.0797046  0.07874076 0.09094472 0.04393389\n",
      " 0.09424327 0.08791274 0.30289032 0.01980452], error = [-0.08753024 -0.24047484 -0.0797046  -0.07874076 -0.09094472 -0.04393389\n",
      " -0.09424327 -0.08791274  0.69710968 -0.01980452]\n",
      "learn time: 1189.4454333782196\n",
      "20000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.11106205 -0.09197969 -0.07760022  0.09899753  0.05532118  0.3543326\n",
      "  0.10306003 -0.1653738   0.27142869  0.07379514], error = [-0.11106205  0.09197969  0.07760022 -0.09899753 -0.05532118  0.6456674\n",
      " -0.10306003  0.1653738  -0.27142869 -0.07379514]\n",
      "learn time: 1251.7697308063507\n",
      "21000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.11650265  0.19356074 -0.09578303  0.25608839  0.10097046  0.0298923\n",
      " -0.00462642  0.4163932   0.07157086  0.29130468], error = [-0.11650265 -0.19356074  0.09578303 -0.25608839 -0.10097046 -0.0298923\n",
      "  0.00462642  0.5836068  -0.07157086 -0.29130468]\n",
      "learn time: 1314.0496282577515\n",
      "22000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.1177115   0.43631746  0.09847255  0.17672339  0.04069977  0.16696938\n",
      " -0.0650462   0.1220885  -0.10966783  0.10289997], error = [-0.1177115  -0.43631746 -0.09847255  0.82327661 -0.04069977 -0.16696938\n",
      "  0.0650462  -0.1220885   0.10966783 -0.10289997]\n",
      "learn time: 1376.4526906013489\n",
      "23000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.08217425 -0.08023781 -0.110331    0.07173085  0.18544509  0.06670077\n",
      " -0.22292306  0.71631956 -0.01013229  0.07734146], error = [-0.08217425  0.08023781  0.110331   -0.07173085 -0.18544509 -0.06670077\n",
      "  0.22292306  0.28368044  0.01013229 -0.07734146]\n",
      "learn time: 1438.9294707775116\n",
      "24000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.09074954  0.11862884  0.17327287  0.18378254 -0.05830637  0.14214329\n",
      "  0.01571547  0.11151606  0.33337392 -0.06185819], error = [-0.09074954 -0.11862884 -0.17327287 -0.18378254  0.05830637 -0.14214329\n",
      " -0.01571547 -0.11151606  0.66662608  0.06185819]\n",
      "learn time: 1501.3470387458801\n",
      "25000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.09015388  0.18684811  0.10942285  0.2097323   0.08515473 -0.02022599\n",
      "  0.04480543  0.23392601  0.02428447  0.15617539], error = [-0.09015388 -0.18684811 -0.10942285  0.7902677  -0.08515473  0.02022599\n",
      " -0.04480543 -0.23392601 -0.02428447 -0.15617539]\n",
      "learn time: 1563.9770209789276\n",
      "26000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.09851141  0.1937105  -0.00708072 -0.00437312  0.37015674  0.03027791\n",
      "  0.1975821   0.09501954 -0.05018955  0.16155507], error = [-0.09851141 -0.1937105   0.00708072  0.00437312  0.62984326 -0.03027791\n",
      " -0.1975821  -0.09501954  0.05018955 -0.16155507]\n",
      "learn time: 1626.3940379619598\n",
      "27000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.11553817 -0.08816925  0.11693599  0.18172395  0.17689343  0.20489227\n",
      "  0.16021435 -0.18491286  0.0972372   0.09652445], error = [-0.11553817  0.08816925 -0.11693599 -0.18172395 -0.17689343  0.79510773\n",
      " -0.16021435  0.18491286 -0.0972372  -0.09652445]\n",
      "learn time: 1689.4447939395905\n",
      "28000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.10125648  0.48614824 -0.02930289 -0.04163239  0.01369003  0.13925143\n",
      "  0.16855746  0.13422299 -0.03751338  0.14089804], error = [-0.10125648  0.51385176  0.02930289  0.04163239 -0.01369003 -0.13925143\n",
      " -0.16855746 -0.13422299  0.03751338 -0.14089804]\n",
      "learn time: 1751.8638212680817\n",
      "29000: expected = [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.09342761  0.16119051  0.42179313  0.21429147 -0.00219373  0.0261977\n",
      "  0.21241025 -0.0416135  -0.02949244 -0.0271367 ], error = [-0.09342761 -0.16119051  0.57820687 -0.21429147  0.00219373 -0.0261977\n",
      " -0.21241025  0.0416135   0.02949244  0.0271367 ]\n",
      "learn time: 1814.2246754169464\n",
      "30000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.09343842  0.03208082  0.16756475  0.44013126 -0.11189045  0.19523584\n",
      "  0.01418257  0.20465138  0.01073657  0.02570783], error = [-0.09343842 -0.03208082 -0.16756475  0.55986874  0.11189045 -0.19523584\n",
      " -0.01418257 -0.20465138 -0.01073657 -0.02570783]\n",
      "learn time: 1876.7671415805817\n",
      "31000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.10018379 -0.01555107  0.19866742 -0.03442717  0.13543115  0.37397651\n",
      "  0.23892325 -0.3445661   0.14645438 -0.06496051], error = [-0.10018379  0.01555107 -0.19866742  0.03442717 -0.13543115 -0.37397651\n",
      "  0.76107675  0.3445661  -0.14645438  0.06496051]\n",
      "learn time: 1939.357881307602\n",
      "32000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.08133491  0.15940806  0.22534684  0.05987562  0.11786194  0.11995217\n",
      "  0.06648293  0.04891737  0.20420402 -0.11979151], error = [-0.08133491 -0.15940806 -0.22534684 -0.05987562 -0.11786194 -0.11995217\n",
      " -0.06648293 -0.04891737  0.79579598  0.11979151]\n",
      "learn time: 2002.0969769954681\n",
      "33000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.09834475  0.178652    0.02414488  0.22576493 -0.04597947  0.20465393\n",
      "  0.07801068  0.11661261  0.06924559  0.14237986], error = [-0.09834475 -0.178652   -0.02414488  0.77423507  0.04597947 -0.20465393\n",
      " -0.07801068 -0.11661261 -0.06924559 -0.14237986]\n",
      "learn time: 2064.6242389678955\n",
      "34000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.15085781 -0.00779757 -0.0086455   0.1339999   0.39861964 -0.00392414\n",
      "  0.16579134 -0.05319019 -0.02077287  0.48245132], error = [-0.15085781  0.00779757  0.0086455  -0.1339999   0.60138036  0.00392414\n",
      " -0.16579134  0.05319019  0.02077287 -0.48245132]\n",
      "learn time: 2127.160326242447\n",
      "35000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.11003023  0.43654703  0.08220793  0.01471208  0.0065005   0.08577802\n",
      "  0.07425798  0.15567979 -0.02040518  0.11690878], error = [-0.11003023  0.56345297 -0.08220793 -0.01471208 -0.0065005  -0.08577802\n",
      " -0.07425798 -0.15567979  0.02040518 -0.11690878]\n",
      "learn time: 2189.485470056534\n",
      "36000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.13285623  0.25665885  0.02968385 -0.01807566  0.23034413 -0.02359295\n",
      "  0.07905092  0.01176013  0.36044398  0.26950347], error = [-0.13285623 -0.25665885 -0.02968385  0.01807566 -0.23034413  0.02359295\n",
      " -0.07905092 -0.01176013 -0.36044398  0.73049653]\n",
      "learn time: 2251.910038471222\n",
      "37000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.10191631 -0.13995143  0.25967991  0.0451516   0.28993591 -0.03210968\n",
      "  0.34523982 -0.19908171 -0.094617    0.11607003], error = [-0.10191631  0.13995143 -0.25967991 -0.0451516   0.71006409  0.03210968\n",
      " -0.34523982  0.19908171  0.094617   -0.11607003]\n",
      "learn time: 2314.320048570633\n",
      "38000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [0.0907847  0.1822727  0.02735895 0.08838588 0.02610239 0.04415117\n",
      " 0.03402151 0.21535314 0.12008231 0.06656113], error = [-0.0907847  -0.1822727  -0.02735895 -0.08838588 -0.02610239 -0.04415117\n",
      " -0.03402151 -0.21535314  0.87991769 -0.06656113]\n",
      "learn time: 2376.839603662491\n",
      "39000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.0935988  -0.01391636 -0.02688664  0.12626112  0.39054414  0.05607018\n",
      "  0.10633904  0.15484306 -0.02465544  0.2834245 ], error = [-0.0935988   0.01391636  0.02688664 -0.12626112 -0.39054414 -0.05607018\n",
      " -0.10633904 -0.15484306  0.02465544  0.7165755 ]\n",
      "learn time: 2439.4477248191833\n",
      "40000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.10704116  0.18682404 -0.05210943  0.09432436 -0.0612188  -0.02206599\n",
      " -0.19974046  0.39078413  0.25406396  0.23175754], error = [-0.10704116 -0.18682404  0.05210943 -0.09432436  0.0612188   0.02206599\n",
      "  0.19974046  0.60921587 -0.25406396 -0.23175754]\n",
      "learn time: 2501.765783548355\n",
      "41000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.09285821  0.03489171 -0.1900178  -0.06596797  0.33483078  0.19735597\n",
      "  0.08269361  0.06244272  0.06640643  0.1694767 ], error = [-0.09285821 -0.03489171  0.1900178   0.06596797 -0.33483078  0.80264403\n",
      " -0.08269361 -0.06244272 -0.06640643 -0.1694767 ]\n",
      "learn time: 2564.325109243393\n",
      "42000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.12328965  0.68168667  0.04172418 -0.01704844  0.04963478  0.06445783\n",
      "  0.05011192  0.13451038  0.03635493  0.13975797], error = [-0.12328965  0.31831333 -0.04172418  0.01704844 -0.04963478 -0.06445783\n",
      " -0.05011192 -0.13451038 -0.03635493 -0.13975797]\n",
      "learn time: 2626.774178504944\n",
      "43000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.12401106 -0.05937846 -0.12731321  0.05674026  0.11925966  0.1018309\n",
      " -0.10918848  0.17093075  0.12525907  0.37454882], error = [-0.12401106  0.05937846  0.12731321 -0.05674026 -0.11925966 -0.1018309\n",
      "  0.10918848 -0.17093075 -0.12525907  0.62545118]\n",
      "learn time: 2689.1957466602325\n",
      "44000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.14602817 -0.10039432 -0.05492123  0.31926087 -0.09410026  0.17044308\n",
      "  0.01969485 -0.03083468  0.06438125  0.06206042], error = [ 0.85397183  0.10039432  0.05492123 -0.31926087  0.09410026 -0.17044308\n",
      " -0.01969485  0.03083468 -0.06438125 -0.06206042]\n",
      "learn time: 2751.4869301319122\n",
      "45000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.10306107  0.08708508  0.07273338  0.32969594 -0.10708016  0.19061894\n",
      "  0.16290159 -0.00309774  0.16170367  0.12372511], error = [-0.10306107 -0.08708508 -0.07273338  0.67030406  0.10708016 -0.19061894\n",
      " -0.16290159  0.00309774 -0.16170367 -0.12372511]\n",
      "learn time: 2813.7214617729187\n",
      "46000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.13097472  0.12863391 -0.04602537  0.03441037  0.1907135   0.04359767\n",
      "  0.08825433  0.00536238  0.17184997  0.43920214], error = [-0.13097472 -0.12863391  0.04602537 -0.03441037 -0.1907135  -0.04359767\n",
      " -0.08825433 -0.00536238 -0.17184997  0.56079786]\n",
      "learn time: 2876.0486364364624\n",
      "47000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.12545644 -0.05888027 -0.01751076  0.2047501  -0.07278802  0.13564945\n",
      " -0.00720556  0.12933037 -0.07636664  0.1286535 ], error = [ 0.87454356  0.05888027  0.01751076 -0.2047501   0.07278802 -0.13564945\n",
      "  0.00720556 -0.12933037  0.07636664 -0.1286535 ]\n",
      "learn time: 2938.4339356422424\n",
      "48000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.09415514  0.14360271 -0.00628796  0.04572402  0.32944763  0.21569179\n",
      "  0.11693531  0.0561032  -0.0427537   0.09072983], error = [-0.09415514 -0.14360271  0.00628796 -0.04572402  0.67055237 -0.21569179\n",
      " -0.11693531 -0.0561032   0.0427537  -0.09072983]\n",
      "learn time: 3000.81498003006\n",
      "49000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.08487798 -0.20255478 -0.01613189 -0.04539406  0.50385682  0.08531617\n",
      "  0.16064863  0.20156339  0.11108986  0.25679059], error = [-0.08487798  0.20255478  0.01613189  0.04539406  0.49614318 -0.08531617\n",
      " -0.16064863 -0.20156339 -0.11108986 -0.25679059]\n",
      "learn time: 3063.2366845607758\n",
      "50000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.08355096  0.16399144  0.20480762  0.23173645 -0.00885564  0.1668499\n",
      "  0.08863717 -0.04394396  0.26034209 -0.2418043 ], error = [-0.08355096 -0.16399144 -0.20480762  0.76826355  0.00885564 -0.1668499\n",
      " -0.08863717  0.04394396 -0.26034209  0.2418043 ]\n",
      "learn time: 3125.663518190384\n",
      "51000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.12916904 -0.02975768 -0.2728934   0.120452    0.13246908  0.03760271\n",
      " -0.10137917  0.7068845   0.11259841  0.41135411], error = [-0.12916904  0.02975768  0.2728934  -0.120452   -0.13246908 -0.03760271\n",
      "  0.10137917  0.2931155  -0.11259841 -0.41135411]\n",
      "learn time: 3187.99773311615\n",
      "52000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.08359712  0.22592677 -0.00227347  0.02195385  0.05431831  0.14300885\n",
      "  0.5610564  -0.06091368  0.00123749  0.02540785], error = [-0.08359712 -0.22592677  0.00227347 -0.02195385 -0.05431831 -0.14300885\n",
      "  0.4389436   0.06091368 -0.00123749 -0.02540785]\n",
      "learn time: 3250.40629863739\n",
      "53000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.10750306 -0.05803399  0.06029993 -0.00177367 -0.05844638  0.37646099\n",
      "  0.16545733 -0.06723623  0.34513899 -0.15505338], error = [-0.10750306  0.05803399 -0.06029993  0.00177367  0.05844638  0.62353901\n",
      " -0.16545733  0.06723623 -0.34513899  0.15505338]\n",
      "learn time: 3312.774416208267\n",
      "54000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.11116218 -0.00599968  0.27373717  0.3275523   0.26327223  0.36458072\n",
      "  0.12074942  0.02714063  0.25897309  0.02027874], error = [-0.11116218  0.00599968 -0.27373717 -0.3275523  -0.26327223  0.63541928\n",
      " -0.12074942 -0.02714063 -0.25897309 -0.02027874]\n",
      "learn time: 3375.2978031635284\n",
      "55000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.12046912  0.58023908  0.11877978  0.09640667  0.07219214  0.02867162\n",
      " -0.05212493  0.17208056  0.09521096  0.03895611], error = [-0.12046912  0.41976092 -0.11877978 -0.09640667 -0.07219214 -0.02867162\n",
      "  0.05212493 -0.17208056 -0.09521096 -0.03895611]\n",
      "learn time: 3437.6213002204895\n",
      "56000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.12771257  0.67196183  0.0999164   0.06586996  0.13210942  0.03974145\n",
      " -0.07876638  0.20917975  0.04608947  0.05759892], error = [-0.12771257  0.32803817 -0.0999164  -0.06586996 -0.13210942 -0.03974145\n",
      "  0.07876638 -0.20917975 -0.04608947 -0.05759892]\n",
      "learn time: 3500.0871410369873\n",
      "57000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.14996082  0.22079476  0.24342613 -0.157194   -0.02310077  0.03472973\n",
      " -0.12794042  0.10023837 -0.06860367  0.07937276], error = [ 0.85003918 -0.22079476 -0.24342613  0.157194    0.02310077 -0.03472973\n",
      "  0.12794042 -0.10023837  0.06860367 -0.07937276]\n",
      "learn time: 3562.596878051758\n",
      "58000: expected = [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.10776159 -0.02501967  0.48596989  0.26345449 -0.11064921 -0.11934272\n",
      " -0.08260393  0.17967617  0.15583041 -0.11305015], error = [-0.10776159  0.02501967  0.51403011 -0.26345449  0.11064921  0.11934272\n",
      "  0.08260393 -0.17967617 -0.15583041  0.11305015]\n",
      "learn time: 3625.012909412384\n",
      "59000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.07697577  0.03567361 -0.04449213 -0.03702443  0.09678314  0.05221424\n",
      "  0.82409442  0.02530266  0.02405403  0.04640653], error = [-0.07697577 -0.03567361  0.04449213  0.03702443 -0.09678314 -0.05221424\n",
      "  0.17590558 -0.02530266 -0.02405403 -0.04640653]\n",
      "learn time: 3687.30859708786\n",
      "60000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.12143654 -0.01475235  0.21171126  0.34865917 -0.26985726  0.3345352\n",
      " -0.01358681  0.1487539   0.18991636 -0.09139173], error = [-0.12143654  0.01475235 -0.21171126 -0.34865917  0.26985726  0.6654648\n",
      "  0.01358681 -0.1487539  -0.18991636  0.09139173]\n",
      "learn time: 3749.759524345398\n",
      "61000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.13048233 -0.03342817  0.00071181  0.14421464 -0.02532274  0.19523333\n",
      "  0.12763885  0.00093387  0.09549824  0.00896868], error = [ 8.69517675e-01  3.34281744e-02 -7.11809969e-04 -1.44214640e-01\n",
      "  2.53227430e-02 -1.95233328e-01 -1.27638853e-01 -9.33866537e-04\n",
      " -9.54982408e-02 -8.96867793e-03]\n",
      "learn time: 3812.1613738536835\n",
      "62000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.12000628 -0.02144754  0.08075998 -0.01508945 -0.080894    0.29370071\n",
      "  0.3932112  -0.28201141  0.06972068  0.01171631], error = [-0.12000628  0.02144754 -0.08075998  0.01508945  0.080894    0.70629929\n",
      " -0.3932112   0.28201141 -0.06972068 -0.01171631]\n",
      "learn time: 3874.7159485816956\n",
      "63000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.14082716 -0.11617598  0.13054983  0.00284082  0.33825907 -0.012304\n",
      "  0.13726081  0.12784985 -0.00287549  0.08190047], error = [-0.14082716  0.11617598 -0.13054983 -0.00284082 -0.33825907  0.012304\n",
      " -0.13726081 -0.12784985  0.00287549  0.91809953]\n",
      "learn time: 3937.208018541336\n",
      "64000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.10157836  0.07330626  0.21786869  0.21851892 -0.17596346 -0.13654633\n",
      " -0.13437522  0.56887192  0.07316159  0.10265429], error = [-0.10157836 -0.07330626 -0.21786869 -0.21851892  0.17596346  0.13654633\n",
      "  0.13437522  0.43112808 -0.07316159 -0.10265429]\n",
      "learn time: 3999.5588369369507\n",
      "65000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.09751834 -0.07469506  0.07574722  0.18659854  0.06085963 -0.16780995\n",
      "  0.17391846  0.40241794  0.0599677   0.1198415 ], error = [-0.09751834  0.07469506 -0.07574722 -0.18659854 -0.06085963  0.16780995\n",
      " -0.17391846  0.59758206 -0.0599677  -0.1198415 ]\n",
      "learn time: 4062.007968187332\n",
      "66000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.10214358 -0.06691277  0.01654581  0.11921618  0.09634132  0.15418729\n",
      "  0.75286858 -0.2289787   0.03118201  0.05294213], error = [-0.10214358  0.06691277 -0.01654581 -0.11921618 -0.09634132 -0.15418729\n",
      "  0.24713142  0.2289787  -0.03118201 -0.05294213]\n",
      "learn time: 4124.45276761055\n",
      "67000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.10377795  0.07958774  0.02890245  0.20033729 -0.04646684  0.20816525\n",
      "  0.12646172  0.00290305  0.16483838  0.16296388], error = [-0.10377795 -0.07958774 -0.02890245 -0.20033729  0.04646684 -0.20816525\n",
      " -0.12646172 -0.00290305  0.83516162 -0.16296388]\n",
      "learn time: 4186.925538778305\n",
      "68000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.18584836 -0.05216739  0.05689304  0.02638903 -0.03443966  0.08845669\n",
      "  0.07622127  0.02493451  0.05933221  0.12593764], error = [ 0.81415164  0.05216739 -0.05689304 -0.02638903  0.03443966 -0.08845669\n",
      " -0.07622127 -0.02493451 -0.05933221 -0.12593764]\n",
      "learn time: 4249.53396320343\n",
      "69000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.11049548 -0.14647155  0.30488251 -0.11384346  0.18761176 -0.01947571\n",
      "  0.52323723  0.08241929  0.12109856  0.02514545], error = [-0.11049548  0.14647155 -0.30488251  0.11384346 -0.18761176  0.01947571\n",
      "  0.47676277 -0.08241929 -0.12109856 -0.02514545]\n",
      "learn time: 4311.889089107513\n",
      "70000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.11925038  0.01580924  0.19381945  0.77668799 -0.13846644  0.13394547\n",
      "  0.08530836  0.14354995  0.04088254 -0.02220296], error = [-0.11925038 -0.01580924 -0.19381945  0.22331201  0.13846644 -0.13394547\n",
      " -0.08530836 -0.14354995 -0.04088254  0.02220296]\n",
      "learn time: 4374.242064476013\n",
      "71000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.0790666   0.00899987  0.07318321 -0.08408354  0.16613616  0.01489306\n",
      "  0.89549365  0.00692988  0.14113662  0.13208343], error = [-0.0790666  -0.00899987 -0.07318321  0.08408354 -0.16613616 -0.01489306\n",
      "  0.10450635 -0.00692988 -0.14113662 -0.13208343]\n",
      "learn time: 4436.639906167984\n",
      "72000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.09764483 -0.12548525  0.26874354  0.12817625  0.00497191 -0.08081513\n",
      " -0.07392908  0.51344774 -0.02791157  0.03292304], error = [-0.09764483  0.12548525 -0.26874354 -0.12817625 -0.00497191  0.08081513\n",
      "  0.07392908  0.48655226  0.02791157 -0.03292304]\n",
      "learn time: 4499.10981464386\n",
      "73000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.13675008  0.21543462 -0.31772728 -0.15274253  0.18554952  0.14604052\n",
      " -0.10849174  0.59426339 -0.07094603  0.23422557], error = [-0.13675008 -0.21543462  0.31772728  0.15274253 -0.18554952 -0.14604052\n",
      "  0.10849174  0.40573661  0.07094603 -0.23422557]\n",
      "learn time: 4561.54309296608\n",
      "74000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.12285691  0.70573921  0.06128019  0.01570335  0.02289079 -0.01143434\n",
      "  0.02241046  0.11795656 -0.07565092  0.20118423], error = [-0.12285691  0.29426079 -0.06128019 -0.01570335 -0.02289079  0.01143434\n",
      " -0.02241046 -0.11795656  0.07565092 -0.20118423]\n",
      "learn time: 4624.038541793823\n",
      "75000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.14801012 -0.08256066 -0.01166259  0.17847567  0.21413975  0.31292275\n",
      "  0.1840999  -0.11690585  0.04553494  0.28944078], error = [-0.14801012  0.08256066  0.01166259 -0.17847567 -0.21413975  0.68707725\n",
      " -0.1840999   0.11690585 -0.04553494 -0.28944078]\n",
      "learn time: 4686.880244016647\n",
      "76000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.10791567 -0.03370106  0.05515053 -0.06336343  0.01207423  0.25371595\n",
      "  0.04142478  0.06834322  0.64075506 -0.03717983], error = [-0.10791567  0.03370106 -0.05515053  0.06336343 -0.01207423 -0.25371595\n",
      " -0.04142478 -0.06834322  0.35924494  0.03717983]\n",
      "learn time: 4749.574549198151\n",
      "77000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.18056903 -0.04345729  0.13688384  0.55965985 -0.08195698  0.12752832\n",
      "  0.00230258 -0.04067596 -0.02233926  0.00077596], error = [-0.18056903  0.04345729 -0.13688384  0.44034015  0.08195698 -0.12752832\n",
      " -0.00230258  0.04067596  0.02233926 -0.00077596]\n",
      "learn time: 4811.932626485825\n",
      "78000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.12840319 -0.07359933  0.08306807  0.13058202  0.5096954  -0.19325921\n",
      "  0.21485967 -0.07856822 -0.09607228  0.33393516], error = [-0.12840319  0.07359933 -0.08306807 -0.13058202  0.4903046   0.19325921\n",
      " -0.21485967  0.07856822  0.09607228 -0.33393516]\n",
      "learn time: 4874.435668468475\n",
      "79000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 8.61498758e-02  8.79321685e-02  1.62005511e-01 -1.20462587e-04\n",
      " -1.81464750e-02  1.93329053e-02 -3.81488625e-02  1.61602034e-01\n",
      "  5.44407922e-01 -1.05011781e-01], error = [-8.61498758e-02 -8.79321685e-02 -1.62005511e-01  1.20462587e-04\n",
      "  1.81464750e-02 -1.93329053e-02  3.81488625e-02 -1.61602034e-01\n",
      "  4.55592078e-01  1.05011781e-01]\n",
      "learn time: 4937.000335216522\n",
      "80000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.17537985 -0.13241396 -0.17439725  0.19261859  0.22468278  0.68508472\n",
      " -0.04495376 -0.24363173  0.27112748 -0.00384846], error = [-0.17537985  0.13241396  0.17439725 -0.19261859 -0.22468278  0.31491528\n",
      "  0.04495376  0.24363173 -0.27112748  0.00384846]\n",
      "learn time: 4999.696653366089\n",
      "81000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.13461661  0.15201236 -0.01064904  0.07781443 -0.01170429 -0.02074304\n",
      " -0.03299557  0.56933134  0.12743242  0.45008378], error = [-0.13461661 -0.15201236  0.01064904 -0.07781443  0.01170429  0.02074304\n",
      "  0.03299557  0.43066866 -0.12743242 -0.45008378]\n",
      "learn time: 5062.147967100143\n",
      "82000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.12654557  0.58634429  0.0138078   0.35165559  0.032288    0.0542146\n",
      " -0.0802316   0.12107348 -0.18065062  0.13274078], error = [-0.12654557 -0.58634429 -0.0138078   0.64834441 -0.032288   -0.0542146\n",
      "  0.0802316  -0.12107348  0.18065062 -0.13274078]\n",
      "learn time: 5124.635376930237\n",
      "83000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.1128762  -0.08072503 -0.25576241  0.07605892  0.17056384  0.02889154\n",
      " -0.01158699  0.93805926  0.12021553  0.11177525], error = [-0.1128762   0.08072503  0.25576241 -0.07605892 -0.17056384 -0.02889154\n",
      "  0.01158699  0.06194074 -0.12021553 -0.11177525]\n",
      "learn time: 5187.339674711227\n",
      "84000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.10041701 -0.00769066  0.25910999  0.09540473 -0.03094684  0.16611817\n",
      " -0.06247165  0.12598446  0.52473641 -0.12771306], error = [-0.10041701  0.00769066 -0.25910999 -0.09540473  0.03094684 -0.16611817\n",
      "  0.06247165 -0.12598446  0.47526359  0.12771306]\n",
      "learn time: 5249.869361877441\n",
      "85000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.0870291   0.21631093  0.14703543  0.25941745  0.02008994 -0.09763536\n",
      "  0.02374235  0.26982297  0.0697475   0.17390255], error = [-0.0870291  -0.21631093 -0.14703543  0.74058255 -0.02008994  0.09763536\n",
      " -0.02374235 -0.26982297 -0.0697475  -0.17390255]\n",
      "learn time: 5312.1951739788055\n",
      "86000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.0892347   0.1571926  -0.03956884 -0.01393454  0.4900591   0.09231794\n",
      "  0.12887742 -0.02345337 -0.04653165  0.17822726], error = [-0.0892347  -0.1571926   0.03956884  0.01393454  0.5099409  -0.09231794\n",
      " -0.12887742  0.02345337  0.04653165 -0.17822726]\n",
      "learn time: 5374.50480556488\n",
      "87000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.17830953 -0.09060135 -0.01012057  0.20411666  0.30235719  0.33977903\n",
      "  0.07249602 -0.21676288  0.14486805  0.14042894], error = [-0.17830953  0.09060135  0.01012057 -0.20411666 -0.30235719  0.66022097\n",
      " -0.07249602  0.21676288 -0.14486805 -0.14042894]\n",
      "learn time: 5436.839533805847\n",
      "88000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.1046852   0.66057709 -0.01081978 -0.02125682 -0.02893717  0.12176647\n",
      "  0.08438076  0.08888569 -0.03447161  0.0321508 ], error = [-0.1046852   0.33942291  0.01081978  0.02125682  0.02893717 -0.12176647\n",
      " -0.08438076 -0.08888569  0.03447161 -0.0321508 ]\n",
      "learn time: 5499.140003919601\n",
      "89000: expected = [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.11055044  0.14862408  0.44022103  0.21876718  0.01990481  0.01088344\n",
      "  0.1396164  -0.06695833  0.05267977  0.02371676], error = [-0.11055044 -0.14862408  0.55977897 -0.21876718 -0.01990481 -0.01088344\n",
      " -0.1396164   0.06695833 -0.05267977 -0.02371676]\n",
      "learn time: 5561.539274215698\n",
      "90000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.1163093   0.05231133 -0.00457262  0.63186581 -0.05690746  0.10526342\n",
      "  0.05441469  0.19842586  0.01771348  0.08475883], error = [-0.1163093  -0.05231133  0.00457262  0.36813419  0.05690746 -0.10526342\n",
      " -0.05441469 -0.19842586 -0.01771348 -0.08475883]\n",
      "learn time: 5623.876337766647\n",
      "91000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.16645795 -0.0969671   0.1411835  -0.08267127  0.12957715  0.54029009\n",
      "  0.30448677 -0.26928769  0.18184719 -0.02838053], error = [-0.16645795  0.0969671  -0.1411835   0.08267127 -0.12957715 -0.54029009\n",
      "  0.69551323  0.26928769 -0.18184719  0.02838053]\n",
      "learn time: 5686.325442075729\n",
      "92000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.09547884  0.07970422  0.18877271  0.03825129  0.20821599  0.15327165\n",
      "  0.05682922  0.01918946  0.4203906  -0.12987841], error = [-0.09547884 -0.07970422 -0.18877271 -0.03825129 -0.20821599 -0.15327165\n",
      " -0.05682922 -0.01918946  0.5796094   0.12987841]\n",
      "learn time: 5748.734459638596\n",
      "93000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.11515847  0.10570986 -0.01268269  0.30315474 -0.01212661  0.29155966\n",
      "  0.07588919  0.06213916  0.09500877  0.13615292], error = [-0.11515847 -0.10570986  0.01268269  0.69684526  0.01212661 -0.29155966\n",
      " -0.07588919 -0.06213916 -0.09500877 -0.13615292]\n",
      "learn time: 5811.053178310394\n",
      "94000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.15761271 -0.03216347  0.04568339  0.11149816  0.51012342 -0.00538502\n",
      "  0.06730504 -0.12164388 -0.14626909  0.60194216], error = [-0.15761271  0.03216347 -0.04568339 -0.11149816  0.48987658  0.00538502\n",
      " -0.06730504  0.12164388  0.14626909 -0.60194216]\n",
      "learn time: 5873.405976057053\n",
      "95000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.10633059  0.67621402  0.09267457  0.05621497 -0.03341086  0.01143354\n",
      " -0.02577346  0.06169082 -0.05920336  0.05358588], error = [-0.10633059  0.32378598 -0.09267457 -0.05621497  0.03341086 -0.01143354\n",
      "  0.02577346 -0.06169082  0.05920336 -0.05358588]\n",
      "learn time: 5935.7147171497345\n",
      "96000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.11798436  0.25792977 -0.07032397  0.00350257  0.33208575 -0.00300304\n",
      "  0.06705608 -0.08047753  0.22915069  0.36170469], error = [-0.11798436 -0.25792977  0.07032397 -0.00350257 -0.33208575  0.00300304\n",
      " -0.06705608  0.08047753 -0.22915069  0.63829531]\n",
      "learn time: 5998.037987947464\n",
      "97000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.12377112 -0.05790974  0.17732954  0.10043844  0.48337617 -0.01754131\n",
      "  0.17240855 -0.29388187 -0.08893751  0.05596129], error = [-0.12377112  0.05790974 -0.17732954 -0.10043844  0.51662383  0.01754131\n",
      " -0.17240855  0.29388187  0.08893751 -0.05596129]\n",
      "learn time: 6060.432510375977\n",
      "98000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.09803258  0.12427963 -0.0223458   0.07909828  0.06455334  0.07585215\n",
      "  0.01728806  0.18211332  0.25102974  0.10364636], error = [-0.09803258 -0.12427963  0.0223458  -0.07909828 -0.06455334 -0.07585215\n",
      " -0.01728806 -0.18211332  0.74897026 -0.10364636]\n",
      "learn time: 6122.736371994019\n",
      "99000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.10840343 -0.00781517 -0.01775772  0.1320403   0.4367624   0.00726627\n",
      "  0.08626424  0.08401909 -0.00945161  0.36640205], error = [-0.10840343  0.00781517  0.01775772 -0.1320403  -0.4367624  -0.00726627\n",
      " -0.08626424 -0.08401909  0.00945161  0.63359795]\n",
      "learn time: 6185.003076791763\n",
      "100000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.10947985  0.12011248 -0.09504092  0.08470499 -0.10112429 -0.04233354\n",
      " -0.14396655  0.52980674  0.21065481  0.22258008], error = [-0.10947985 -0.12011248  0.09504092 -0.08470499  0.10112429  0.04233354\n",
      "  0.14396655  0.47019326 -0.21065481 -0.22258008]\n",
      "learn time: 6247.359396457672\n",
      "101000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.09749801 -0.11278883 -0.14956046  0.0109197   0.4108956   0.33639364\n",
      "  0.09054864 -0.01441552  0.05377486  0.00334955], error = [-0.09749801  0.11278883  0.14956046 -0.0109197  -0.4108956   0.66360636\n",
      " -0.09054864  0.01441552 -0.05377486 -0.00334955]\n",
      "learn time: 6309.585927963257\n",
      "102000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.12473996  0.89400055  0.03535335 -0.02074766  0.03129088  0.02463278\n",
      "  0.00795615  0.06348722  0.01963923  0.08413524], error = [-0.12473996  0.10599945 -0.03535335  0.02074766 -0.03129088 -0.02463278\n",
      " -0.00795615 -0.06348722 -0.01963923 -0.08413524]\n",
      "learn time: 6371.829187870026\n",
      "103000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.12344262 -0.06006586 -0.06091024 -0.01279708  0.02198211  0.05862113\n",
      " -0.25718901  0.1222372   0.12523123  0.46959005], error = [-0.12344262  0.06006586  0.06091024  0.01279708 -0.02198211 -0.05862113\n",
      "  0.25718901 -0.1222372  -0.12523123  0.53040995]\n",
      "learn time: 6434.177099227905\n",
      "104000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.33138215 -0.01891475 -0.1018463   0.32726804 -0.07159053  0.07213822\n",
      "  0.08275748  0.01535897  0.0172047   0.02831387], error = [ 0.66861785  0.01891475  0.1018463  -0.32726804  0.07159053 -0.07213822\n",
      " -0.08275748 -0.01535897 -0.0172047  -0.02831387]\n",
      "learn time: 6496.543578863144\n",
      "105000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.09093686  0.05349456  0.03549933  0.49272584 -0.09374515  0.15180883\n",
      "  0.12990578 -0.06922458  0.17194212  0.05192109], error = [-0.09093686 -0.05349456 -0.03549933  0.50727416  0.09374515 -0.15180883\n",
      " -0.12990578  0.06922458 -0.17194212 -0.05192109]\n",
      "learn time: 6558.899105787277\n",
      "106000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.11201139  0.11803164  0.07396366 -0.13928375  0.1436821   0.01169552\n",
      " -0.00892611 -0.08055551  0.14826695  0.61241849], error = [-0.11201139 -0.11803164 -0.07396366  0.13928375 -0.1436821  -0.01169552\n",
      "  0.00892611  0.08055551 -0.14826695  0.38758151]\n",
      "learn time: 6621.187538385391\n",
      "107000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.28324391 -0.02336548 -0.04021236  0.16054397 -0.15578185  0.05956122\n",
      "  0.0746815   0.07106581 -0.05135618  0.13011874], error = [ 0.71675609  0.02336548  0.04021236 -0.16054397  0.15578185 -0.05956122\n",
      " -0.0746815  -0.07106581  0.05135618 -0.13011874]\n",
      "learn time: 6683.442454814911\n",
      "108000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [0.09567909 0.11356805 0.01141547 0.03995428 0.4014274  0.27253869\n",
      " 0.16409721 0.03396984 0.00636023 0.06515938], error = [-0.09567909 -0.11356805 -0.01141547 -0.03995428  0.5985726  -0.27253869\n",
      " -0.16409721 -0.03396984 -0.00636023 -0.06515938]\n",
      "learn time: 6745.7581787109375\n",
      "109000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.08688032 -0.11344122 -0.04179465 -0.04911762  0.59597117  0.13456878\n",
      "  0.15314267  0.12381898  0.07690074  0.2377441 ], error = [-0.08688032  0.11344122  0.04179465  0.04911762  0.40402883 -0.13456878\n",
      " -0.15314267 -0.12381898 -0.07690074 -0.2377441 ]\n",
      "learn time: 6808.017158269882\n",
      "110000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.07585423  0.13599174  0.27939994  0.37869638  0.04442342  0.15252801\n",
      "  0.00546581 -0.10980648  0.31376301 -0.36432622], error = [-0.07585423 -0.13599174 -0.27939994  0.62130362 -0.04442342 -0.15252801\n",
      " -0.00546581  0.10980648 -0.31376301  0.36432622]\n",
      "learn time: 6870.268911361694\n",
      "111000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.15391085 -0.06157867 -0.20395171  0.02747937 -0.00666378 -0.00613592\n",
      " -0.01910653  0.89203465  0.07707867  0.3413596 ], error = [-0.15391085  0.06157867  0.20395171 -0.02747937  0.00666378  0.00613592\n",
      "  0.01910653  0.10796535 -0.07707867 -0.3413596 ]\n",
      "learn time: 6932.582112073898\n",
      "112000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.0660523   0.1565465  -0.06502989  0.04281423  0.02355768  0.07434683\n",
      "  0.64887732 -0.00340685  0.02317757  0.01583568], error = [-0.0660523  -0.1565465   0.06502989 -0.04281423 -0.02355768 -0.07434683\n",
      "  0.35112268  0.00340685 -0.02317757 -0.01583568]\n",
      "learn time: 6994.894910812378\n",
      "113000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.17139451 -0.07888248  0.11103858 -0.02117466 -0.0209223   0.43606019\n",
      "  0.09401309 -0.09850775  0.2404397  -0.10305074], error = [-0.17139451  0.07888248 -0.11103858  0.02117466  0.0209223   0.56393981\n",
      " -0.09401309  0.09850775 -0.2404397   0.10305074]\n",
      "learn time: 7057.231477975845\n",
      "114000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.16920912  0.00501059  0.19906962  0.35442019  0.30868137  0.51272084\n",
      "  0.07297254 -0.01641951  0.22279472  0.08154846], error = [-0.16920912 -0.00501059 -0.19906962 -0.35442019 -0.30868137  0.48727916\n",
      " -0.07297254  0.01641951 -0.22279472 -0.08154846]\n",
      "learn time: 7119.590044975281\n",
      "115000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.11887082  0.66157661  0.03350476  0.08757892  0.07646099  0.06676762\n",
      " -0.04025538  0.12997727  0.10506734  0.03577906], error = [-0.11887082  0.33842339 -0.03350476 -0.08757892 -0.07646099 -0.06676762\n",
      "  0.04025538 -0.12997727 -0.10506734 -0.03577906]\n",
      "learn time: 7181.911126613617\n",
      "116000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.11799158  0.81058799  0.05320268  0.09464986  0.12859134  0.06825208\n",
      " -0.08551986  0.15364053  0.01490403  0.04035529], error = [-0.11799158  0.18941201 -0.05320268 -0.09464986 -0.12859134 -0.06825208\n",
      "  0.08551986 -0.15364053 -0.01490403 -0.04035529]\n",
      "learn time: 7244.2059960365295\n",
      "117000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.43892311  0.19814612  0.06202547 -0.0981049   0.02166212  0.07589346\n",
      " -0.15962466 -0.00085784 -0.12061535  0.14616444], error = [ 0.56107689 -0.19814612 -0.06202547  0.0981049  -0.02166212 -0.07589346\n",
      "  0.15962466  0.00085784  0.12061535 -0.14616444]\n",
      "learn time: 7306.558756351471\n",
      "118000: expected = [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.11898735 -0.08709449  0.56957842  0.18520599 -0.06880073 -0.16181666\n",
      " -0.10266249  0.13783384  0.1804697  -0.04404836], error = [-0.11898735  0.08709449  0.43042158 -0.18520599  0.06880073  0.16181666\n",
      "  0.10266249 -0.13783384 -0.1804697   0.04404836]\n",
      "learn time: 7368.8745539188385\n",
      "119000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.07208538 -0.0465362  -0.05123397 -0.01191333 -0.00410812 -0.0264433\n",
      "  0.93339851  0.06533022 -0.05768667  0.00385938], error = [-0.07208538  0.0465362   0.05123397  0.01191333  0.00410812  0.0264433\n",
      "  0.06660149 -0.06533022  0.05768667 -0.00385938]\n",
      "learn time: 7431.2743191719055\n",
      "120000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.15377161 -0.05446819  0.3909171   0.30779342 -0.16133472  0.4687628\n",
      " -0.11956787  0.05260505  0.09536263 -0.18697766], error = [-0.15377161  0.05446819 -0.3909171  -0.30779342  0.16133472  0.5312372\n",
      "  0.11956787 -0.05260505 -0.09536263  0.18697766]\n",
      "learn time: 7493.5009779930115\n",
      "121000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.24104191 -0.03140315 -0.03922614  0.06033688 -0.04435892  0.19878343\n",
      "  0.09640157 -0.06353266  0.16049982  0.01768033], error = [ 0.75895809  0.03140315  0.03922614 -0.06033688  0.04435892 -0.19878343\n",
      " -0.09640157  0.06353266 -0.16049982 -0.01768033]\n",
      "learn time: 7555.783751249313\n",
      "122000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.17126017 -0.05178967  0.07350215  0.02650073 -0.08454139  0.36207022\n",
      "  0.35419944 -0.28903485 -0.01040045  0.00802925], error = [-0.17126017  0.05178967 -0.07350215 -0.02650073  0.08454139  0.63792978\n",
      " -0.35419944  0.28903485  0.01040045 -0.00802925]\n",
      "learn time: 7618.053368091583\n",
      "123000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.26597405 -0.04918368  0.06619256 -0.01142905  0.32821737 -0.01160392\n",
      "  0.15028925  0.06975317  0.08609162  0.11639478], error = [-0.26597405  0.04918368 -0.06619256  0.01142905 -0.32821737  0.01160392\n",
      " -0.15028925 -0.06975317 -0.08609162  0.88360522]\n",
      "learn time: 7680.477648735046\n",
      "124000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.08730106 -0.00562947  0.27913036  0.23171972 -0.15989446 -0.14945413\n",
      " -0.10497253  0.70446262 -0.0221486   0.10223545], error = [-0.08730106  0.00562947 -0.27913036 -0.23171972  0.15989446  0.14945413\n",
      "  0.10497253  0.29553738  0.0221486  -0.10223545]\n",
      "learn time: 7742.845903635025\n",
      "125000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 1.02937691e-01 -2.66783539e-04  1.16693135e-02  1.69845290e-01\n",
      " -5.16406758e-02 -1.69925161e-01  1.19826226e-01  4.28471497e-01\n",
      "  1.70544680e-01  1.02508957e-01], error = [-1.02937691e-01  2.66783539e-04 -1.16693135e-02 -1.69845290e-01\n",
      "  5.16406758e-02  1.69925161e-01 -1.19826226e-01  5.71528503e-01\n",
      " -1.70544680e-01 -1.02508957e-01]\n",
      "learn time: 7805.112154722214\n",
      "126000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.10127466 -0.10638754 -0.01934009  0.14228601  0.02498389  0.11491333\n",
      "  0.8969111  -0.13129162 -0.04138235  0.06346251], error = [-0.10127466  0.10638754  0.01934009 -0.14228601 -0.02498389 -0.11491333\n",
      "  0.1030889   0.13129162  0.04138235 -0.06346251]\n",
      "learn time: 7867.490596532822\n",
      "127000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.07378684  0.10606443 -0.0090103   0.19641469 -0.02362365  0.19497759\n",
      "  0.0724691  -0.00733709  0.30986691  0.18317729], error = [-0.07378684 -0.10606443  0.0090103  -0.19641469  0.02362365 -0.19497759\n",
      " -0.0724691   0.00733709  0.69013309 -0.18317729]\n",
      "learn time: 7929.998749494553\n",
      "128000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.44689312  0.06961922 -0.01559233 -0.02877776 -0.08359491  0.01084432\n",
      "  0.11871877 -0.05311573  0.00870329  0.14997178], error = [ 0.55310688 -0.06961922  0.01559233  0.02877776  0.08359491 -0.01084432\n",
      " -0.11871877  0.05311573 -0.00870329 -0.14997178]\n",
      "learn time: 7992.406108617783\n",
      "129000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.13834579 -0.13521947  0.23188724  0.05052596  0.18534902 -0.03448705\n",
      "  0.52654986 -0.03784222  0.15754905 -0.05308035], error = [-0.13834579  0.13521947 -0.23188724 -0.05052596 -0.18534902  0.03448705\n",
      "  0.47345014  0.03784222 -0.15754905  0.05308035]\n",
      "learn time: 8054.720038175583\n",
      "130000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.1209158  -0.02745649  0.12918568  0.96189449 -0.06528957  0.13649358\n",
      "  0.12229022  0.11103077 -0.0841515  -0.07903352], error = [-0.1209158   0.02745649 -0.12918568  0.03810551  0.06528957 -0.13649358\n",
      " -0.12229022 -0.11103077  0.0841515   0.07903352]\n",
      "learn time: 8116.9511342048645\n",
      "131000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.05389181  0.00759264 -0.05757569 -0.06467068  0.10856399 -0.06864102\n",
      "  0.97523077  0.05970662  0.08247455  0.15588688], error = [-0.05389181 -0.00759264  0.05757569  0.06467068 -0.10856399  0.06864102\n",
      "  0.02476923 -0.05970662 -0.08247455 -0.15588688]\n",
      "learn time: 8179.307567119598\n",
      "132000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.10119632 -0.17128371  0.24288268  0.15918875  0.0535348  -0.06801166\n",
      " -0.01467326  0.5965177   0.00285478  0.00644667], error = [-0.10119632  0.17128371 -0.24288268 -0.15918875 -0.0535348   0.06801166\n",
      "  0.01467326  0.4034823  -0.00285478 -0.00644667]\n",
      "learn time: 8241.581943511963\n",
      "133000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.1323502   0.10841602 -0.22187529 -0.2700464   0.0477044   0.19881246\n",
      " -0.08444357  0.74281116 -0.04625952  0.17477005], error = [-0.1323502  -0.10841602  0.22187529  0.2700464  -0.0477044  -0.19881246\n",
      "  0.08444357  0.25718884  0.04625952 -0.17477005]\n",
      "learn time: 8303.91067123413\n",
      "134000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.07623952  0.89353386  0.04669102  0.01941181  0.02650517 -0.05414575\n",
      " -0.04932793  0.01316564 -0.10383041  0.20611303], error = [-0.07623952  0.10646614 -0.04669102 -0.01941181 -0.02650517  0.05414575\n",
      "  0.04932793 -0.01316564  0.10383041 -0.20611303]\n",
      "learn time: 8366.195984363556\n",
      "135000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.17716973 -0.12276276 -0.04135141  0.22249807  0.25614514  0.40841116\n",
      "  0.2208505  -0.13604315  0.0184614   0.35146099], error = [-0.17716973  0.12276276  0.04135141 -0.22249807 -0.25614514  0.59158884\n",
      " -0.2208505   0.13604315 -0.0184614  -0.35146099]\n",
      "learn time: 8428.524312734604\n",
      "136000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.0818192  -0.09233756  0.07780547 -0.02441138  0.03469089  0.17144855\n",
      "  0.03578392  0.09058199  0.82761459 -0.06314538], error = [-0.0818192   0.09233756 -0.07780547  0.02441138 -0.03469089 -0.17144855\n",
      " -0.03578392 -0.09058199  0.17238541  0.06314538]\n",
      "learn time: 8490.90006017685\n",
      "137000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.28061943 -0.00682499  0.07779471  0.53207332  0.01418097  0.15904458\n",
      " -0.0833816   0.02536503  0.02464809 -0.03182597], error = [-0.28061943  0.00682499 -0.07779471  0.46792668 -0.01418097 -0.15904458\n",
      "  0.0833816  -0.02536503 -0.02464809  0.03182597]\n",
      "learn time: 8553.299204826355\n",
      "138000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.11533302 -0.03609589  0.11055116  0.08694088  0.51858138 -0.24275582\n",
      "  0.16108519 -0.10060174 -0.10687485  0.34088938], error = [-0.11533302  0.03609589 -0.11055116 -0.08694088  0.48141862  0.24275582\n",
      " -0.16108519  0.10060174  0.10687485 -0.34088938]\n",
      "learn time: 8615.566980361938\n",
      "139000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.05904389  0.03607489  0.16917823  0.03237719  0.02562731 -0.01418664\n",
      " -0.08862681  0.17528387  0.6473902  -0.09652221], error = [-0.05904389 -0.03607489 -0.16917823 -0.03237719 -0.02562731  0.01418664\n",
      "  0.08862681 -0.17528387  0.3526098   0.09652221]\n",
      "learn time: 8678.389323711395\n",
      "140000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.25014285 -0.07616317 -0.11752278  0.04804798  0.22835749  0.79497325\n",
      " -0.12035448 -0.15513632  0.34907913 -0.01492314], error = [-0.25014285  0.07616317  0.11752278 -0.04804798 -0.22835749  0.20502675\n",
      "  0.12035448  0.15513632 -0.34907913  0.01492314]\n",
      "learn time: 8741.648124456406\n",
      "141000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.09745143  0.08765508  0.08798058  0.01525506 -0.03063458 -0.00647776\n",
      " -0.00764322  0.65877976  0.13996114  0.4499028 ], error = [-0.09745143 -0.08765508 -0.08798058 -0.01525506  0.03063458  0.00647776\n",
      "  0.00764322  0.34122024 -0.13996114 -0.4499028 ]\n",
      "learn time: 8804.128492116928\n",
      "142000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.08262479  0.59272993 -0.05521477  0.46788787  0.01179696 -0.01637216\n",
      " -0.06418049  0.11312321 -0.17479439  0.18680634], error = [-0.08262479 -0.59272993  0.05521477  0.53211213 -0.01179696  0.01637216\n",
      "  0.06418049 -0.11312321  0.17479439 -0.18680634]\n",
      "learn time: 8866.618167161942\n",
      "143000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.10954385 -0.04589801 -0.24508074  0.09596181 -0.00904297  0.0427382\n",
      "  0.04067001  1.08427833  0.06364434 -0.04020714], error = [-0.10954385  0.04589801  0.24508074 -0.09596181  0.00904297 -0.0427382\n",
      " -0.04067001 -0.08427833 -0.06364434  0.04020714]\n",
      "learn time: 8929.095043897629\n",
      "144000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.07818755 -0.06340413  0.29286071  0.08160297 -0.00146322  0.11304243\n",
      " -0.0719848   0.05369586  0.62607198 -0.09234716], error = [-0.07818755  0.06340413 -0.29286071 -0.08160297  0.00146322 -0.11304243\n",
      "  0.0719848  -0.05369586  0.37392802  0.09234716]\n",
      "learn time: 8991.58844447136\n",
      "145000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.0488961   0.21083916  0.12037011  0.26561366  0.02397964 -0.1196756\n",
      "  0.00160628  0.24573688  0.12448233  0.19045017], error = [-0.0488961  -0.21083916 -0.12037011  0.73438634 -0.02397964  0.1196756\n",
      " -0.00160628 -0.24573688 -0.12448233 -0.19045017]\n",
      "learn time: 9054.032016038895\n",
      "146000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.04933699  0.07098049  0.00513134 -0.0496679   0.52585368  0.06673465\n",
      "  0.09347933 -0.04276412  0.02282483  0.23987428], error = [-0.04933699 -0.07098049 -0.00513134  0.0496679   0.47414632 -0.06673465\n",
      " -0.09347933  0.04276412 -0.02282483 -0.23987428]\n",
      "learn time: 9116.40534901619\n",
      "147000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.22309174 -0.05026129 -0.04974311  0.15907535  0.3072952   0.40092885\n",
      " -0.01806215 -0.16779831  0.22956343  0.17995901], error = [-0.22309174  0.05026129  0.04974311 -0.15907535 -0.3072952   0.59907115\n",
      "  0.01806215  0.16779831 -0.22956343 -0.17995901]\n",
      "learn time: 9178.80622625351\n",
      "148000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.06300592  0.74391542 -0.01830846 -0.01154858 -0.04832255  0.11211442\n",
      "  0.00631576  0.03521675  0.02267573 -0.00560122], error = [-0.06300592  0.25608458  0.01830846  0.01154858  0.04832255 -0.11211442\n",
      " -0.00631576 -0.03521675 -0.02267573  0.00560122]\n",
      "learn time: 9241.102371931076\n",
      "149000: expected = [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.0912256   0.11330375  0.54227753  0.1753628   0.04538517  0.06044809\n",
      "  0.08394472 -0.04804349  0.11262428  0.05536268], error = [-0.0912256  -0.11330375  0.45772247 -0.1753628  -0.04538517 -0.06044809\n",
      " -0.08394472  0.04804349 -0.11262428 -0.05536268]\n",
      "learn time: 9303.485117912292\n",
      "150000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.09937497  0.049578   -0.12424547  0.75018419  0.00680286  0.01815226\n",
      "  0.08714316  0.15300927 -0.01009563  0.12633164], error = [-0.09937497 -0.049578    0.12424547  0.24981581 -0.00680286 -0.01815226\n",
      " -0.08714316 -0.15300927  0.01009563 -0.12633164]\n",
      "learn time: 9365.810278654099\n",
      "151000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.22112763 -0.1144518   0.14830269 -0.14080473  0.05386857  0.63558457\n",
      "  0.32779845 -0.13076788  0.1872295   0.00621416], error = [-0.22112763  0.1144518  -0.14830269  0.14080473 -0.05386857 -0.63558457\n",
      "  0.67220155  0.13076788 -0.1872295  -0.00621416]\n",
      "learn time: 9428.242095470428\n",
      "152000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.07066685  0.04650905  0.17981307  0.03376086  0.20426795  0.12827318\n",
      "  0.02373781 -0.00720369  0.58118506 -0.11041556], error = [-0.07066685 -0.04650905 -0.17981307 -0.03376086 -0.20426795 -0.12827318\n",
      " -0.02373781  0.00720369  0.41881494  0.11041556]\n",
      "learn time: 9490.654564857483\n",
      "153000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.08969838  0.04747908 -0.01880376  0.39412019 -0.00218211  0.33533336\n",
      "  0.0668591   0.02359545  0.10486168  0.12676794], error = [-0.08969838 -0.04747908  0.01880376  0.60587981  0.00218211 -0.33533336\n",
      " -0.0668591  -0.02359545 -0.10486168 -0.12676794]\n",
      "learn time: 9553.065760850906\n",
      "154000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.11146229 -0.13434594  0.08737373  0.09609705  0.55276044 -0.0282159\n",
      "  0.03480555 -0.12037473 -0.1636907   0.64496611], error = [-0.11146229  0.13434594 -0.08737373 -0.09609705  0.44723956  0.0282159\n",
      " -0.03480555  0.12037473  0.1636907  -0.64496611]\n",
      "learn time: 9615.456469297409\n",
      "155000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.05879308  0.82050694  0.05688689  0.05801826 -0.04893863 -0.01318375\n",
      " -0.07279384 -0.02669298 -0.00449055  0.04596226], error = [-0.05879308  0.17949306 -0.05688689 -0.05801826  0.04893863  0.01318375\n",
      "  0.07279384  0.02669298  0.00449055 -0.04596226]\n",
      "learn time: 9677.849025011063\n",
      "156000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.05996415  0.2205656  -0.19187028  0.02462125  0.4134074   0.05414354\n",
      "  0.10286254 -0.12383861  0.14867739  0.41726548], error = [-0.05996415 -0.2205656   0.19187028 -0.02462125 -0.4134074  -0.05414354\n",
      " -0.10286254  0.12383861 -0.14867739  0.58273452]\n",
      "learn time: 9740.396341085434\n",
      "157000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.09434302 -0.03376502  0.09478111  0.13291441  0.63328688  0.01111921\n",
      "  0.12861537 -0.21396775 -0.03681426  0.06683875], error = [-0.09434302  0.03376502 -0.09478111 -0.13291441  0.36671312 -0.01111921\n",
      " -0.12861537  0.21396775  0.03681426 -0.06683875]\n",
      "learn time: 9802.852223157883\n",
      "158000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 6.66950147e-02  8.07387233e-02 -1.85900263e-02  8.81794776e-02\n",
      "  6.94836866e-02  6.73445322e-02 -3.30335102e-04  1.32387955e-01\n",
      "  3.81072172e-01  1.29304778e-01], error = [-6.66950147e-02 -8.07387233e-02  1.85900263e-02 -8.81794776e-02\n",
      " -6.94836866e-02 -6.73445322e-02  3.30335102e-04 -1.32387955e-01\n",
      "  6.18927828e-01 -1.29304778e-01]\n",
      "learn time: 9865.276046276093\n",
      "159000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.07481935 -0.00814211 -0.00136237  0.14287412  0.42316209 -0.02477404\n",
      "  0.09552923  0.02887126  0.00628475  0.42024629], error = [-0.07481935  0.00814211  0.00136237 -0.14287412 -0.42316209  0.02477404\n",
      " -0.09552923 -0.02887126 -0.00628475  0.57975371]\n",
      "learn time: 9927.6420814991\n",
      "160000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.06756808  0.10206524 -0.11318128  0.05200686 -0.08916377 -0.0217135\n",
      " -0.05949095  0.69968632  0.15612556  0.21350684], error = [-0.06756808 -0.10206524  0.11318128 -0.05200686  0.08916377  0.0217135\n",
      "  0.05949095  0.30031368 -0.15612556 -0.21350684]\n",
      "learn time: 9990.063098669052\n",
      "161000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.06372791 -0.18710253 -0.09490943  0.04845942  0.48012864  0.37273784\n",
      "  0.06371477 -0.01602321  0.04247453 -0.0347368 ], error = [-0.06372791  0.18710253  0.09490943 -0.04845942 -0.48012864  0.62726216\n",
      " -0.06371477  0.01602321 -0.04247453  0.0347368 ]\n",
      "learn time: 10052.4801633358\n",
      "162000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.07934855  1.00954345  0.01282773 -0.01807694  0.04508503  0.01885157\n",
      " -0.00395635 -0.00298926  0.04381788  0.07253444], error = [-0.07934855 -0.00954345 -0.01282773  0.01807694 -0.04508503 -0.01885157\n",
      "  0.00395635  0.00298926 -0.04381788 -0.07253444]\n",
      "learn time: 10114.80676817894\n",
      "163000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.07384135 -0.06945285  0.02172076  0.04720229 -0.0416019   0.01803683\n",
      " -0.28577255  0.12854526  0.03801917  0.52737611], error = [-0.07384135  0.06945285 -0.02172076 -0.04720229  0.0416019  -0.01803683\n",
      "  0.28577255 -0.12854526 -0.03801917  0.47262389]\n",
      "learn time: 10177.181899547577\n",
      "164000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.53309637  0.04622443 -0.11058523  0.22369467 -0.03961652  0.00123248\n",
      "  0.00746781  0.03498071  0.05127297  0.00717125], error = [ 0.46690363 -0.04622443  0.11058523 -0.22369467  0.03961652 -0.00123248\n",
      " -0.00746781 -0.03498071 -0.05127297 -0.00717125]\n",
      "learn time: 10239.614214420319\n",
      "165000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.05136533  0.04566224  0.02196477  0.58597747 -0.04165307  0.14678131\n",
      "  0.08084731 -0.1130873   0.24666957 -0.00967493], error = [-0.05136533 -0.04566224 -0.02196477  0.41402253  0.04165307 -0.14678131\n",
      " -0.08084731  0.1130873  -0.24666957  0.00967493]\n",
      "learn time: 10302.049887418747\n",
      "166000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.05608047  0.06621144  0.14021452 -0.17462536  0.11000063 -0.01952035\n",
      " -0.06364043 -0.08220717  0.12016597  0.67387075], error = [-0.05608047 -0.06621144 -0.14021452  0.17462536 -0.11000063  0.01952035\n",
      "  0.06364043  0.08220717 -0.12016597  0.32612925]\n",
      "learn time: 10364.475294828415\n",
      "167000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 4.62441846e-01  1.48976871e-03 -4.54985202e-02  1.11673641e-01\n",
      " -1.57517667e-01  3.36796353e-02  5.48349985e-02 -1.47467193e-04\n",
      "  2.55545968e-02  1.14256330e-01], error = [ 5.37558154e-01 -1.48976871e-03  4.54985202e-02 -1.11673641e-01\n",
      "  1.57517667e-01 -3.36796353e-02 -5.48349985e-02  1.47467193e-04\n",
      " -2.55545968e-02 -1.14256330e-01]\n",
      "learn time: 10426.835026025772\n",
      "168000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [0.06068638 0.0708809  0.0627651  0.05122664 0.44406464 0.27452755\n",
      " 0.20756217 0.04153424 0.04167057 0.04191069], error = [-0.06068638 -0.0708809  -0.0627651  -0.05122664  0.55593536 -0.27452755\n",
      " -0.20756217 -0.04153424 -0.04167057 -0.04191069]\n",
      "learn time: 10489.191733121872\n",
      "169000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.05575317 -0.03651071 -0.04137623  0.00318801  0.6529561   0.15475515\n",
      "  0.14358121  0.06622483  0.04859195  0.1696701 ], error = [-0.05575317  0.03651071  0.04137623 -0.00318801  0.3470439  -0.15475515\n",
      " -0.14358121 -0.06622483 -0.04859195 -0.1696701 ]\n",
      "learn time: 10551.599817991257\n",
      "170000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.04763566  0.15681223  0.32982764  0.45965068  0.06976911  0.12634464\n",
      " -0.03881019 -0.14838707  0.273099   -0.34425891], error = [-0.04763566 -0.15681223 -0.32982764  0.54034932 -0.06976911 -0.12634464\n",
      "  0.03881019  0.14838707 -0.273099    0.34425891]\n",
      "learn time: 10614.076757192612\n",
      "171000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.12447354 -0.07309604 -0.14060191 -0.0027742  -0.10589911  0.02987552\n",
      "  0.04574761  0.96074268  0.00783605  0.25679356], error = [-0.12447354  0.07309604  0.14060191  0.0027742   0.10589911 -0.02987552\n",
      " -0.04574761  0.03925732 -0.00783605 -0.25679356]\n",
      "learn time: 10676.520040988922\n",
      "172000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.03467795  0.08210518 -0.09595955  0.04009646  0.00835519  0.03072042\n",
      "  0.7296826   0.07425664  0.06632142  0.01410887], error = [-0.03467795 -0.08210518  0.09595955 -0.04009646 -0.00835519 -0.03072042\n",
      "  0.2703174  -0.07425664 -0.06632142 -0.01410887]\n",
      "learn time: 10738.943146705627\n",
      "173000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.19708782 -0.0575498   0.09153647 -0.02131107 -0.00630047  0.50634335\n",
      "  0.0325942  -0.11729448  0.13086784 -0.06618017], error = [-0.19708782  0.0575498  -0.09153647  0.02131107  0.00630047  0.49365665\n",
      " -0.0325942   0.11729448 -0.13086784  0.06618017]\n",
      "learn time: 10801.331452608109\n",
      "174000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.16124999 -0.00889126  0.12873729  0.33201662  0.25420973  0.56301568\n",
      "  0.02342354 -0.02333592  0.10635794  0.16497041], error = [-0.16124999  0.00889126 -0.12873729 -0.33201662 -0.25420973  0.43698432\n",
      " -0.02342354  0.02333592 -0.10635794 -0.16497041]\n",
      "learn time: 10863.77088046074\n",
      "175000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.08061705  0.75173584 -0.01181817  0.08019499  0.07848481  0.05096389\n",
      " -0.0248494   0.10554039  0.09726533  0.01588843], error = [-0.08061705  0.24826416  0.01181817 -0.08019499 -0.07848481 -0.05096389\n",
      "  0.0248494  -0.10554039 -0.09726533 -0.01588843]\n",
      "learn time: 10926.108762979507\n",
      "176000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.07453528  0.89500745  0.00354335  0.09410741  0.10974293  0.06764566\n",
      " -0.07756657  0.12567832  0.00593717  0.04272878], error = [-0.07453528  0.10499255 -0.00354335 -0.09410741 -0.10974293 -0.06764566\n",
      "  0.07756657 -0.12567832 -0.00593717 -0.04272878]\n",
      "learn time: 10988.53016281128\n",
      "177000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.6512807   0.17665869 -0.03688926 -0.10178294 -0.00642718  0.00668236\n",
      " -0.10513489 -0.04702819 -0.05556322  0.14421176], error = [ 0.3487193  -0.17665869  0.03688926  0.10178294  0.00642718 -0.00668236\n",
      "  0.10513489  0.04702819  0.05556322 -0.14421176]\n",
      "learn time: 11050.988709926605\n",
      "178000: expected = [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.08097159 -0.11012742  0.63494079  0.17422621 -0.04099182 -0.18922866\n",
      " -0.12904338  0.09566353  0.25602777 -0.02631168], error = [-0.08097159  0.11012742  0.36505921 -0.17422621  0.04099182  0.18922866\n",
      "  0.12904338 -0.09566353 -0.25602777  0.02631168]\n",
      "learn time: 11113.360582113266\n",
      "179000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.05149617 -0.09110154 -0.06455849 -0.01318333 -0.03381579 -0.04056224\n",
      "  0.99857543  0.04615062 -0.09339877 -0.02229129], error = [-0.05149617  0.09110154  0.06455849  0.01318333  0.03381579  0.04056224\n",
      "  0.00142457 -0.04615062  0.09339877  0.02229129]\n",
      "learn time: 11175.694648504257\n",
      "180000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.13963342 -0.04471869  0.4232444   0.33320525 -0.10862419  0.53970959\n",
      " -0.14890512  0.00456185 -0.04845577 -0.19200637], error = [-0.13963342  0.04471869 -0.4232444  -0.33320525  0.10862419  0.46029041\n",
      "  0.14890512 -0.00456185  0.04845577  0.19200637]\n",
      "learn time: 11238.003628969193\n",
      "181000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.331643   -0.02167715 -0.0320318  -0.00570994 -0.02402627  0.19180088\n",
      "  0.04605872 -0.07526312  0.17668871 -0.00089897], error = [ 0.668357    0.02167715  0.0320318   0.00570994  0.02402627 -0.19180088\n",
      " -0.04605872  0.07526312 -0.17668871  0.00089897]\n",
      "learn time: 11300.426716566086\n",
      "182000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.17542647 -0.02349755  0.0832221   0.02347204 -0.06143936  0.42194303\n",
      "  0.30166496 -0.21659653 -0.11062542  0.004829  ], error = [-0.17542647  0.02349755 -0.0832221  -0.02347204  0.06143936  0.57805697\n",
      " -0.30166496  0.21659653  0.11062542 -0.004829  ]\n",
      "learn time: 11362.693369626999\n",
      "183000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.344495    0.03047863  0.02432429 -0.03522977  0.3083061   0.00951823\n",
      "  0.11535794  0.01247937  0.13839949  0.15195694], error = [-0.344495   -0.03047863 -0.02432429  0.03522977 -0.3083061  -0.00951823\n",
      " -0.11535794 -0.01247937 -0.13839949  0.84804306]\n",
      "learn time: 11425.020603656769\n",
      "184000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.05053216 -0.04776333  0.26666146  0.24686957 -0.12849501 -0.1452436\n",
      " -0.07618398  0.80119682 -0.04185912  0.07198464], error = [-0.05053216  0.04776333 -0.26666146 -0.24686957  0.12849501  0.1452436\n",
      "  0.07618398  0.19880318  0.04185912 -0.07198464]\n",
      "learn time: 11487.475505590439\n",
      "185000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.0721744   0.03240591  0.0142918   0.1090726  -0.09963597 -0.15036149\n",
      "  0.10329069  0.38787921  0.22000042  0.11447951], error = [-0.0721744  -0.03240591 -0.0142918  -0.1090726   0.09963597  0.15036149\n",
      " -0.10329069  0.61212079 -0.22000042 -0.11447951]\n",
      "learn time: 11549.931529521942\n",
      "186000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.07668109 -0.10074316 -0.03180893  0.11649297  0.00735301  0.11496975\n",
      "  0.93370577 -0.09933896 -0.07691328  0.06831503], error = [-0.07668109  0.10074316  0.03180893 -0.11649297 -0.00735301 -0.11496975\n",
      "  0.06629423  0.09933896  0.07691328 -0.06831503]\n",
      "learn time: 11612.285024166107\n",
      "187000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.03962221  0.13564632 -0.01286595  0.17611882  0.00795143  0.21088285\n",
      "  0.00255339  0.00818151  0.43953574  0.13790695], error = [-0.03962221 -0.13564632  0.01286595 -0.17611882 -0.00795143 -0.21088285\n",
      " -0.00255339 -0.00818151  0.56046426 -0.13790695]\n",
      "learn time: 11674.677848100662\n",
      "188000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.6367121   0.0488717  -0.06207445 -0.0497809  -0.06730928 -0.02519513\n",
      "  0.08713255 -0.11057815 -0.00584379  0.14713983], error = [ 0.3632879  -0.0488717   0.06207445  0.0497809   0.06730928  0.02519513\n",
      " -0.08713255  0.11057815  0.00584379 -0.14713983]\n",
      "learn time: 11737.03049492836\n",
      "189000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.11588489 -0.00556117  0.0761257   0.03904284  0.11196387 -0.07019025\n",
      "  0.61134221 -0.11372517  0.27594919 -0.09107554], error = [-0.11588489  0.00556117 -0.0761257  -0.03904284 -0.11196387  0.07019025\n",
      "  0.38865779  0.11372517 -0.27594919  0.09107554]\n",
      "learn time: 11799.824914932251\n",
      "190000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 9.05890879e-02 -4.43418959e-02  1.00615613e-01  1.03712100e+00\n",
      "  7.31803233e-04  1.14059788e-01  1.07983358e-01  7.49598261e-02\n",
      " -1.43788943e-01 -3.49340422e-02], error = [-0.09058909  0.0443419  -0.10061561 -0.037121   -0.0007318  -0.11405979\n",
      " -0.10798336 -0.07495983  0.14378894  0.03493404]\n",
      "learn time: 11862.298033714294\n",
      "191000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 3.35941996e-02 -7.20869235e-03 -1.68058282e-01 -5.32942633e-02\n",
      "  5.49713137e-02 -9.68280803e-02  1.02829912e+00 -6.53000907e-04\n",
      "  4.69022566e-02  9.19740418e-02], error = [-0.0335942   0.00720869  0.16805828  0.05329426 -0.05497131  0.09682808\n",
      " -0.02829912  0.000653   -0.04690226 -0.09197404]\n",
      "learn time: 11924.755460262299\n",
      "192000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.07350811 -0.20611683  0.19090006  0.15545154  0.0460865  -0.08127374\n",
      " -0.02679538  0.64852927  0.05717838 -0.02277694], error = [-0.07350811  0.20611683 -0.19090006 -0.15545154 -0.0460865   0.08127374\n",
      "  0.02679538  0.35147073 -0.05717838  0.02277694]\n",
      "learn time: 11987.18108534813\n",
      "193000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.10777017  0.04642092 -0.12213973 -0.30382741 -0.03071293  0.24875272\n",
      " -0.05024579  0.83204849 -0.043551    0.14348816], error = [-0.10777017 -0.04642092  0.12213973  0.30382741  0.03071293 -0.24875272\n",
      "  0.05024579  0.16795151  0.043551   -0.14348816]\n",
      "learn time: 12049.5931558609\n",
      "194000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 3.93562154e-02  9.85329508e-01  1.42172260e-02 -1.00904376e-02\n",
      "  2.89881085e-02 -6.15195970e-02 -5.46359807e-02  6.97851852e-04\n",
      " -8.19291900e-02  2.21170183e-01], error = [-0.03935622  0.01467049 -0.01421723  0.01009044 -0.02898811  0.0615196\n",
      "  0.05463598 -0.00069785  0.08192919 -0.22117018]\n",
      "learn time: 12111.961328744888\n",
      "195000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.15155841 -0.12116842 -0.01879789  0.18481429  0.23182446  0.4947964\n",
      "  0.21109613 -0.12516029 -0.04839562  0.37329653], error = [-0.15155841  0.12116842  0.01879789 -0.18481429 -0.23182446  0.5052036\n",
      " -0.21109613  0.12516029  0.04839562 -0.37329653]\n",
      "learn time: 12174.397725343704\n",
      "196000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 4.89143693e-02 -1.01838193e-01  5.65839309e-02  7.93889501e-03\n",
      " -2.88870060e-04  8.23681345e-02  2.93262892e-02  1.25981614e-02\n",
      "  8.97653282e-01 -3.60340497e-02], error = [-0.04891437  0.10183819 -0.05658393 -0.0079389   0.00028887 -0.08236813\n",
      " -0.02932629 -0.01259816  0.10234672  0.03603405]\n",
      "learn time: 12236.828505516052\n",
      "197000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.31344678  0.02935975  0.1032777   0.47338698  0.05493848  0.17119372\n",
      " -0.08900354  0.08813091  0.06448335 -0.02720706], error = [-0.31344678 -0.02935975 -0.1032777   0.52661302 -0.05493848 -0.17119372\n",
      "  0.08900354 -0.08813091 -0.06448335  0.02720706]\n",
      "learn time: 12299.267995119095\n",
      "198000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.08207011 -0.02928574  0.12129422  0.05176444  0.55439164 -0.22196257\n",
      "  0.09123169 -0.0780369  -0.09778225  0.29868568], error = [-0.08207011  0.02928574 -0.12129422 -0.05176444  0.44560836  0.22196257\n",
      " -0.09123169  0.0780369   0.09778225 -0.29868568]\n",
      "learn time: 12361.659702539444\n",
      "199000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.03591532  0.00938846  0.18857458  0.03711931  0.00745066 -0.04010509\n",
      " -0.08209636  0.14619252  0.67342722 -0.0720838 ], error = [-0.03591532 -0.00938846 -0.18857458 -0.03711931 -0.00745066  0.04010509\n",
      "  0.08209636 -0.14619252  0.32657278  0.0720838 ]\n",
      "learn time: 12424.16711640358\n",
      "200000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.25857901 -0.03355699 -0.08387393 -0.04114153  0.20294774  0.82431363\n",
      " -0.11601742 -0.09499288  0.30433545  0.02874605], error = [-0.25857901  0.03355699  0.08387393  0.04114153 -0.20294774  0.17568637\n",
      "  0.11601742  0.09499288 -0.30433545 -0.02874605]\n",
      "learn time: 12486.535250663757\n",
      "201000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.06285194  0.04538418  0.13970706  0.01283987 -0.01313585  0.01665794\n",
      "  0.00930432  0.66070517  0.12513858  0.42954358], error = [-0.06285194 -0.04538418 -0.13970706 -0.01283987  0.01313585 -0.01665794\n",
      " -0.00930432  0.33929483 -0.12513858 -0.42954358]\n",
      "learn time: 12548.930774211884\n",
      "202000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.04798224  0.59368095 -0.09583715  0.53014842  0.00505203 -0.05783006\n",
      " -0.04156006  0.13863136 -0.14075351  0.20611398], error = [-0.04798224 -0.59368095  0.09583715  0.46985158 -0.00505203  0.05783006\n",
      "  0.04156006 -0.13863136  0.14075351 -0.20611398]\n",
      "learn time: 12611.336456537247\n",
      "203000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.08417764 -0.0413028  -0.21615644  0.06677678 -0.11433663  0.0561326\n",
      "  0.01075137  1.08364705  0.00276197 -0.07145536], error = [-0.08417764  0.0413028   0.21615644 -0.06677678  0.11433663 -0.0561326\n",
      " -0.01075137 -0.08364705 -0.00276197  0.07145536]\n",
      "learn time: 12673.786000967026\n",
      "204000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.0494887  -0.08385264  0.25987155  0.09985914 -0.00525195  0.07435196\n",
      " -0.05370575  0.00158935  0.65881026 -0.04319186], error = [-0.0494887   0.08385264 -0.25987155 -0.09985914  0.00525195 -0.07435196\n",
      "  0.05370575 -0.00158935  0.34118974  0.04319186]\n",
      "learn time: 12736.132227182388\n",
      "205000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.02627895  0.19048778  0.09124707  0.26724849  0.03388195 -0.1179157\n",
      "  0.00196902  0.27064604  0.16585248  0.16092202], error = [-0.02627895 -0.19048778 -0.09124707  0.73275151 -0.03388195  0.1179157\n",
      " -0.00196902 -0.27064604 -0.16585248 -0.16092202]\n",
      "learn time: 12798.46995973587\n",
      "206000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.02662256  0.02574057  0.02656799 -0.01899106  0.59072718  0.04671075\n",
      "  0.06049168 -0.02363211  0.05566428  0.25340619], error = [-0.02662256 -0.02574057 -0.02656799  0.01899106  0.40927282 -0.04671075\n",
      " -0.06049168  0.02363211 -0.05566428 -0.25340619]\n",
      "learn time: 12860.854096651077\n",
      "207000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.21756467  0.00199381 -0.0367235   0.10480366  0.28903307  0.45151068\n",
      " -0.05840571 -0.11446055  0.22483679  0.19844499], error = [-0.21756467 -0.00199381  0.0367235  -0.10480366 -0.28903307  0.54848932\n",
      "  0.05840571  0.11446055 -0.22483679 -0.19844499]\n",
      "learn time: 12923.289963960648\n",
      "208000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.03605751  0.78037127 -0.03424096 -0.0033481  -0.05808021  0.10168838\n",
      " -0.02417723  0.01462157  0.07740871 -0.02819134], error = [-0.03605751  0.21962873  0.03424096  0.0033481   0.05808021 -0.10168838\n",
      "  0.02417723 -0.01462157 -0.07740871  0.02819134]\n",
      "learn time: 12985.569780349731\n",
      "209000: expected = [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.06049805  0.09035172  0.60198101  0.16256724  0.05543198  0.08852072\n",
      "  0.09216483 -0.01639037  0.11704508  0.04153839], error = [-0.06049805 -0.09035172  0.39801899 -0.16256724 -0.05543198 -0.08852072\n",
      " -0.09216483  0.01639037 -0.11704508 -0.04153839]\n",
      "learn time: 13048.031455039978\n",
      "210000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.07005758  0.04074697 -0.16745441  0.83505902  0.03611828 -0.0481861\n",
      "  0.0913585   0.1300668   0.00651036  0.14063781], error = [-0.07005758 -0.04074697  0.16745441  0.16494098 -0.03611828  0.0481861\n",
      " -0.0913585  -0.1300668  -0.00651036 -0.14063781]\n",
      "learn time: 13110.471385002136\n",
      "211000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.22437413 -0.10469998  0.12601185 -0.14194521  0.03253927  0.68550325\n",
      "  0.34862679 -0.01862366  0.14903131 -0.02479395], error = [-0.22437413  0.10469998 -0.12601185  0.14194521 -0.03253927 -0.68550325\n",
      "  0.65137321  0.01862366 -0.14903131  0.02479395]\n",
      "learn time: 13172.860794782639\n",
      "212000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.04770273  0.02027376  0.17452263  0.04523614  0.16608369  0.0991724\n",
      "  0.01302759 -0.03308812  0.6597672  -0.09755455], error = [-0.04770273 -0.02027376 -0.17452263 -0.04523614 -0.16608369 -0.0991724\n",
      " -0.01302759  0.03308812  0.3402328   0.09755455]\n",
      "learn time: 13235.124658584595\n",
      "213000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.06225289  0.02102875 -0.01499856  0.44546717  0.00801008  0.35806203\n",
      "  0.0465805   0.00910738  0.10263255  0.11777354], error = [-0.06225289 -0.02102875  0.01499856  0.55453283 -0.00801008 -0.35806203\n",
      " -0.0465805  -0.00910738 -0.10263255 -0.11777354]\n",
      "learn time: 13297.586450338364\n",
      "214000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.07178636 -0.17173781  0.10706673  0.06925365  0.56362574  0.00198609\n",
      "  0.00520777 -0.07324524 -0.15769766  0.63034976], error = [-0.07178636  0.17173781 -0.10706673 -0.06925365  0.43637426 -0.00198609\n",
      " -0.00520777  0.07324524  0.15769766 -0.63034976]\n",
      "learn time: 13360.206510543823\n",
      "215000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.03328682  0.90270695  0.02911259  0.03131563 -0.06824454 -0.02154362\n",
      " -0.0655845  -0.03241134  0.02868893  0.04670265], error = [-0.03328682  0.09729305 -0.02911259 -0.03131563  0.06824454  0.02154362\n",
      "  0.0655845   0.03241134 -0.02868893 -0.04670265]\n",
      "learn time: 13422.618517875671\n",
      "216000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.03552277  0.17404496 -0.17481737  0.01603788  0.44354864  0.09530622\n",
      "  0.12092137 -0.11876992  0.08887458  0.47642725], error = [-0.03552277 -0.17404496  0.17481737 -0.01603788 -0.44354864 -0.09530622\n",
      " -0.12092137  0.11876992 -0.08887458  0.52357275]\n",
      "learn time: 13485.033036470413\n",
      "217000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.06314755 -0.04174291  0.10713763  0.12556347  0.70616604  0.05532138\n",
      "  0.12556815 -0.12639477  0.01314818  0.05213294], error = [-0.06314755  0.04174291 -0.10713763 -0.12556347  0.29383396 -0.05532138\n",
      " -0.12556815  0.12639477 -0.01314818 -0.05213294]\n",
      "learn time: 13547.504822969437\n",
      "218000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.04471807  0.04163373 -0.00807055  0.08964918  0.06547877  0.05480513\n",
      " -0.0136804   0.1043276   0.46174205  0.15162449], error = [-0.04471807 -0.04163373  0.00807055 -0.08964918 -0.06547877 -0.05480513\n",
      "  0.0136804  -0.1043276   0.53825795 -0.15162449]\n",
      "learn time: 13610.05994606018\n",
      "219000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.04658996 -0.01405128  0.00099059  0.15097409  0.42951366 -0.03068152\n",
      "  0.0943168   0.00670153  0.01631891  0.40103644], error = [-0.04658996  0.01405128 -0.00099059 -0.15097409 -0.42951366  0.03068152\n",
      " -0.0943168  -0.00670153 -0.01631891  0.59896356]\n",
      "learn time: 13672.537319660187\n",
      "220000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.04250153  0.10081601 -0.09104132  0.02757375 -0.05502933 -0.00138583\n",
      "  0.0060769   0.84779099  0.1136602   0.17939103], error = [-0.04250153 -0.10081601  0.09104132 -0.02757375  0.05502933  0.00138583\n",
      " -0.0060769   0.15220901 -0.1136602  -0.17939103]\n",
      "learn time: 13734.912569761276\n",
      "221000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.04310322 -0.1897159  -0.08051317  0.06665898  0.53511614  0.37913679\n",
      "  0.04378294  0.01587969  0.05770132 -0.02357147], error = [-0.04310322  0.1897159   0.08051317 -0.06665898 -0.53511614  0.62086321\n",
      " -0.04378294 -0.01587969 -0.05770132  0.02357147]\n",
      "learn time: 13797.32585644722\n",
      "222000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.04923135  1.04750992 -0.00583125 -0.00502314  0.03925032  0.01586012\n",
      " -0.00685323 -0.0395385   0.05456293  0.05654994], error = [-0.04923135 -0.04750992  0.00583125  0.00502314 -0.03925032 -0.01586012\n",
      "  0.00685323  0.0395385  -0.05456293 -0.05654994]\n",
      "learn time: 13859.617183685303\n",
      "223000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.04454596 -0.07870123  0.04405411  0.11919306 -0.05549734  0.0219012\n",
      " -0.28291477  0.14487996 -0.03630752  0.56822857], error = [-0.04454596  0.07870123 -0.04405411 -0.11919306  0.05549734 -0.0219012\n",
      "  0.28291477 -0.14487996  0.03630752  0.43177143]\n",
      "learn time: 13922.021889209747\n",
      "224000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.63894956  0.06431635 -0.08727501  0.14278505 -0.02485763 -0.01315397\n",
      " -0.04858648  0.0404857   0.04280184 -0.01491319], error = [ 0.36105044 -0.06431635  0.08727501 -0.14278505  0.02485763  0.01315397\n",
      "  0.04858648 -0.0404857  -0.04280184  0.01491319]\n",
      "learn time: 13984.46016573906\n",
      "225000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.03100983  0.03759478  0.02946149  0.62287116  0.01266562  0.14835633\n",
      "  0.05986902 -0.10397821  0.2745244  -0.02708688], error = [-0.03100983 -0.03759478 -0.02946149  0.37712884 -0.01266562 -0.14835633\n",
      " -0.05986902  0.10397821 -0.2745244   0.02708688]\n",
      "learn time: 14046.8831782341\n",
      "226000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.03117232  0.03408156  0.14054804 -0.1206064   0.114017   -0.02869502\n",
      " -0.07799864 -0.07373006  0.05257668  0.71726875], error = [-0.03117232 -0.03408156 -0.14054804  0.1206064  -0.114017    0.02869502\n",
      "  0.07799864  0.07373006 -0.05257668  0.28273125]\n",
      "learn time: 14109.297515392303\n",
      "227000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.57247972 -0.00428357 -0.0325485   0.09040691 -0.13315195  0.02964072\n",
      "  0.03037935 -0.041173    0.07504068  0.09110336], error = [ 0.42752028  0.00428357  0.0325485  -0.09040691  0.13315195 -0.02964072\n",
      " -0.03037935  0.041173   -0.07504068 -0.09110336]\n",
      "learn time: 14171.721825838089\n",
      "228000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [0.03973468 0.02850018 0.09793435 0.0600046  0.49396165 0.27412684\n",
      " 0.21694142 0.05287044 0.04000533 0.01765924], error = [-0.03973468 -0.02850018 -0.09793435 -0.0600046   0.50603835 -0.27412684\n",
      " -0.21694142 -0.05287044 -0.04000533 -0.01765924]\n",
      "learn time: 14234.975599765778\n",
      "229000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.03687617  0.00454268 -0.05067437  0.0502646   0.68786288  0.14911443\n",
      "  0.10618508  0.01785854  0.02262308  0.11606487], error = [-0.03687617 -0.00454268  0.05067437 -0.0502646   0.31213712 -0.14911443\n",
      " -0.10618508 -0.01785854 -0.02262308 -0.11606487]\n",
      "learn time: 14300.71145606041\n",
      "230000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.03170669  0.14736993  0.37021917  0.49053466  0.07997915  0.11533739\n",
      " -0.03094174 -0.14432145  0.22600954 -0.30069187], error = [-0.03170669 -0.14736993 -0.37021917  0.50946534 -0.07997915 -0.11533739\n",
      "  0.03094174  0.14432145 -0.22600954  0.30069187]\n",
      "learn time: 14363.163653612137\n",
      "231000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.09744515 -0.07452382 -0.11789613 -0.00107555 -0.14633845  0.06259941\n",
      "  0.07071112  0.95405299 -0.03257691  0.21046218], error = [-0.09744515  0.07452382  0.11789613  0.00107555  0.14633845 -0.06259941\n",
      " -0.07071112  0.04594701  0.03257691 -0.21046218]\n",
      "learn time: 14425.592523813248\n",
      "232000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.02077326  0.03957108 -0.09043729  0.03951655  0.0055308   0.01937992\n",
      "  0.78462032  0.11122606  0.10524844 -0.00550577], error = [-0.02077326 -0.03957108  0.09043729 -0.03951655 -0.0055308  -0.01937992\n",
      "  0.21537968 -0.11122606 -0.10524844  0.00550577]\n",
      "learn time: 14487.94728589058\n",
      "233000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.19118252 -0.01879304  0.07932124 -0.08017414 -0.01076209  0.60249203\n",
      "  0.00597332 -0.09730542  0.06605947 -0.05622028], error = [-0.19118252  0.01879304 -0.07932124  0.08017414  0.01076209  0.39750797\n",
      " -0.00597332  0.09730542 -0.06605947  0.05622028]\n",
      "learn time: 14550.318176984787\n",
      "234000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.13797365 -0.01170358  0.07502178  0.28749095  0.18536003  0.57480927\n",
      "  0.00159632 -0.01030055  0.04530124  0.2176944 ], error = [-0.13797365  0.01170358 -0.07502178 -0.28749095 -0.18536003  0.42519073\n",
      " -0.00159632  0.01030055 -0.04530124 -0.2176944 ]\n",
      "learn time: 14612.757027387619\n",
      "235000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.05571664  0.81441172 -0.03550296  0.07052338  0.06778     0.03317655\n",
      " -0.0202904   0.08098267  0.07515757 -0.00676109], error = [-0.05571664  0.18558828  0.03550296 -0.07052338 -0.06778    -0.03317655\n",
      "  0.0202904  -0.08098267 -0.07515757  0.00676109]\n",
      "learn time: 14675.234292268753\n",
      "236000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.04900257  0.92263958 -0.02423196  0.09356761  0.08553698  0.06761461\n",
      " -0.07233302  0.10402597 -0.0155003   0.03587173], error = [-0.04900257  0.07736042  0.02423196 -0.09356761 -0.08553698 -0.06761461\n",
      "  0.07233302 -0.10402597  0.0155003  -0.03587173]\n",
      "learn time: 14737.639585494995\n",
      "237000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.75138904  0.14646488 -0.08030283 -0.08908401 -0.05562826 -0.02048338\n",
      " -0.07121969 -0.06762033  0.01828357  0.14229715], error = [ 0.24861096 -0.14646488  0.08030283  0.08908401  0.05562826  0.02048338\n",
      "  0.07121969  0.06762033 -0.01828357 -0.14229715]\n",
      "learn time: 14800.098755121231\n",
      "238000: expected = [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.05250899 -0.14046843  0.65823634  0.1511155  -0.01652986 -0.20358975\n",
      " -0.16053719  0.06672096  0.31537426 -0.00659583], error = [-0.05250899  0.14046843  0.34176366 -0.1511155   0.01652986  0.20358975\n",
      "  0.16053719 -0.06672096 -0.31537426  0.00659583]\n",
      "learn time: 14862.51376748085\n",
      "239000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.03846033 -0.09517577 -0.04748791 -0.01372835 -0.02880257 -0.03853663\n",
      "  1.01840649  0.02017621 -0.09657998 -0.03081547], error = [-0.03846033  0.09517577  0.04748791  0.01372835  0.02880257  0.03853663\n",
      " -0.01840649 -0.02017621  0.09657998  0.03081547]\n",
      "learn time: 14924.99383354187\n",
      "240000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.11434869 -0.02345346  0.4018353   0.34649201 -0.09102015  0.58388305\n",
      " -0.15797645 -0.00295778 -0.13718062 -0.16832979], error = [-0.11434869  0.02345346 -0.4018353  -0.34649201  0.09102015  0.41611695\n",
      "  0.15797645  0.00295778  0.13718062  0.16832979]\n",
      "learn time: 14987.453358650208\n",
      "241000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.40521432 -0.01689123 -0.00738576 -0.04776455 -0.00136247  0.19546977\n",
      "  0.01340583 -0.07513926  0.15487084 -0.00429767], error = [ 0.59478568  0.01689123  0.00738576  0.04776455  0.00136247 -0.19546977\n",
      " -0.01340583  0.07513926 -0.15487084  0.00429767]\n",
      "learn time: 15049.848084688187\n",
      "242000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.16110893  0.02811886  0.11037949  0.01827093 -0.07415231  0.45125091\n",
      "  0.2803445  -0.16325816 -0.17558802  0.01058166], error = [-0.16110893 -0.02811886 -0.11037949 -0.01827093  0.07415231  0.54874909\n",
      " -0.2803445   0.16325816  0.17558802 -0.01058166]\n",
      "learn time: 15112.203719854355\n",
      "243000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.38746255  0.07568687  0.01017627 -0.05742951  0.29703475  0.02505805\n",
      "  0.10429154 -0.01025014  0.14786445  0.17907073], error = [-0.38746255 -0.07568687 -0.01017627  0.05742951 -0.29703475 -0.02505805\n",
      " -0.10429154  0.01025014 -0.14786445  0.82092927]\n",
      "learn time: 15174.64000248909\n",
      "244000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.03116473 -0.0609447   0.23687349  0.25199746 -0.10930375 -0.14016536\n",
      " -0.07173529  0.8552739  -0.05578724  0.03567518], error = [-0.03116473  0.0609447  -0.23687349 -0.25199746  0.10930375  0.14016536\n",
      "  0.07173529  0.1447261   0.05578724 -0.03567518]\n",
      "learn time: 15237.044730424881\n",
      "245000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.04840675  0.02478836  0.04846135  0.0518839  -0.09055151 -0.14176343\n",
      "  0.09748695  0.39069289  0.26022607  0.13244651], error = [-0.04840675 -0.02478836 -0.04846135 -0.0518839   0.09055151  0.14176343\n",
      " -0.09748695  0.60930711 -0.26022607 -0.13244651]\n",
      "learn time: 15299.48696064949\n",
      "246000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.05625009 -0.08159058 -0.04933943  0.08554782  0.00774635  0.09882986\n",
      "  0.95119968 -0.0857748  -0.06664765  0.06174206], error = [-0.05625009  0.08159058  0.04933943 -0.08554782 -0.00774635 -0.09882986\n",
      "  0.04880032  0.0857748   0.06664765 -0.06174206]\n",
      "learn time: 15361.918212652206\n",
      "247000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.02438468  0.15160332 -0.00365078  0.1559321   0.04301274  0.22497628\n",
      " -0.03435212  0.01574557  0.48464683  0.10226741], error = [-0.02438468 -0.15160332  0.00365078 -0.1559321  -0.04301274 -0.22497628\n",
      "  0.03435212 -0.01574557  0.51535317 -0.10226741]\n",
      "learn time: 15424.34067082405\n",
      "248000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.73734363 -0.00500699 -0.04339531 -0.0283722  -0.0274139  -0.04971058\n",
      "  0.05282641 -0.11805709 -0.01306148  0.12357175], error = [ 0.26265637  0.00500699  0.04339531  0.0283722   0.0274139   0.04971058\n",
      " -0.05282641  0.11805709  0.01306148 -0.12357175]\n",
      "learn time: 15486.811293125153\n",
      "249000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 9.47838639e-02  6.90033426e-02 -1.98080698e-04  3.09057511e-02\n",
      "  5.21494090e-03 -1.03025391e-01  6.82927358e-01 -1.76433596e-01\n",
      "  3.34770748e-01 -1.04081749e-01], error = [-9.47838639e-02 -6.90033426e-02  1.98080698e-04 -3.09057511e-02\n",
      " -5.21494090e-03  1.03025391e-01  3.17072642e-01  1.76433596e-01\n",
      " -3.34770748e-01  1.04081749e-01]\n",
      "learn time: 15549.341500997543\n",
      "250000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.06475531 -0.04949345  0.08449081  1.07242164  0.04092681  0.09192158\n",
      "  0.08702323  0.05175064 -0.13023904 -0.00595407], error = [-0.06475531  0.04949345 -0.08449081 -0.07242164 -0.04092681 -0.09192158\n",
      " -0.08702323 -0.05175064  0.13023904  0.00595407]\n",
      "learn time: 15611.8394947052\n",
      "251000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.02330253  0.01855445 -0.21177805 -0.05777414  0.01756974 -0.10151861\n",
      "  1.03217018 -0.03778586  0.05045525  0.04940204], error = [-0.02330253 -0.01855445  0.21177805  0.05777414 -0.01756974  0.10151861\n",
      " -0.03217018  0.03778586 -0.05045525 -0.04940204]\n",
      "learn time: 15674.31060743332\n",
      "252000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.05253866 -0.1997798   0.15296004  0.16664251  0.02545472 -0.08088747\n",
      " -0.04293938  0.66271636  0.08980596 -0.04718169], error = [-0.05253866  0.1997798  -0.15296004 -0.16664251 -0.02545472  0.08088747\n",
      "  0.04293938  0.33728364 -0.08980596  0.04718169]\n",
      "learn time: 15736.759791135788\n",
      "253000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.09069003  0.03394061 -0.07699923 -0.30659035 -0.06743864  0.27038141\n",
      " -0.02531419  0.85282586 -0.03906844  0.14202186], error = [-0.09069003 -0.03394061  0.07699923  0.30659035  0.06743864 -0.27038141\n",
      "  0.02531419  0.14717414  0.03906844 -0.14202186]\n",
      "learn time: 15799.160514831543\n",
      "254000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.02342189  1.0436757  -0.01198974 -0.03583863  0.01541967 -0.06605924\n",
      " -0.05111309  0.00494106 -0.08417297  0.21246296], error = [-0.02342189 -0.0436757   0.01198974  0.03583863 -0.01541967  0.06605924\n",
      "  0.05111309 -0.00494106  0.08417297 -0.21246296]\n",
      "learn time: 15861.65561056137\n",
      "255000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.12435343 -0.11769052  0.02116529  0.13908395  0.19205352  0.57281372\n",
      "  0.18752052 -0.0863884  -0.11823152  0.36044388], error = [-0.12435343  0.11769052 -0.02116529 -0.13908395 -0.19205352  0.42718628\n",
      " -0.18752052  0.0863884   0.11823152 -0.36044388]\n",
      "learn time: 15929.33128118515\n",
      "256000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.03230146 -0.09343552  0.03305175  0.02438611 -0.01573654  0.02657866\n",
      "  0.01345054 -0.04214695  0.96810838 -0.03190857], error = [-0.03230146  0.09343552 -0.03305175 -0.02438611  0.01573654 -0.02657866\n",
      " -0.01345054  0.04214695  0.03189162  0.03190857]\n",
      "learn time: 15995.112489938736\n",
      "257000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.3176428   0.0368602   0.1242091   0.45439338  0.06202845  0.18742986\n",
      " -0.0865115   0.11461212  0.06406054 -0.036195  ], error = [-0.3176428  -0.0368602  -0.1242091   0.54560662 -0.06202845 -0.18742986\n",
      "  0.0865115  -0.11461212 -0.06406054  0.036195  ]\n",
      "learn time: 16058.66475367546\n",
      "258000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.06078599 -0.02773748  0.13844998  0.02783108  0.58901539 -0.18178005\n",
      "  0.05570747 -0.03060553 -0.08707915  0.26211096], error = [-0.06078599  0.02773748 -0.13844998 -0.02783108  0.41098461  0.18178005\n",
      " -0.05570747  0.03060553  0.08707915 -0.26211096]\n",
      "learn time: 16122.15717959404\n",
      "259000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.0242692  -0.00799714  0.18498773  0.02844698 -0.01120747 -0.04901308\n",
      " -0.06590267  0.1426308   0.69290958 -0.05506912], error = [-0.0242692   0.00799714 -0.18498773 -0.02844698  0.01120747  0.04901308\n",
      "  0.06590267 -0.1426308   0.30709042  0.05506912]\n",
      "learn time: 16185.406729698181\n",
      "260000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.24552164 -0.0329725  -0.05471471 -0.09262241  0.16250813  0.86114962\n",
      " -0.10393287 -0.04660577  0.22949123  0.03002157], error = [-0.24552164  0.0329725   0.05471471  0.09262241 -0.16250813  0.13885038\n",
      "  0.10393287  0.04660577 -0.22949123 -0.03002157]\n",
      "learn time: 16248.714991092682\n",
      "261000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [4.50367381e-02 1.94855864e-02 1.47105438e-01 2.33507377e-02\n",
      " 6.67996360e-05 1.81377518e-02 1.40048880e-02 6.29069253e-01\n",
      " 1.16537557e-01 4.35445627e-01], error = [-4.50367381e-02 -1.94855864e-02 -1.47105438e-01 -2.33507377e-02\n",
      " -6.67996360e-05 -1.81377518e-02 -1.40048880e-02  3.70930747e-01\n",
      " -1.16537557e-01 -4.35445627e-01]\n",
      "learn time: 16312.010672092438\n",
      "262000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.03075467  0.59394069 -0.12510141  0.56460228 -0.00730383 -0.0803764\n",
      " -0.02196295  0.1477024  -0.1131123   0.19944494], error = [-0.03075467 -0.59394069  0.12510141  0.43539772  0.00730383  0.0803764\n",
      "  0.02196295 -0.1477024   0.1131123  -0.19944494]\n",
      "learn time: 16374.728813648224\n",
      "263000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.06414609 -0.04378216 -0.1815926   0.03740437 -0.11559081  0.0709635\n",
      " -0.02537311  1.09601325 -0.01966987 -0.08351298], error = [-0.06414609  0.04378216  0.1815926  -0.03740437  0.11559081 -0.0709635\n",
      "  0.02537311 -0.09601325  0.01966987  0.08351298]\n",
      "learn time: 16437.3429312706\n",
      "264000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 3.32326645e-02 -9.43354078e-02  2.30367945e-01  9.76278456e-02\n",
      " -8.64842527e-03  5.86431566e-02 -2.86496874e-02 -1.73635037e-04\n",
      "  6.82775027e-01 -9.26699524e-03], error = [-3.32326645e-02  9.43354078e-02 -2.30367945e-01 -9.76278456e-02\n",
      "  8.64842527e-03 -5.86431566e-02  2.86496874e-02  1.73635037e-04\n",
      "  3.17224973e-01  9.26699524e-03]\n",
      "learn time: 16499.852405548096\n",
      "265000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.01624962  0.16850768  0.07468428  0.25295068  0.02776376 -0.1033157\n",
      "  0.00327931  0.31167228  0.17086596  0.11687757], error = [-0.01624962 -0.16850768 -0.07468428  0.74704932 -0.02776376  0.1033157\n",
      " -0.00327931 -0.31167228 -0.17086596 -0.11687757]\n",
      "learn time: 16562.412212848663\n",
      "266000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.01683527  0.00843113  0.03340781  0.00513313  0.64548831  0.03559609\n",
      "  0.03409441 -0.0143207   0.06349691  0.2481608 ], error = [-0.01683527 -0.00843113 -0.03340781 -0.00513313  0.35451169 -0.03559609\n",
      " -0.03409441  0.0143207  -0.06349691 -0.2481608 ]\n",
      "learn time: 16624.971665143967\n",
      "267000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.20097256  0.0228793  -0.02020643  0.06307298  0.26083669  0.50001209\n",
      " -0.0511604  -0.05923054  0.1896224   0.18380133], error = [-0.20097256 -0.0228793   0.02020643 -0.06307298 -0.26083669  0.49998791\n",
      "  0.0511604   0.05923054 -0.1896224  -0.18380133]\n",
      "learn time: 16687.509389400482\n",
      "268000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.02316844  0.80599601 -0.05003391  0.00575615 -0.06551018  0.08335355\n",
      " -0.03245367 -0.00607331  0.10933805 -0.03225572], error = [-0.02316844  0.19400399  0.05003391 -0.00575615  0.06551018 -0.08335355\n",
      "  0.03245367  0.00607331 -0.10933805  0.03225572]\n",
      "learn time: 16750.020423173904\n",
      "269000: expected = [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.], gained = [0.04110697 0.07287177 0.6357852  0.16480516 0.06079035 0.11031085\n",
      " 0.10492783 0.00078878 0.11550007 0.01966517], error = [-0.04110697 -0.07287177  0.3642148  -0.16480516 -0.06079035 -0.11031085\n",
      " -0.10492783 -0.00078878 -0.11550007 -0.01966517]\n",
      "learn time: 16812.4704310894\n",
      "270000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.04880417  0.02751003 -0.18195413  0.89099894  0.04322825 -0.09064679\n",
      "  0.09811752  0.11912505  0.02525783  0.14267428], error = [-0.04880417 -0.02751003  0.18195413  0.10900106 -0.04322825  0.09064679\n",
      " -0.09811752 -0.11912505 -0.02525783 -0.14267428]\n",
      "learn time: 16875.055418729782\n",
      "271000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.21714891 -0.09741072  0.12831878 -0.14207575  0.01655221  0.72488926\n",
      "  0.3506957   0.04464667  0.11481171 -0.07203058], error = [-0.21714891  0.09741072 -0.12831878  0.14207575 -0.01655221 -0.72488926\n",
      "  0.6493043  -0.04464667 -0.11481171  0.07203058]\n",
      "learn time: 16937.58383822441\n",
      "272000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.03440099  0.00375716  0.15445574  0.04737506  0.13026618  0.08070756\n",
      "  0.02360224 -0.03966464  0.69889746 -0.09098624], error = [-0.03440099 -0.00375716 -0.15445574 -0.04737506 -0.13026618 -0.08070756\n",
      " -0.02360224  0.03966464  0.30110254  0.09098624]\n",
      "learn time: 17000.155072450638\n",
      "273000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.0446444   0.01318033 -0.00851373  0.47020891  0.00623555  0.37188618\n",
      "  0.03572237  0.00075227  0.09710322  0.11464318], error = [-0.0446444  -0.01318033  0.00851373  0.52979109 -0.00623555 -0.37188618\n",
      " -0.03572237 -0.00075227 -0.09710322 -0.11464318]\n",
      "learn time: 17062.690631628036\n",
      "274000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.04799651 -0.16885918  0.09286961  0.0608989   0.56856395  0.02907155\n",
      " -0.02311904 -0.03590563 -0.13639057  0.59862345], error = [-0.04799651  0.16885918 -0.09286961 -0.0608989   0.43143605 -0.02907155\n",
      "  0.02311904  0.03590563  0.13639057 -0.59862345]\n",
      "learn time: 17125.335202217102\n",
      "275000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.02070965  0.9550867   0.00817376  0.01621997 -0.0842566  -0.03185173\n",
      " -0.05422545 -0.03350718  0.03217095  0.03746667], error = [-0.02070965  0.0449133  -0.00817376 -0.01621997  0.0842566   0.03185173\n",
      "  0.05422545  0.03350718 -0.03217095 -0.03746667]\n",
      "learn time: 17187.873269319534\n",
      "276000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.02479888  0.12666698 -0.15153643  0.02554724  0.44458602  0.11729061\n",
      "  0.11668742 -0.13171643  0.03284156  0.53415421], error = [-0.02479888 -0.12666698  0.15153643 -0.02554724 -0.44458602 -0.11729061\n",
      " -0.11668742  0.13171643 -0.03284156  0.46584579]\n",
      "learn time: 17250.457892417908\n",
      "277000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.04342849 -0.02725205  0.11903132  0.10105914  0.74171498  0.09232761\n",
      "  0.09226192 -0.07826494  0.02426908  0.02568551], error = [-0.04342849  0.02725205 -0.11903132 -0.10105914  0.25828502 -0.09232761\n",
      " -0.09226192  0.07826494 -0.02426908 -0.02568551]\n",
      "learn time: 17313.03192591667\n",
      "278000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.03264461  0.01219899 -0.00556065  0.08928246  0.06529516  0.04729137\n",
      " -0.02042299  0.09307579  0.51503921  0.16743328], error = [-0.03264461 -0.01219899  0.00556065 -0.08928246 -0.06529516 -0.04729137\n",
      "  0.02042299 -0.09307579  0.48496079 -0.16743328]\n",
      "learn time: 17375.58770251274\n",
      "279000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.030553   -0.02617034 -0.00088314  0.14556362  0.44877199 -0.03383602\n",
      "  0.08171343 -0.0086337   0.02245696  0.38040011], error = [-0.030553    0.02617034  0.00088314 -0.14556362 -0.44877199  0.03383602\n",
      " -0.08171343  0.0086337  -0.02245696  0.61959989]\n",
      "learn time: 17438.181152820587\n",
      "280000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.02962501  0.10416948 -0.06665908  0.01413151 -0.02119128  0.01465758\n",
      "  0.04706407  0.93348549  0.08430218  0.14109948], error = [-0.02962501 -0.10416948  0.06665908 -0.01413151  0.02119128 -0.01465758\n",
      " -0.04706407  0.06651451 -0.08430218 -0.14109948]\n",
      "learn time: 17500.746390342712\n",
      "281000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.0324332  -0.16837206 -0.07605654  0.07176083  0.5523029   0.38685577\n",
      "  0.02597372  0.02415421  0.07588316 -0.02844015], error = [-0.0324332   0.16837206  0.07605654 -0.07176083 -0.5523029   0.61314423\n",
      " -0.02597372 -0.02415421 -0.07588316  0.02844015]\n",
      "learn time: 17563.27878189087\n",
      "282000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.0326202   1.06181707 -0.01650417  0.00855375  0.03554096  0.01064712\n",
      " -0.0192289  -0.06182251  0.06482232  0.03938951], error = [-0.0326202  -0.06181707  0.01650417 -0.00855375 -0.03554096 -0.01064712\n",
      "  0.0192289   0.06182251 -0.06482232 -0.03938951]\n",
      "learn time: 17625.74621629715\n",
      "283000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.03054166 -0.0824564   0.05379597  0.1395392  -0.06522575  0.02718564\n",
      " -0.26315239  0.13907171 -0.08060559  0.62681803], error = [-0.03054166  0.0824564  -0.05379597 -0.1395392   0.06522575 -0.02718564\n",
      "  0.26315239 -0.13907171  0.08060559  0.37318197]\n",
      "learn time: 17688.256073474884\n",
      "284000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.70222463  0.06643812 -0.07137723  0.09799275 -0.01916869 -0.01578471\n",
      " -0.07848296  0.03857678  0.03143079 -0.03622673], error = [ 0.29777537 -0.06643812  0.07137723 -0.09799275  0.01916869  0.01578471\n",
      "  0.07848296 -0.03857678 -0.03143079  0.03622673]\n",
      "learn time: 17750.772836208344\n",
      "285000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.02080485  0.02677437  0.02223676  0.65888106  0.03844022  0.14981746\n",
      "  0.0421347  -0.09822208  0.26540627 -0.03050904], error = [-0.02080485 -0.02677437 -0.02223676  0.34111894 -0.03844022 -0.14981746\n",
      " -0.0421347   0.09822208 -0.26540627  0.03050904]\n",
      "learn time: 17813.34806060791\n",
      "286000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.02034571  0.02100197  0.13834892 -0.07021585  0.11887318 -0.02909473\n",
      " -0.0782259  -0.08374651  0.00176156  0.76358853], error = [-0.02034571 -0.02100197 -0.13834892  0.07021585 -0.11887318  0.02909473\n",
      "  0.0782259   0.08374651 -0.00176156  0.23641147]\n",
      "learn time: 17875.925749063492\n",
      "287000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.64467771 -0.01448885 -0.02086822  0.08022401 -0.10787008  0.02340838\n",
      "  0.01852952 -0.05389083  0.10237347  0.06576065], error = [ 0.35532229  0.01448885  0.02086822 -0.08022401  0.10787008 -0.02340838\n",
      " -0.01852952  0.05389083 -0.10237347 -0.06576065]\n",
      "learn time: 17938.522523641586\n",
      "288000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.0282503  -0.00064178  0.1136217   0.06086086  0.53780124  0.27468606\n",
      "  0.21219533  0.06482418  0.03590292 -0.00099661], error = [-0.0282503   0.00064178 -0.1136217  -0.06086086  0.46219876 -0.27468606\n",
      " -0.21219533 -0.06482418 -0.03590292  0.00099661]\n",
      "learn time: 18001.15074658394\n",
      "289000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.02623094  0.03222512 -0.05137179  0.07514945  0.72259453  0.12150794\n",
      "  0.07711657  0.00153079  0.00917391  0.08454785], error = [-0.02623094 -0.03222512  0.05137179 -0.07514945  0.27740547 -0.12150794\n",
      " -0.07711657 -0.00153079 -0.00917391 -0.08454785]\n",
      "learn time: 18063.741520881653\n",
      "290000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.02244576  0.13052379  0.38510296  0.50918798  0.07321002  0.11289495\n",
      " -0.02151404 -0.14357556  0.1913577  -0.2650678 ], error = [-0.02244576 -0.13052379 -0.38510296  0.49081202 -0.07321002 -0.11289495\n",
      "  0.02151404  0.14357556 -0.1913577   0.2650678 ]\n",
      "learn time: 18126.317603111267\n",
      "291000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.07744146 -0.06071546 -0.1062014   0.01365634 -0.15396789  0.07820259\n",
      "  0.08399219  0.94929152 -0.04195825  0.16818943], error = [-0.07744146  0.06071546  0.1062014  -0.01365634  0.15396789 -0.07820259\n",
      " -0.08399219  0.05070848  0.04195825 -0.16818943]\n",
      "learn time: 18188.898978233337\n",
      "292000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.01404859  0.01863517 -0.07804903  0.03735671  0.00255     0.01399582\n",
      "  0.81875631  0.11817601  0.12301692 -0.01888253], error = [-0.01404859 -0.01863517  0.07804903 -0.03735671 -0.00255    -0.01399582\n",
      "  0.18124369 -0.11817601 -0.12301692  0.01888253]\n",
      "learn time: 18251.524117469788\n",
      "293000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.18006041  0.01080003  0.05133485 -0.11242603 -0.0353288   0.69528113\n",
      "  0.0074208  -0.07112131  0.03361504 -0.03962266], error = [-0.18006041 -0.01080003 -0.05133485  0.11242603  0.0353288   0.30471887\n",
      " -0.0074208   0.07112131 -0.03361504  0.03962266]\n",
      "learn time: 18314.13399362564\n",
      "294000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.11595984 -0.00160396  0.04345234  0.25597595  0.13227464  0.58637462\n",
      " -0.00415877 -0.01246571  0.02154885  0.244462  ], error = [-0.11595984  0.00160396 -0.04345234 -0.25597595 -0.13227464  0.41362538\n",
      "  0.00415877  0.01246571 -0.02154885 -0.244462  ]\n",
      "learn time: 18376.731697559357\n",
      "295000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.04100568  0.85435637 -0.04781056  0.06460284  0.0595003   0.01574595\n",
      " -0.01771471  0.07278591  0.05395937 -0.02142097], error = [-0.04100568  0.14564363  0.04781056 -0.06460284 -0.0595003  -0.01574595\n",
      "  0.01771471 -0.07278591 -0.05395937  0.02142097]\n",
      "learn time: 18439.31566309929\n",
      "296000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.03446494  0.92902662 -0.03344451  0.09458746  0.0692543   0.07060081\n",
      " -0.07024815  0.09634399 -0.03445405  0.03269766], error = [-0.03446494  0.07097338  0.03344451 -0.09458746 -0.0692543  -0.07060081\n",
      "  0.07024815 -0.09634399  0.03445405 -0.03269766]\n",
      "learn time: 18501.875314235687\n",
      "297000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.8059761   0.10131301 -0.07965134 -0.06413216 -0.07639725 -0.03663889\n",
      " -0.04773235 -0.07233513  0.06761421  0.12328564], error = [ 0.1940239  -0.10131301  0.07965134  0.06413216  0.07639725  0.03663889\n",
      "  0.04773235  0.07233513 -0.06761421 -0.12328564]\n",
      "learn time: 18564.398678779602\n",
      "298000: expected = [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.03566532 -0.17349917  0.66487185  0.12991351  0.00632622 -0.20248131\n",
      " -0.16786686  0.06077769  0.35102691  0.00908273], error = [-0.03566532  0.17349917  0.33512815 -0.12991351 -0.00632622  0.20248131\n",
      "  0.16786686 -0.06077769 -0.35102691 -0.00908273]\n",
      "learn time: 18626.90421772003\n",
      "299000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.02996814 -0.08793842 -0.02741597 -0.00731818 -0.01340846 -0.03036551\n",
      "  1.01213868  0.00705787 -0.09581324 -0.03667572], error = [-0.02996814  0.08793842  0.02741597  0.00731818  0.01340846  0.03036551\n",
      " -0.01213868 -0.00705787  0.09581324  0.03667572]\n",
      "learn time: 18689.503588438034\n",
      "\n",
      "final learn time: 18752.05394244194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AMats\\NeuralNetworks\\lab_4_convolution\\analysis\\classification_error_analyser.py:117: RuntimeWarning: invalid value encountered in divide\n",
      "  precisions = np.divide(self.__TP.reshape(total), self.__TP.reshape(total) + self.__FP.reshape(total))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learn\n",
      " border = 0.5\n",
      " recall = 0.5660002869048365\n",
      " precision = 0.9396519809699632\n",
      " accuracy = 0.9534733333333332\n",
      " F-score = 0.7064623116071603\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 2 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiEAAAGwCAYAAAB/xbX8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAABlq0lEQVR4nO3dd3xUVfo/8M+dnl4h9NAhJAFCsGMDQVCRqqvuCrvq6n53KXYFVMCGiBVwfyq77OLKuq4UEQQEVHTFgqJAEiAk0lt6nz5zfn9MZpLQkoHJ3Jl7P+/XKy/IzJ2Z50ySM88959zzSEIIASIiIqIg08gdABEREakTkxAiIiKSBZMQIiIikgWTECIiIpIFkxAiIiKSBZMQIiIikgWTECIiIpIFkxAiIiKSBZMQIiIikoVO7gAo9A0dOhTHjx/3fS9JEmJjY5GdnY1nnnkG7du3BwDU1NTgr3/9KzZu3IjS0lK0a9cON998M+6//35ERkY2ec6TJ0/irbfewtdff43q6mp07doVv//97zF27NhgNo2IgqBxHyJJEiIiItCnTx/85S9/wdVXXw0AuPvuu7F9+/azPn7evHno2LEjJk2adM7XGDduHF566aXAB0+tSuK27dScoUOHYvLkybjpppsAAG63G4WFhZg9ezY6dOiA9957D7W1tbjzzjuh1+vx0EMPoVu3bigsLMRrr70GnU6Hf/3rX4iKigIAHDp0CHfddRcGDRqEe++9F0lJSfjuu+/w4osvYvr06bjnnnvkbC4RBVjjPsTtdqOqqgoff/wx/vnPf+Jvf/sbrrzyStx9993IyMg4699/TEwMNBoNqqqqfLcNGTIEixYtQlZWFgDAZDIhJiYmaG2iwOBICLVITEwM2rRp4/s+JSUF06ZNw2OPPYaamhosXLgQdrsdH374oW/Uo1OnTsjOzsbo0aOxePFiPPHEEwCAuXPnom/fvli0aBEkSQIAdOnSBXa7Ha+99homTpyI2NjY4DeSiFpN4z4kJSUFjz/+OEpKSjBv3jysXbsWABAZGdmknznd6ffFxcWd93gKfVwTQhfMYDD4/r9q1SpMmjTpjGmXmJgYTJo0CatWrYLL5cKpU6fw3Xff4fe//70vAfGaOHEilixZcsZzEJEy/eY3v8H+/ftx+PBhuUMhmXAkhC7IkSNH8O677+Lqq69GcXExamtrkZmZedZjs7OzUVlZiSNHjuDIkSMQQpz12IiICAwePLi1QyeiENGjRw8AQGFhocyRkFyYhFCLzJ49G8899xwAwOl0Qq/XY9iwYZg5cyYOHDgAwDM0ejbeqZXKykpUV1cDAOduicjXD9TV1QEA3nnnHSxduvSM43755ZegxkXBwySEWmTatGkYMWIE6urqsGjRIhw/fhyPPPIIEhISEB8fDwAoKSlBamrqGY8tLi4GAMTHx6O2thYAUF1djcTExKDFT0Shx9sfREdHAwDuuOMO3H333XKGREHGNSHUIklJSUhNTUW/fv3w5ptvAgD+/Oc/w+FwIDU1FfHx8cjLyzvrY3NzcxEfH4/OnTsjPT0dkiQhNzf3jOPMZjP+8Ic/YN++fa3aFiIKDfn5+QCAXr16AfCMpqampp7xRcrFJIT8ZjAY8Pzzz2Pv3r345z//CZ1Oh/Hjx+Pvf/+7b1jVq7a2Fv/4xz8wfvx46HQ6JCYm4qqrrsKyZctw+tXhK1euxE8//eTbd4SIlG3lypVIT09H586d5Q6FZMIkhC5I//79MXHiRPz1r39FUVERpkyZguTkZNx9993Ytm0bTpw4gW3btmHSpElo06YNpk6d6nvsjBkzsHv3bkyfPh27d+/GwYMHsXTpUixYsACPPPLIOdeWEFH4qqmpQUlJCYqLi5Gfn48XXngB69evx5NPPuk7xmw2o6Sk5Iwv77QNKQ83K6NmDR06FFOmTMH48eOb3F5eXo6RI0fimmuuwSuvvIK6ujq8++67WL9+PYqKipCSknLOHVMLCgqwaNEi7NixA3V1dejevTv+8Ic/YPTo0cFsGhEFwek7piYmJqJfv37405/+5Lsi7nw7pk6cOBEvvPBCk9v69OmD9957D5dddlnrBk+tikkIERERyYLTMURERCQLJiFEREQkCyYhREREJAsmIURERCQLJiFEREQkCyYhREREJAsmIURERCQLJiFEREQki5CvoltWVoPzbacmSUBSUkyzxymRWtuu1nYDwW+79/XCEfuOs1NruwH1tj2U+42QT0KEQIvetJYep0Rqbbta2w2ou+0txb7j/NTabkC9bQ/FdnM6hoiIiGTBJISIiIhkwSSEiIiIZMEkhIiIiGTBJISIiIhkwSSEiIiIZHHBSYjdbsctt9yCH3744ZzH7NmzB7fddhsGDBiACRMmIDc390JfjoiIiBTmgpIQm82Ghx9+GAUFBec8xmw24/7778fgwYOxatUqZGVl4YEHHoDZbL7gYImIiEg5/E5CCgsLcfvtt+PIkSPnPW79+vUwGo14/PHH0aNHD8yaNQtRUVHYuHHjBQdLREREyuF3ErJ9+3Zcdtll+PDDD8973K5du5CdnQ1JkgAAkiRh0KBB2Llz5wUFSkThj9O4RNSY39u233XXXS06rqSkBD179mxyW1JS0nmncM6mPodp9v7mjlMitbZdre0Ggt/2QL6OzWbDI4880qJp3NGjR+Oll17CBx98gAceeACbN29GZGRk4IIhopDQarVjLBYLDAZDk9sMBgPsdrtfz9PSIjjhWmQrENTadrW2Gwi/thcWFuKRRx6BaKZwReNpXEmSMGvWLHz99dfYuHEjxo8fH6RoiShYWi0JMRqNZyQcdrsdJpPJr+dhJcxzU2vb1dpuwI+2CwE4zNBYy6GxlEOyVkBjLYdkLYfGWgGhj4JlwB8Brb5Fr3exvNO4Dz30EAYOHHjO4843jetvEtLcKI5p74dAx16Q4gb59bzhjiOJ6mt7KI+gtloSkpKSgtLS0ia3lZaWom3btn49DythNk+tbVdNu4WA5KiDZK2A1loOlFthKD4OyZdcVECylHsSDmuFJ9GwVEByn3/U0dE2C46OVwSlCcGexvU87jzJU00R8PkjQFRbJD3m/3MrQbiNpgWSWtseiu1utSRkwIABWLJkCYQQkCQJQgj8/PPP+NOf/tRaL0kU+holFBpreX3y4B2lOD2hqL/tLAlFS7sSoTXCbUqAMCXCHZHo+78zoQcc7bID376LFKhpXOD8o6iS2YwkAKgrRllJJYSk9T/YMMWRRPW1Pdjt9mcENaBJSElJCWJiYmAymTBy5Ei8+uqreOGFF3DHHXfgP//5DywWC0aNGhXIlySSjy+hKD/LaERFk2mQ8yUULX45rRHuiERoo5Jh18fBbUqEMCXAbUqAO8L7/0SIiES4jZ7boIsIq7HnQE3jAucfKROGRh2kvQ7CEOv384c71YwknoVa2x6K7Q5oEjJkyBDMmzcP48ePR3R0NN555x3Mnj0b//3vf9GnTx+8++67XOFOocnfhMJSP2pxkQmFqE8WzptQmDwjGNBFQNJISE6OQXWpMs/kAjWN2yytEUKjh+R2QHLUAipMQohCwUUlIfn5+ef9vn///li9evXFvASR/xonFE2Sh4aEomH6I3AJRUMikQgRkeAbjRD1ScTpCUU4jVAESzCncYU+EpKtCpKDuzgTyaXV1oQQBcRZEgqtrRwoMCOy7JRvxKJpQlEOye24sJdrklB4k4cEX/LgXVvhTTbcpkRAHxHgRquLXNO4Qh8F2Kog2WsD/txE1DJMQih4TksozlyM2fQy0uYSiuYm9phQhAe5pnGFIRoAIDnqAv7cRNQyTEIooDTVR2E8sAHayoN+JRTN8SQUSb5pDUNcW1ikaF/yIExNpz6YUISuUJnGFXpPYsPpGCL5MAmhi6apOQHjr+tgLFwLfdEvzR7fJKHwjUbEnyOhqF9D0SihkCQgOTkGdQpdnEnBIfRRnv9wJIRINkxC6IJo6opgLFwH46/roD/5o+92AQmOjpfD0f4yuCOS6hdjJpwzoSCSjdZz2a/ktMocCJF6MQmhFpPMpTAeWA9jwSfQn/gBEhqGIRztL4W152jYetwMERXgyymJWoGoT4Ylp0XmSIjUi0kInZdkKYfxwAbPVMvxbyEJt+8+R8og2HrdCluPm+GObi9jlET+EzomIURyYxJCZ5CslTAc/Aymwk+gP/oNJOHy3edoOwC2nqNh63EL3LGdZIyS6OL4khAHkxAiuTAJIQCAZKuG4dAmGAvWwnD06yZXsTiS0z2JR8/RcMelyhglUeB4kxBwJIRINkxC1MxeB+OhzTAWroXhyFZILpvvLmdiH89US8/RcMV3lzFIolai48JUIrkxCVEbhwWGw5/DVPgJDIc+b5p4JPT0jXi4EnvLGCRR6+OaECL5MQlRA6cFhiNbYSxYC+OhzU06XWdcV9h63gpbr9FwJfZlPRNSDaHnSAiR3JiEKJXLBsORr2Es/ASGg5ugabQhkyumM2y9RsPW81Y4k9OZeJAqcU0IkfyYhCiJyw7DsW88azwOfAaNvbrhrugOvqkWZ9sBTDyIOB1DJDsmIeHO5YT+yNcwFHwC44EN0NiqGu6KSoGtxy2w9boVzpQsQNLIGChRaBFcmEokOyYh4cjtgv7E9zAWrgUObkCcuazhrog2sPW8Cbaet8LR/hImHkTnwIWpRPJjEhIuhBv6kz/CWPgJjIXrobGU+O5ymxJh63ETbD1Hw9HhckCjlTFQovDgWxPCzcqIZMMkJJQJAV3RzzAWfALjr+ugrSvy3eU2xsHeYxRMg25HeewgCIk/SiK/cCSESHb85Ao1QkBXvAvGwrUwFq6Dtva47y63IQb27iNh63EL7J2vhqQzwJQcA5TWACxpT+QXFrAjkh+TkFAgBHSlefVTLeugrT7iu8utj4K92wjYeo6Gvcu1gNYoY6BEysGFqUTyYxIiFyGgLd/n2UCscC10VQcb7tJFwNZ1OGy9RsPe5TrfsDERBU6ThalC8LJ1IhkwCQkybXlB/VTLWugqCny3C60R9q7DYOsxGrauwwB9pIxREqlA4+TeZWWyTyQDJiFBoK080JB4lO3z3S40BthTr4et5y2wdx0OYYiWMUoidfFOxwCeKRnBJIQo6JiEtBJN1WHf4lJ9aa7vdqHRw975Gs8aj24jIIyxMkZJpGIaHaA1AC47JIcFwpQgd0REqsMkJMA01ccQu2Uq9Cd/9N0mJC0cnYd4plq6j4QwxcsXIBE10EV4khAXF6cSyYFJSCA5LIjdcC/0pXkQkgaOjlfC1vMW2LrfBBGRKHd0RHQ6fQRgq4LkMMsdCZEqMQkJFCEQ89UM6Evz4I5IQsWET+COS5U7KiI6H723ki5HQojkwMIiAWLKfQ+m/BUQkgbVI/7KBIQoHNRfhcYNy4jkwSQkAHQnf0L0N7MBAHVXzIKj01UyR0RELaLnhmVEcmIScpGkumLEbnwAktsJa8/RsAy8X+6QiKilOBJCJCsmIRfD5UDsZ/8HrbkIzoTeqLn+Fe66SBROfGtCmIQQyYFJyEWI+vZ5GE7+ALchBtU3/Q0wRMkdEhH5g0XsiGTFJOQCGfevRuTuvwMAaoa9AVd8d5kjIiK/+erHcE0IkRyYhFwAbekexHz5GACgLnsa7N1vlDkiIrogHAkhkhWTED9J1krEbfgjJKcV9i7XwnzpI3KHREQXigtTiWTFJMQfwo2YLdOgrT4MV0xnVA9fDGi0ckdFRBfKuzDVwSSESA5MQvwQ+ePrMB7+AkJrRPWoJSx4RRTuvPuEsHYMkSyYhLSQ4dAWRP34OgCg5rr5cLbJkDkiIrpo3ukYjoQQyYJJSAtoKg8iZvM0AIAlczJsfSfKHBERBQQXphLJiklIcxxmxG24Dxp7NRztBqP2qtlyR0REgeJbmMrpGCI5MAk5HyEQ8+Vj0JXnwx3RBtUj3wa0BrmjIqJA0XnWhLCKLpE8mIScR8Tuv8NUsAZCo0P1yLfhjmond0hEFEi8RJdIVkxCzkF//DtEbXsOAFB35dNwdLhM5oiIKOC4JoRIVkxCzkJTexKxn/0fJOGCtfc4WPrfI3dIRNQamIQQyYpJyOlcNsRufAAaSymcSWmoue5lVsYlUio9a8cQyYlJyGmiv5kLfdHPcBvjUDVqScOOikSkPN6/b46EEMmCSUgjxr3/RUTuexCQUHPDQrjjusodEhG1psYLU4WQORgi9WESUk9XkoOYr2YAAMyXPgx712EyR0REra7+El1JuAG3XeZgiNSHSQgAyVqB2A1/hOSywdb1BpgHT5c7JCIKhvqREIDrQojkwCTE7ULspr9AW3MMrthU1NzwJiDxbSFSBa0eQvJUwuYVMkTBp/pP26gfFsBw9GsIXQSqbvobhDFO7pCIKFgkCUJXvziVReyIgk7VSYjhwAZE/rwYAFBz/QK4ktJkjoiIgs67LoQjIURBp9okRFtRiJgtDwEAzAPug633WHkDIiJZeEdCuCaEKPhUmYRI9lrEbvgjNI5a2DtchrorZskdEhHJRHDXVCLZqDIJif7yMegqCuCKSkH1jW8DWr3cIRGRTDgSQiQf1SUhmuojMBWuhZC0qB75LkRkG7lDIiI51a8J4a6pRMHndxJis9kwc+ZMDB48GEOGDMHSpUvPeezmzZsxatQoZGVl4c4770ReXt5FBRsI+lM/AwCcbTLhbJctczREJDfhW5jKkRCiYPM7CXn55ZeRm5uLZcuWYfbs2Vi8eDE2btx4xnEFBQV45JFH8MADD2DNmjVIS0vDAw88AItF3rMNXZEnCXG0GyRrHEQUGhqmYzgSQhRsfiUhZrMZH330EWbNmoX09HQMHz4c9913H5YvX37Gsdu2bUPPnj0xduxYdOnSBQ8//DBKSkpQWFgYsOAvhG8kJIVJCFGwhPIIKpMQIvn4lYTs27cPTqcTWVlZvtuys7Oxa9cuuN3uJsfGx8ejsLAQO3bsgNvtxqpVqxAdHY0uXboEJvIL4bRCV+rp0DgSQhQ8IT2Cyn1CiGSj8+fgkpISJCQkwGAw+G5LTk6GzWZDZWUlEhMTfbffdNNN+OKLL3DXXXdBq9VCo9HgnXfeQVycfzuSSlLL7m/uOADQleZBcjvgjkiGiO3coseEMn/ariRqbTcQ/LYH4nW8I6hLlixBeno60tPTUVBQgOXLl2PkyJFNjm08ggoADz/8MJYvX47CwkJkZmZefDBn4dsxlWtCiILOryTEYrE0SUAA+L6325tWoKyoqEBJSQmeeeYZDBgwAB988AFmzJiB1atXIykpqcWvmZQUE7jjCjyjIJrOlyC5TWyLYwh1LX2PlEat7QbCq+3nGkF9++234Xa7odE0DMg2HkHNysoKyggqp2OI5ONXEmI0Gs9INrzfm0ymJre/8sor6N27N377298CAJ577jmMGjUKK1euxP3339/i1ywrq4EQ575fkjwdcnPHAUDMge9hBFCX2B+W0poWxxCq/Gm7kqi13UDw2+59vYshxwiqN/YW3d9oszI1jK5xJFF9bQ/lEVS/kpCUlBRUVFTA6XRCp/M8tKSkBCaTCbGxTUcW8vLycPfdd/u+12g06Nu3L06cOOHPS0IItKizbclxuvpFqY6UQYr68Grpe6Q0am03EF5tl2MEFWh58hQZ60lwIrRORCSHzwjTxQqn0bRAU2vbQ7HdfiUhaWlp0Ol02LlzJwYPHgwA2LFjBzIzM5sMqQJA27Zt8euvvza57eDBg602r9scTV0RtDXHICDB2XaALDEQqZEcI6hAy0dRa+0aRAOw1dWgRgEjpM3hSKL62h7KI6h+JSEREREYO3Ys5syZgxdffBHFxcVYunQp5s2bB8AzKhITEwOTyYTbb78dTz75JDIyMpCVlYWPPvoIJ06cwLhx4/xvUQDoin4BALiS+kAYomWJgUiN5BhBBfwYRfUuTHVYVPXBFE6jaYGm1raHYrv9SkIAYMaMGZgzZw4mT56M6OhoTJ06FSNGjAAADBkyBPPmzcP48eNx0003oa6uDu+88w5OnTqFtLQ0LFu2zO8h1UDRFzVMxRBR8IT6CCoXphLJx+8kJCIiAvPnz8f8+fPPuC8/P7/J97fddhtuu+22C48ugHS+TcqymjmSiAIp5EdQuU8IkWz8TkLCktsJffFuABwJIZJDKI+gsooukXxUkYRoy/dDcprhNsTAldhL7nCIVCeUR1CZhBDJx+8CduHIVy+m7UBAUkWTiaiFhN67YyqnY4iCTRWfyHpWziWicxBargkhkosqkhBdERelEtE56L1JCKdjiIJN8UmIZK2ErqIQAOBgEkJEp/GtCXE7AJdD5miI1EXxSYiueBcAwBWbChEhzx4lRBS6fJuVAZBcHA0hCibFJyFcD0JE56U1QqC+4paD60KIgknxSUjjonVERGeQpIYNyzgSQhRUyk5ChPCNhDg5EkJE5+BbF8KREKKgUnQSoq06CI2tCkJrhDMpTe5wiChECW7dTiQLRSchvnoxbTIBrUHmaIgoVPlGQjgdQxRUik5CWDmXiFrCd4UMp2OIgkrRSYiu6BcAvDKGiJqh99aPYRJCFEzKTUIcFuhK9wAAnBwJIaLz4NbtRPJQbBKiL9kNSbjgikqBO7q93OEQUQhjJV0ieSg2CfEtSk3J8uwDQER0DoLTMUSyUGwSwkWpRNRS3kt0wZEQoqBSbBKi4yZlRNRS3CeESBaKTEI0tSegrSuCkLRwtOkvdzhEFOK4JoRIHopMQnzrQZLSAH2kzNEQUahrSEI4EkIUTIpMQvT1+4M4U7JkjoSIwgGTECJ5KDQJqV+UyvUgRNQCvoWp3DGVKKiUl4S47NAV7wbATcqIqIVYO4ZIFopLQnRleyG5bHAb4+CK7yZ3OEQUBnzTMRwJIQoq5SUhjdeDSIprHhG1Aq4JIZKH4j6lvfViHG0HyBwJEYULwX1CiGShuCREY6sEALgj28obCBGFDe9ICHdMJQouxSUhkq0aACCMsTJHQkRhg7VjiGSh3CTEwCSEiFqGa0KI5KG4JERjqwIAuI1xMkdCROFCaL1rQjgdQxRMiktCJDunY4jIP8I7HeOyAW6XzNEQqYeykhAhGk3HxMgcDBGFC2GI9v1fstfIGAmRuigrCXGYIQnPWQynY4ioxbTGhnUh9VO6RNT6FJWEaOyezkNodL5tmImIWsJtigfQcJk/EbU+RSUhks0zjCoMsYAkyRwNEYUTYYwHAEjWSlnjIFITZSUh9YtS3VyUSkR+4kgIUfApKgnR+DYq43oQIvKPt9/gSAhR8CgqCfEuKONGZUTkL3f9dIyGC1OJgkZZSQj3CCGiCyTqp2M4EkIUPIpKQrzTMW7uEUJ0TsU1Nny+vwQut5A7lJDSMBJSKWscRGqiqCTENx3DNSFEZ3W8yoLJy3/Bk2v3YveJarnDCSkcCSEKPmUlIZyOITqnsjo7pqzIQWmdHT2SI9E3Jbr5B6kIR0KIzu1YpQW/+9fPeGPrgYA+ry6gzyYz7z4hbi5MJWqixurE1JU5OFZpRYc4ExZNyESEXit3WCGF+4QQnV1JrQ1/WZGDE1VWJEXpA/rcihoJ0XAkhOgMVocLD3+ci4KSOiRG6rF4QibaRBvlDivk+KZjeHUMkU+VxYEp9QlIp3gTnh7RO6DPr6gkhGtCiJpyutyYsW4vdh6vRrRRi0UTMtE5gSUNzqbJdIzgol0is92FB1fn4kCZGclRBiyemInkAJ/AKCwJ8V4dw5EQIrcQePaz/fjmQDmMOg1eH5uB3m25DuRcfCMhLhvgtMobDJHM7E43HluTh9yTNYgz6bB4YiY6xgX+BEZRSQinY4g8hBB47ctfsWFvMbQaCfNH98PAThwhPB+hj/IUvwSgsVXIHA2RfJxugafW78P2I5WI0GvwxvgM9EiOapXXUk4SIoRvJIQ7ppLa/f37I/jwlxMAgNkje+Oq7okyRxQGJImLU0n1hBB4cdN+fFlQCr1WwoIx6cho33qfqcpJQpxWSG4HAI6EkLp9tPME3vn2MADg0et7YFRaiswRhQ93/XoyXqZLaiSEwBtfHcDavCJoJOD5m9NwWWpCq76mYpIQb70HIWkg9K0zbEQU6j7bW4wFnxcCAP54RRf8ZlBHmSMKL7xChtTsn9uP4t87jgMAZg3vjaG9klv9NRWThEh2zx4hwhALSJLM0RAF37aD5Zi9MR8CwO0DO+CPV6TKHVLY8V0hw+kYUpkVO0/gr98cAgA8eG133JrZLiivq5wkxLsehJfnkgrtOl6FJz7ZA5db4Ma+bfDI0B6QmIz7jVu3kxp9trcYL9ePoN5zWWf8dnCnoL22cpIQu2f41M31IKQyBSW1eGh1HmxON67sloA5I/tAwwTkgnDrdlKbbQcaRlAnDmiPP13VNaivr5wkhFfGkAodq7Rg6spc1NicGNAhFvNH94NOq5g/66DjSAipyS/HqvDE2oYR1MeG9Qz6CKpieiuNjXuEkLqU1towZUUOyurs6NUmCq+Py4CJ9WAuivfqGC5MJaXLL6rFQ6tzYXO6MaR7omwjqH4nITabDTNnzsTgwYMxZMgQLF269JzH5ufn484770T//v0xevRofP/99xcV7Plwt1RSk2qrA1NX5uJ4lRUd40xYOD4DMSZF1aOUheB0DKnA4XIzpq7MQZ3dhayOsZh3S5psI6h+v+rLL7+M3NxcLFu2DLNnz8bixYuxcePGM46rqanBPffcg549e2Lt2rUYPnw4pkyZgrKysoAEfjrvmhCOhJDSWewuPLQ6D4WldUhqpXoOgRaqJy+n43QMKV1RjWcEtcLiQO82UXhN5hFUv5IQs9mMjz76CLNmzUJ6ejqGDx+O++67D8uXLz/j2NWrVyMyMhJz5sxBamoqpk2bhtTUVOTm5gYs+MYkTseQCjhdbvzf8h3YdbwaMUYdFk/IRKf40C9IF6onL6fjwlRSskqzA1NW7MapGhu6JERg0cRMRBvlHUH169X37dsHp9OJrKws323Z2dl4++234Xa7odE05DTbt2/HsGHDoNU2ZFgrV670O8Dmpqi892u8+4QYY1WzTYi3nWppr5da2+0WAnM25mNrfgmMOg3eGJ+OXm1bd2O+QLzH3pOXJUuWID09Henp6SgoKMDy5csxcuTIJsc2PnnRarWYNm0avvrqK+Tm5uLaa6+9+GCawZEQUqoaqwPTVubgULkFbaM9I6iJkQa5w/IvCSkpKUFCQgIMhobAk5OTYbPZUFlZicTEhvoUR48eRf/+/fH000/jiy++QMeOHfHEE08gOzvbrwCTkmJadJzRXQcAiE5KQXRyyx6jFC19j5RGTe0WQmDOJ3nYuLcEOo2Et+/OxvV92sodVovIcfJyoXwjIY5awOUAtPqgvTZRa7E53Zjy3k/YU1SL+Ag93prYH+1jTXKHBcDPJMRisTRJQAD4vrfb7U1uN5vNePfddzFp0iQsWbIEn376Ke69915s2LAB7du3b/FrlpXVQIhz3y9Jng8jR2059ACqHQbYS2ta/PzhzNv25t4jpVFju9/ZdgjLvjsCCcCrtw9A/+QIlAbh99z7Xl8MOU5evLG35P4mx5kapnM1jmoIXZLfrxvq1DqSCKiz7U63wMx1e/H9gXJEGbRYNCED3ZIjW/U1/Xl//UpCjEbjGcmG93uTqWlWpdVqkZaWhmnTpgEA+vXrh23btmHNmjX405/+1OLXFAIt+qBpfHWMWj6YvFr6HimNWtr94c/HseS7IwCAx4f1xJiBHVFaGj4JmBwnL0DLk6czjjPGAbYqJEU4AAWPqqppJPF0amm72y3w6Ee78FVhGQw6Df42+RJc0SO0Emu/kpCUlBRUVFTA6XRCp/M8tKSkBCaTCbGxTReEtmnTBt27d29yW9euXXHy5MmLDPnsNHZeokvKs2FvEV758lcAwP1XpuK2rA4yR+Q/OU5egJaPop5+XIIxDlpbFSpPHocTwamfEUxqHEn0UlPbhRB45YtfseqXE9BKwFt3DULveEPIjaD6dXVMWloadDoddu7c6bttx44dyMzMbDKvCwADBw5Efn5+k9sOHDiAjh1bp6ond0wlpfnmQBnmbtwPAPhNVgfcd3kXmSO6MI1PXryCcfLiHSk739fZjvOuC5GslS16jnD8aun7o8QvtbR9ybdH8OEvJwAAz4zsg+H9UoL+PreEX0lIREQExo4dizlz5mD37t3YsmULli5dikmTJgHwdCxWqxUAcMcddyA/Px+LFi3C4cOH8eabb+Lo0aMYM2aMPy/ZMg4rJJcNAHiJLinCzmNVeHLtXrjcAqPS2uLh68O3IF0on7ycje8KGVtF0F6TKJD+8/NxvPvdYQDAI9f3wM3pKTJHdG5+b1Y2Y8YMpKenY/LkyZg7dy6mTp2KESNGAACGDBmC9evXAwA6duyIv/3tb/jyyy9xyy234Msvv8S7776LlJRWeDOs9RuVQYIwRAf++YmCaH9xLR76uGE75Wdu7B3WBelC9uTlHLxbt2t4mS6FofV7ivCqdwr3ilTcMSh4CfyF8HuXkoiICMyfPx/z588/477Tz2Cys7OxatWqC4+upRpvVCYpphwOqdDRCgumrsxBrc2FgTJvpxxIM2bMwJw5czB58mRER0efcfIyb948jB8/3nfy8sILL+Ddd99Fjx49Wu/k5Ry8W7ezfgyFm68Ky/DsRs/n8G+yOuC+K0J/ClcZxSa8IyFcD0JhrKTWhikrdqPc7ECvNlF4baxyCtKF5MnLObi5YRmFoR1HKzFz3R64BHBzv/CZwg3/UywAqO8s3FwPQmHKU5AuByeqbegUb8LCCZksSCcTFrGjcLPnVA0e+TgPdpfANT2S8NSN8lTEvRAKSUJYvI7Cl8XhwoOr8vBrqRnJ3oJ0UfJvp6xWHAmhcHKwzIxp9RVxB3eOw4u3pEGnCY8EBFBaEsLpGAozDpcbT3yyBzknqxFr0mHRxEx0jAv9gnRKxpEQChcnq62YsmI3qqxOpKVE45Wx6TDqwutjPbyiPReHBQAgdOy8KXy43AJzNuTju0MVMOk0eH1cBnomt25BOmqeMHmujuHCVApl5WY7pqzIQXGtHV0TI7BwfCaiDOE3hauMJMRdvwmSJvx+AKROnt0MC7Ep31OQ7uUx/dC/A0fyQoGviB2nYyhE1dqcmLoiB0cqLGgXY8Tiif0RHxmexRaVkYS4HAAAwSSEwsQ73x7Gil0nIQGYO6oPruia2OxjKDgaNiurBIRb1liITmd1uPDw6lzsL6lDYqQeiydmIiXGKHdYF0wZSYhvJCQ8M0FSlw9+Po6/f+8pSPfEDT0xom9bmSOixryblUnCDcleK3M0RA2cLjeeXLsXvxyvRpRBi4UTMpGa2LoVcVubMpKQ+pEQaJSxpwIp1/o9RXitfjfD/7uqKyYMCL+CdIqni4DQeQrrSVycSiHCLQTmbMzHtoPlMOo0eGNcBvq0Df8dwpWRhNSPhAiOhFAI+/rXht0M7xzUEX+4rLPMEdG5cF0IhRIhBBZ8XojP9pVAq5Ewf3Q/DOwUJ3dYAaGoJIQLUylU/XysEjPX7fXtZvjgdd3DYjdDtRJGXiFDoePtbYca1pCN7IOruitnDZkykhDfwlSOhFDoyS+uxcOr82BzunF190Q8NSK8C9KpgXfDMo6EkNze/+kYlv5wFADw+LCeuDFNWWvIlJGEuLkmhELTkQqLbzfDrE71uxkqoCCd0jUUsauUNQ5St09yTuHNrw4AAP48pCsmDlTeGjJl9Ia8OoZCUHFNQ0G63m2i8NrYdMUUpFM6joSQ3L4oKMULm/cDAH6b3Qm/v1SZa8iUkYS4vAtTuSaEQkOVxVOQ7mS1DZ3rC9JFG/n7GS44EkJy+uFwBZ76dC/cAhiT0Q7Tr+2m2DVkykhCfNMxHAkh+ZntLjy4OhcHysxoE23A4on9kcSCdGGFSQjJJfdkNR5bkweHS2Bor2TMGN5LsQkIoJQkxLcwlUPdJC+701OQLvdkDeJMOiyakIkOcSa5wyI/cTqG5FBYWofpq3JhcbhxaZd4PHdTX2jDqCLuhVBGEsI1IRQCXG6B2Rvy8f3hCkToNXhjfAZ6sCBdWOJICAXb8SoLpq7IQbXViYz2MVgwJh2GMKuIeyGU0ULuE0IyE0Lg5c8LsWW/pyDdglvTkdGeBenCFUdCKJhKa234y0c5KK2zo0dyJN4Yl4FIgzpG9pWRhHCfEJLZ29sOYdVuz2ZCz93UF5d1TZA7JLoITYrYEbWiaqsDU1fm4niVFR3iTFg0IRNxEer5LFNGEuIbCVFH5kih5d87GjYTenJ4L9zQp43MEdHF4rbtFAwWhwsPrspDYWkdkqIMeGtiJtpEh29F3AuhqCSEIyEUbOvyTuH1rQ2bCY3v317miCgQfNu2u2yA0yJzNKREdqcbj6/Zg5yT1Ygx6rB4QiY6xUfIHVbQKSMJ8VXR5ZoQCp6vCsvw/GeezYTuyu6o2M2E1EgYYiAkz8iqhvVjKMA8i9j34fvDFTDpPIvYe7ZR5yJ2ZSQh3CeEgmzH0UrMXLcHLgHckp6CB69lQTpFkaSG0RBOyVAACSHw0pYCbNlf6lnEPqYf+ndQ7yJ2hSQh3ukYrgmh1revqAaPfJwHu0vg2h5JmDWiNxMQBfJdIcPFqRRAi/93CB/nnIJGAp6/uS8u76qcirgXQhlJiIv7hFBwHC43Y9rKXNTZXRjUKQ4v3JIGncI3E1Ir314hHAmhAFm2/Sje+9GziH3GDb0wrDcXsSsjCXFzTQi1vqIaG6asyEGFxYG+baPx6th0GFWwmZBaca8QCqRVu09i8f8OAgCmXdMNY7mIHYBSkhDfPiFMQqh1VFocmLoiB6dqbOiSEIE3J2SwIJ3C+daEcGEqXaRN+4rx0uYCAMDkSzvj7ku4iN1LGUmI2+X5l0kItYI6uxMPrsrFwXIz2kYbsHhiJhIjWZBO6dzcup0C4NuD5Zi9IR8CwPj+7fGXIV3lDimkKCQJ4Y6p1Dq81/LnnfIUpFs8sT/ax7IgnRoITsfQRdp1vAqPf7IHTrfA8D5t8PiwnlzEfhplJCHcJ4Ragcst8MyGfdh+pBIReg3eHJ+BbkmRcodFQcIidnQx9hfX4sHVubA53biiawLmjuqj+Iq4F0IZSQgL2FGAea/l/3x/KfRaCQvGpCOdBelUhQtT6UIdrbBg6soc1NpcGNAhFi/f2g96rTI+bgNNGe+Kb58QJiEUGH/9ptG1/Df1xWWpLEinNg0jIVyYSi1XXGPDlBW7UW52oFebKLw+LgMmPfewOhdlJCGcjqEAev+nY/jn9oZr+YfyWn5V4mZl5K9KiwNTVubgRLUNneM9FXFjTPxcOp/wT0KE4MJUCphPck/hza88BemmXM1r+dWMm5WRP3xX0ZWZ0SbagMUT+yMpilfRNUcBSYir4f8cCaGLsLWgFC9s8hSk+93gTph0SSeZIyI5+UZC7NUN686IzsLmdOPRJlfRZaJDHK+ia4nwT0Iadw5MQugC/XSkErM+3Qu3AG7NSMG0a7rxUjqV825WBgCSrVrGSCiUOd0CT326Fz8dqUSkXos3x2ege5I6K+JeiLBPQqRGSQgXptKF2FtUg0fXeArSXdczCTOGsyAdAdDo4DbEeP7LdSF0Fm4h8MKm/dhaWAa9VsIrY/vxKjo/hX0S4qsbA7CAHfntUKOCdIM7x+H5m1mQjhr4tm7nuhA6jRACb351AOvyiqCRgBdvTsMlXXgVnb8UkIQ0mo6ReBkUtdypaiumrMhBpcWBtJRoLBjDgnTUlHfrdo6E0On+8cNR/HvHcQDAUyN647peyTJHFJ7CvseVGu8RwiF0aqFKswNTV+agqMaG1IQIvDmeBenoTN6t2zkSQo19tPME/t+2QwCAh67rjtEZ7eQNKIyFfRLC3VLJX3V2J6atysGhcouvIF0CC9LRWbCIHZ1u495iLPi8EABw3+VdcFc2r6K7GApIQrhHCLWcvf5Sur1FtYgz6fDWxP5ox4J0dA4sYkeN/e/XMszZsA8CwO0DO+D+K1PlDinshX0SIrnr9wnhSAg1w+kWeGr9Pt+ldAsnZKIrC9LReXDrdvL6+VglZqzbC5cARqa1xSNDe/AqugAI+yTEd3UMkxA6DyEEXtpcgC8LSn2X0vVrFyN3WBTi3PVXx3BhqrrtK6rBw6vzYHO6MaR7Imbf2BsaJiABoYAkhMXrqHmL/3cQa3I9Bele4KV01EJcmEqNL+PP6hSHebekQceKuAET9u+k5BsJ4ZoQOrv3th/Fez8eAwDMGt4b1/NSOmohFrFTN+9l/BUWB/q2jcZrY9NZETfAwj4JQf2aEI6E0NmsyTmJRf87CACYdk033JrJS+mo5VjETr0qzHZMWdFwGf/CCbyMvzWEfRIicU0IncMXBaV4cXMBAGDSJZ1x9yWdZY6Iwg1HQtSp1ubEtJW5OFxhQUqMkZfxt6KwT0K4TwidzfbDFXiqviDdmMx2mHJ1V7lDojDU5OoYIeQNhoLC6nDhkY/zsK+4FvEReiyemMnL+FuRYpIQ7hNCXnmnavDYmj1wuASu75WMGTf04qV0dEF8m5W5nZAcdfIGQ63O6XJj5rq9+PlYFaIMWiyakIGuibyMvzWFfRIiifp9QqSwbwoFwMEyM6avzIHZ4cIlXeLx/E19oWVBOrpQOhOE1giA60KUzi0Env1sP/53oBxGnQavjk1H3xRext/awv+T2ztEyiRE9Twr2XejyupEv3YxWDCmHwwsSEcXQ5JYxE4FhBB47ctfsWFvMbQSMO+WNGR3jpc7LFUI/x5auOv/w7NdNfOuZC+utaNbYiTeHJeBKAPXCdHF414hyrfku8P48JcTAIDZo/rg6h5JMkekHuGfhIAjIWpXa3Ni+irPSvZ2MUYsmpiJ+EiuEaLAYBE7Zfvg5+NY8t0RAMBjQ3tiVFqKzBGpi9+f3DabDTNnzsTgwYMxZMgQLF26tNnHHDt2DFlZWfjhhx8uKMjz8o6EcOGhKtmcbjy2Jg97izwr2RdNzERKjFHusEhBBLduV6xP84rw2pe/AgD+dFUqbs/qIHNE6uN3EvLyyy8jNzcXy5Ytw+zZs7F48WJs3LjxvI+ZM2cOzGbzBQfZMkxC1MbpFnjq07346ahnJftCrmQPWSF38uIHTsco01eFpXjus3wAwF3ZHXHPZV1kjkid/Jo0N5vN+Oijj7BkyRKkp6cjPT0dBQUFWL58OUaOHHnWx3zyySeoq2vFS9vqR0IER0JURQiBFzbtx9bCMhi0El4dm440rmQPWY1PXk6cOIEnnngCHTp0OGe/AQTr5KV5XJiqPD8dqcTM+oq4t6SnYPq13XkZv0z8GgnZt28fnE4nsrKyfLdlZ2dj165dcLvdZxxfUVGBBQsW4Nlnn734SM+FV8eojhACL67fi7W5Rb6CdFzJHrq8Jy+zZs1Ceno6hg8fjvvuuw/Lly8/52Na/eTFDxwJUZZdRyvx8Oo82F0C1/VMwqwRrIgrJ79GQkpKSpCQkACDoWH72uTkZNhsNlRWViIxMbHJ8S+99BLGjRuHXr16XXCAzf1uSPCuCdGoblmIt71qa/c/tx/Fkv8dAgA8NaI3ru+tnoJ0wf6ZB+J1znXy8vbbb8PtdkOjaXoC4T15Wbp0KW655ZaLD+Aicet25ThQWocH/rsbZocLg7vE4/mb06DjPkKy8isJsVgsTRIQAL7v7XZ7k9u//fZb7NixA+vWrbuoAJOSmhliP+5ZhGjQ65CcrM7h+GbfIwX5YPsRLP76EABg1k1puOea7vIGJJNw+pnLcfICtOAEpoUJnW8kxFaliIRfrScvJ6rqK+KaHUhvF4NXx/aDSa+OEfRQPnnxKwkxGo1nJBve702mhr31rVYrnnnmGcyePbvJ7ReirKzmvCUbjLVmxACwO92oLq25qNcKN5Lk+TBq7j1Sii35JZi1bi8A4M/X9cD49DYo5c88KK93MeQ4eQFaHnezx1V6Ki8bnDWKOtEJp0T2YpXU2DBt1U8orrWjV9tovP/Hy5EQpb6CdKH4M/crCUlJSUFFRQWcTid0Os9DS0pKYDKZEBsb6ztu9+7dOHr0KKZNm9bk8X/84x8xduxYv9aICNFM3Si3905JFR/EZ9Pse6QAPxyqwFOf7oNbAOP6t8NjN/ZBWVmt4tt9LuH0M5fj5AVo/gSmpQmdzmZEPABXXRkqFJD0qu3kpcbqxAMf7sKhMjPaxxrxr3svg9tqQ6nFJndoQRPKJy9+JSFpaWnQ6XTYuXMnBg8eDADYsWMHMjMzm8zr9u/fH5s2bWry2BEjRuD555/HVVdd5c9LtgAXpipd3slqPPZJHpxugWG9k/EkC9KFFTlOXoCWJ2rNHefyXh1jrVTUh3Y4JbIXyupw4cFVudhfUofESD3+elt/tIszobTUofi2n00o/sz9SkIiIiIwduxYzJkzBy+++CKKi4uxdOlSzJs3D4CnY4mJiYHJZEJqauoZj09JSUFSUoC3w+W27Yp2oKwO01flwuJw47LUeDw7igXpwk1onry0nG9NiNMCOK2AjmXdw4HD5cYTa/dg14lqRBu1WDQhE50TIuQOi07j9/DBjBkzkJ6ejsmTJ2Pu3LmYOnUqRowYAQAYMmQI1q9fH/Agz8+T1gmOhCjOyWorpq7IQZXVifR2MXj51nQWpAtDjU9edu/ejS1btmDp0qWYNGkSAM/Ji9Vq9Z28NP4CWunkxQ/CEOPrXzS2KtnioJZzuQVmb8jHtwcrYNRp8Ma4DPRuGy13WHQWflf4ioiIwPz58zF//vwz7svPzz/n485330Xhtu2KVN64IF1SJN4Yn4FIg1busOgCzZgxA3PmzMHkyZMRHR19xsnLvHnzMH78eJmjPAdJA2GMg2StgGSrAqJYWySUCSGw4ItCbM4vgU4j4eVb+2FAxzi5w6JzCP8yo77NypiEKEWtzYlpK3NxpMKC9rFGLJ6QifgIFqQLZyF38uIntzEOGmsFNywLA/9v2yGs3HUSEoC5o/rgym6JzT6G5BP2Y9uSd2Fq+DeF4FlI9sjHecgvrkVChB6LJmSiLQvSkcwEt24PC//68Sj+8cNRAMCTw3thRN+2MkdEzQn/T25OxyiG0y0w69N9+PmYpyDdogmZSGVBOgoB3Lo99K3JOYmFXx8EAEy5uhvG928vc0TUEgpIQniJrhK4hcDzm/bj61/LYNRp8OrYdPRJ4UIyCg0sYhfavthfghc3FwAAJl3SCZMv7SxzRNRS4f/JzUt0w54QAm9+dQCf5hVBy4J0FIIab91OoeWHQxV4ar1nI8Mxme0w5epucodEfgj/JISblYW9f24/in/vOA4AePrGPri2p3yXYxKdjdvoubpCw+mYkJJzwrORocMlcEPvZMzgRoZhJ/w/ueunY0JsEzhqoVW7TuCv3xwCADx0XXfcnM7LHyn0CFMCAEDidEzIKCypw4OrPRsZXp6agLncyDAshX8SwpGQsLU5vwQvbSkEANxzWWfcld1J5oiIzs7daOt2kt+xSgumrMxBtdWJzPaxeHlMP25kGKbC/qcm8eqYsPT9oXI8s34fBIAJA9rjT1d1lTskonNqWBNSKWscBJTU2vCXFTkoq7OjZ3IU3hifjgg9NzIMV2GfhDQsTA3/pqhFzolqPLZmD5xugRt6t8FjQ3tyHpdCGkdCQkOVxYGpK3NwosqKjnEmLJqQgVgTNzIMZwr45OZ0TDj5tdQzj2t1euZxn72pD+dxKeRxJER+ZrsLD63Oxa+lZiRHGbB4YiaSo7mRYbgL/09uTseEjRNVVkz1zePG4OUx/aDXhv+vICmf9+oYyVYNuF0yR6M+dqcbj3+Sh5yTNYg16bBoYiY6xbMirhKE/yeA97IYJiEhrazOjikrdqOk1o7uSZF4fVwG53EpbAhvEgIByV4tczTq4nILPL1+H344XIkIvacibs/kKLnDogAJ/yQE3Kws1HkK0uXgaKUVHWKNWDQhE3EsSEfhRGuAW+/54OPW7cEjhMC8zQX4oqAUeq2EBWPSkdkhVu6wKIDCPwnx7hPCNSEhyepw4eHVudhfUofESD0WTezPgnQUlljELriEEFj49UGsyT0FjQQ8f3MaLktNkDssCrDw/+Tmtu0hy+lyY8a6vfjleDWiDFosnJCJLgmcx6Xw5GYRu6Batv0o3v/pGABg1vDeGNorWeaIqDWEfxLCq2NCklsIPPvZfnxzoBxGnQavj8tAn7YsSEfhq2EkhPVjWtuqXSfwVv1Oyg9e2x23ZraTNyBqNWH/yc3NykKPEAKvbz2ADXuLoZWAebekIatTnNxhEV0UYfJeIVMpbyAKt2lfcZOdlH87mDspK1nYJyHeNSGcjgkdS384gv/87ClI98zIPri6BwvSUfjjhmWt79uD5XhmQz53UlaR8E9COB0TUlbsPIG3tx0GADxyfQ/c1I8F6UgZ3JFtAADaqoMyR6JMu45X4fFP9sDlFrixbxs8Pow7KatB+H9yczomZGzaV4yXP/cMo957eRfcMaijzBERBY6j01UAAMORrdywLMD2F9fiwdW5sDnduKpbIuaM7AMN+3RVUEAS4p2OCf+mhLPGw6gTB7THA1emyh0SUUA52l0CtyEWGksZdMU75Q5HMY5UWDB1ZQ5qbS4M7BiLl0anQcedlFVDAT9pjoTIrfEw6og+bfAYh1FJibR62LtcBwAwHPpc3lgUoqjGhikrdqPc7EDvNlF4bWwGTNxJWVXCPwnxbVbGDz05FJbU4aHVebA53biiawLmjOIwKimXvesNAADjoS0yRxL+Ks0OTF2Rg5PVNnRJiMDCCZmIMenkDouCTDFJCK+OCb5jlZ5h1BqbE/07xGL+rSxIR8pmT70eQtJAV7YHmpoTcocTtursTkxblYOD5Wa0jfZUxE2KMsgdFskg7D8xJF4dI4vSOjumrsxBaZ0dPZIj8fq4dBakI8UTpgQ42w0GABgOc0rmQticbjz6cR72FtUizqTD4on90T7WJHdYJJPw/+T2XR0T/k0JFzVWT0G6Y5VWdIgzYfGETMSaWJCO1MHWdRgAwHBos8yRhB+nW2DWur346WgVIvWeUg7dkiLlDotkFP6f3KwdE1RWhwsPf5yLgvqCdG9NzERyNAvSkXrYUz3rQgzHtgEOs8zRhA+3EHh+03589WsZDFoJr45NR792MXKHRTIL/yTENx3DJKS1eQvS7TxejWijFosnZqJTPAvSkbq4EnvDFdMZksvmSUSoWd5SDp/mFUErAS/e0g+Du8TLHRaFgPBPQgSTkGBwC4G5jQvSjc1ArzYsSEcqJEmw+6ZkeJVMS/z9+6alHK7tyVIO5BH+SYh3JIRajRACr335KzbuLYZWI2H+6H4YyIJ0pGK2+kt1DYe3NLpCj87mv78cxzvfspQDnZ0CkhAvjoS0lr99fwQf/uK5HHHOyD64qnuizBERycvR4XIIXSS0dUXQlebJHU7I2rC3CAu++BUAcP8VqSzlQGdQUBJCreG/vxzHu/VnMY8N7YGRaW1ljogoBOhMsHe+GgCnZM7l61/LMHdDPgDgN1kdcN8VXWSOiEKRgpIQjoQE2sa9xU3OYm7P4lkMkZd391QmIWfacbQSM9fthUsAN/Vri4ev78FSDnRW4Z+EcD62VWw7UI45Gz1nMbcP5FkM0ensqUMBAPrinZDMJTJHEzr2FdXgkY89pRyu6ZGEp0f0ZikHOqfwT0LA2jGBtut4FZ5Y6ylId2PfNnhkKM9iiE7njkqBo+0AAIDh8BcyRxMaDpWZMXVlLursLmR3jsOLt7AiLp0ffzuoiYKSWl9Buqu6JWLOSBakIzoXFrRrcKraiikrc1BpcSAtJRqvjEmHUcePGDq/8P8NYQG7gPEUpMtFjc2JAR1i8dJonsUQnY83CdEf/Rpw2WSORj7lZjv+siIHRTU2dE2MwJvjMxBtZEVcah4/YQgAUFprw5QVOSirs6NXmyi8Pi4DJhakIzovZ3IGXFEp0DjqoD/+vdzhyKLW5sS0lbk4UmFBuxgjFk3IREIkK+JSyyggCeGOqRer2urA1JW5OF5lRad4ExZOyESMiWcxRM2SJNhT1bt7qtXhwsOrc5FfXIuECD0WT8xEO1bEJT8oIAnxYhJyISwOFx5anYfC0jokRxmwaEImkqN4FkPUUr51IYc/V9XVet5aUr8cr0aUQYtFEzKRmsiKuOSfsE9CJBX90Qeaw+XGE5/swe4T1Ygx6rBoAgvSEfnL3mkIhNYIbfURaCsK5A4nKNxCYM7GfF8tqTfGZaBPCmtJkf/CPgnhdMyFcQuBuRvz8d2hCk9BunHp6NkmSu6wiMKPPhL2jlcCUMeUjBACr3zxKz7bV8JaUnTRFJCEkL9O70RevrUfBnRkJ0J0oRp2T/1c5kha3zvfHsZHO09AAjCXtaToIikgCeEluv5697RO5Mpu7ESILoZ3car+1I+QrBUyR9N6/r3jGP7+/REAwOPDeuJG1pKii6SAJIT88Z+fj+Nv9Z3IY+xEiALCHdsJzqS+kIQbhiNb5Q6nVazNPYXXtx4AAPx5SFdMHNhB5ohICcI/CeFmZS22YW8RXv3SU5DugStTcRs7EaKAsacqt6Dd1oJSPL9pPwDgt9md8PtLO8scESlF+Cch1CLfHGhaVvvey1mQjiiQbN51IUe2Ai6HvMEE0PbDFZj56V64BXBrRgqmX9uNtaQoYJSThPBv4px+OVaFJ9d6ymqPSmNZbaLW4EzJgtuUCI2tCvpTP8kdTkDknazGo2vy4HAJXN8rGTOG92bfQQEV/kkI9wk5r/3FtXj441zYnG4M6Z6IZ25kWW2iVqHRwp46FIAypmQOlNVh+qpcWBxuXNIlHs/f1Bc6DfsOCqzwT0J8+MdxuqMVFkxdmYNamwtZHWMxj2W1iVqVb0rmcHhfqnuiyoopK3JQZXUio30MXhmTDgMr4lIrUMBvFTcrO5uSWhumrNiNcrMDvdpE4dWxLEhH1Nocna+B0OigqyiEpvKg3OFckNI6O/6yYjdKau3onhSJ18dlINLAvoNahwKSEDpdlcWBqStzcKLahs7xJixiQTqioBDGWDjaXwagvpZMmKm2OjBtZQ6OVVrRIc6ExRMzER+hlzssUrDwT0J4iW4T3oJ0v5aa0SbagEUTM5HEgnREQROuu6d6+46CkjokRRnw1sRMtIk2yh0WKVz4JyHk43C58fgne5BzshqxJh0WTshExzgWpCMKJnvX+t1TT3wPyV4jczQt4+07vMUsF7OYJQWJApIQjoQAgMstMHtDPr4/VAGTToPXx2WgZzIL0hEFmyu+O5zx3SG5HdAf/VrucJrlcgs8s76h73hjfAaLWVLQ+J2E2Gw2zJw5E4MHD8aQIUOwdOnScx67detWjBkzBllZWRg9ejQ+/7w1hid5ia4QAgu+KMTm/BLoNBJeHtMP/TvEyh0WkU/o9Ruty7t7qjHEp2SEEJj/eQG27Pf0HQvYd1CQ+Z2EvPzyy8jNzcWyZcswe/ZsLF68GBs3bjzjuH379mHKlCmYMGECPv74Y9xxxx2YPn069u3bF5DAz6Diq2Pe3nYYK3ed9BSkG9UHV3RlQToKLSHbb7QS75SM4fDngHDLHM25vfXNIazefQoaCXj+5r64nH0HBZlfl0yYzWZ89NFHWLJkCdLT05Geno6CggIsX74cI0eObHLsunXrcPnll2PSpEkAgNTUVHzxxRfYsGED+vbtG7gWqHyzsr9/c9BX1fKJG3piRF8WpKPQEpL9RitztL8UbkMMNJYy6Ip+gbNdttwhnWHZ9qNYtv0oAGDGDb0wrHcbmSMiNfIrCdm3bx+cTieysrJ8t2VnZ+Ptt9+G2+2GRtMwsDJu3Dg4HGfWT6ipaZ2FWkKFa0LW5RXhufp6MP93VVdMGMCCdBR6QrnfaDVaPexdroOpcC0Mhz4PuSTkg+1HsOhrzz4m067phrH928scEamVX0lISUkJEhISYDA0XPKZnJwMm82GyspKJCY2DOX16NGjyWMLCgrw3Xff4Y477vArwOZmWaT6NSGSJKlqRubrX8vw3EZPAvLb7I645/LOqmm/t51qaW9jwW57IF5Hjn4DaEHf0crvpaPbDTAVroXx8BZYrni8dV7kAmzZX4KZa/cCAH5/aWdMUlFFXLX2HaHcb/iVhFgsliYdCQDf93a7/ZyPKy8vx9SpUzFo0CAMGzbMn5dEUlLM+Q8wejbSiYoyIiq5mWMV4ocDZZhRX5BuwqBOeG5Cf2hUWNOh2d8NBQuntsvRbwAtf49a7b2MGA1seQi60j1I1lcBcZ1a53X88PX+Ejz96T4IAdx5aRfMHpehyoJ04fT3E0ih2G6/khCj0XhGp+H93mQynfUxpaWl+MMf/gAhBBYuXNhk6LUlyspqzrvsI8ZmhxFAndkOS2mYDdlegH1FtXjgw12wOd24tmcS5k/IREVFraqWxkiS54+pud8NJQp2272vdzHk6DeA5vuO1n8vDYhrlw39yR9R+8snsGbc3Rov0mK7T1Tjz//dDYdL4Ob+7fHQ1akoK6uVNaZgU2vfEcr9hl9JSEpKCioqKuB0OqHTeR5aUlICk8mE2NgzL+sqKiryLTB77733mgy7tpQQzaw9FS08TgGOVFgwbWUO6uwuZHWKwws394VOq1FF289Gre0GwqvtcvQbQMvfo9Z8L22pw6A/+SP0B7fAki5fElJQUovpK3NhdbpxZdcEvH77QFRX1oXN71CghdPfTyCFYrv9Or1IS0uDTqfDzp07fbft2LEDmZmZZ5ypmM1m3HfffdBoNHj//feRkpISkIDPSeEjisU1DQXp+rSNxmtj01mQjsJCSPcbrcy3hfuxbwCHRZYYjlVaMHVlLmpsTvTvEIv5t/ZjRVwKGX79JkZERGDs2LGYM2cOdu/ejS1btmDp0qW+s5aSkhJYrVYAwDvvvIMjR45g/vz5vvtKSkpaYZV7iKV1raDS4sCUlTk4WW1Dl4QILJyQgWgjC9JReAjNfiM4XIl94IrpBMllg+H4tqC/fkmtDX9ZkYOyOjt6tYnC6+PSEcGKuBRC/E6HZ8yYgfT0dEyePBlz587F1KlTMWLECADAkCFDsH79egDAZ599BqvVittuuw1Dhgzxfb3wwguBbYGPModCzHYXHlqdi4NlZrSNNmDxxEwkRrIgHYWX0O03WpkkNWxcdmhLUF+60uLAlBU5OFFl9VXTjjWxIi6FFkmIUJshaqq09PwLaWI/+z8YC9ei9uq5sPS/N3iBBYHd6cbDH+fih8OViDPp8O4dA9A9qaGmgyQByckxzb5HSqPWdgPBb7v39cJRc+9RsN5L/eEvEb/ubrii2qF88o9BuU7SbHfhzx/tRt6pGrSJNuBvdwxEhzjPImD+/aiv7aHcbyhoYlBZIyGegnT78MPhSkToPUWlGicgRBQeHB2vgNBFQFt3CrrSvFZ/PbvTjUfX5CHvVA3iTDosnpjpS0CIQo2CkhDlEELg5c8LsWV/qaeo1K3pyGjPolJEYUlngr3zNQBaf0rG6RZ4av0+/HikEpF6Ld7kyQuFOCYhIej/bTuEVbs9Bemev7kvLuuaIHdIRHQRgrEuRAiBFzftx5cFpdBrJbwyth/SefJCIU4BSUj9BJdCdv1b/tMx/OOH+qJSw1lUikgJ7KlDAQD64p2QzCUBf34hBN746gDW5hVBIwEv3pyGS7rw5IVCnwKSEOVYl3cKb3x1AADwlyFdMY5FpYgUwR3VDo42/QEAhsNfBPz5/7n9KP694zgA4KkRvXFdr+SAvwZRa2ASEiK+KizF85/tBwD8NrsTJquoqBSRGng3LjMGeEpmxc4T+Os3hwAAD13XHaMz2gX0+YlaE5OQELDjaCVmrvMUpBudnoLp13ZTZVEpIiXzJiH6o18DLltAnvOzvcV4+fNCAMC9l3fBXdnyF8kj8kf4JyG+i57D80N7X1ENHvk4D3aXwHU9kzBzRG8mIEQK5GyTAVdkCjSOOuhP/HDRz7ftQDlmb8yHAHDbwA544MrUiw+SKMjCPwkJY4fLzZi2Mhd1dheyO8fh+ZvToNMwASFSJEkDe1fPAtWLvUrml2NVeGLtHrjcAjf2bYNHh/bgyQuFJSYhMimqsWHKihxUWBxIS4nGK2PSYWRRKSJFs6c2WhdygVtX5hfV4qHVubA53RjSPRFzRvaBhgkIhSnlfOqF0R9hpdmBqStycKrGU5DuzfEsSEekBvZOQyC0Rmirj0BbUej34w+XmzF1ZQ7q7C5kdYrDvFvSoNMqpxsn9VHAb294FQCoszsxfXUuDpZ7CtK9NTETCSxIR6QOhig4Ol7h+e+hzX49tPHoad+20XhtbDpMelbEpfCmgCQkfNidbjy2Zg/2+Go69Ee7WNZ0IFITW/1VMoZDn7f4MRVmO6as2I1TNTakJkRg4QSOnpIyMAkJEpdb4OnGNR0mZKJbUqTcYRFRkNlTPVu460/9CMla0ezxtTYnpq/KxaFyC1JijFjM0VNSEAUlIaG7JkQIgXlbCvBFfU2HBWP6Ib1deJZHJ6KL447tDGdiH0jCDcORrec91upw4ZGP87C3qBbxEXosnpjJ0VNSlPBPQi5whXkwvfXNIazJOQWNBDx/cxouTWVNByI1s/umZM59qa7T5cbMdXvx87EqRBm0WDQhA10TOXpKyhL+SUiI+9ePR7Fsu6cg3czhvTCUNR2IVM+3LuTIVsDtPON+txB4btN+/O9AOYw6DV4dm46+KRw9JeVRThISgpfofpJzCgu/PggAmHp1N4zJZEE6IgKcKYPgNiVAY6uC/tRPTe4TQuC1L3/F+j3F0ErAvFvSkN05Xp5AiVqZcpKQEPNlQSle2OwpSDfpkk6YxIJ0ROSl0cKeevbdU//23RF8+MsJAMDsUX1wdY+koIdHFCwKSEJCb03IT0cqMevTvXALYExGO0y5upvcIRFRiPHuntr4Ut3//Hwc7353GADw2NAeGJWWIktsRMHCC80DbM8pT0E6h0vg+l7JeHJ4L9Z0IKIz2LtcA6HRQVdRAE3VIaw7HoFXv/wVAPDAlam4PaujzBEStT4FjIR4yf9Bf6jMjOmrcmF2uDC4Szyeu6kvC9IR0VkJYxwc7S8FABz96RM8uzEfAHDnoI649/IucoZGFDQKSkLkdaraiikrc1DpK0jXjwXpiOi8vJfq1u1ZD5cAbk5PwYPXdefoKamGAj4l5V8TUml2YOrKHBTV2NA1MQILx2ciysCZLiI6vz1RlwMALpH2YmR3E54a0ZsVcUlVFJCEeMiVitTZnZi2Kse3pfKiCZmIj9TLFA0RhYuDZWb8cVMtDrjbwSC58EJ6MadvSXUUk4TIweZ049E1exq2VJ7ALZWJqHknq62YsmI3qqxO7Iq4DAAQdfQLmaMiCr7wT0JkGgJxugWe+nQvfvIWpBufga4sSEdEzSirs2PKihwU19rRLSkSl1x/OwDAcPhzQLhljo4ouMI/CfEK4jyqEAIvbS7A1sIy6LUSXh2bjn4sSEdEzaixOjFtZQ6OVFjQPtaIxRMyYex6BdyGGGgsZdAV7ZQ7RKKgUk4SEkSL/3cQa3I9BelevDkNg7vEyx0SEYU4q8OFhz/Oxf6SOiRG6vHWxP5oG2MEtAbYO18LoH40hEhFmIT46b3tR/Hej8cAALNG9MZ1LEhHRM1wuNx4Yu0e7DxejWijFosmZKJzQoTv/pZU1SVSIiYhfvh490ks+p+nIN20a7rh1ox2MkdERKHO5RaYsyEf3x6sgFGnwRvjMtC7bXSTY+ypQyEgQV+ah4hdf4Nkq5YpWqLgYhLSQl8UlGLelgIAwORLO+PuS1iQjojOTwiBBV8UYlN+CXQaCS/f2g8DOsadeVxEIuxdhwMAor+Zg6R/ZiP6i0ehK94V7JCJgopJSAtsP1yBp+oL0o3NbIe/DOkqd0hEFAbe3nYIK3edhARg7qg+uLJb4jmPrR6xGDVXPwdnYh9ITgsi9v4HCR/djPj/joIpbzlgrwte4ERBwiSkGXmnavDYmj1wuASG9krGkzewIB0RNe/9n45h6Q9HAQBP3tATI/q2Pf8D9JGw9v8DKu7Ygorxq2HtPR5Ca4S+JAcxW5/wjI58NQva0j1BiJ4oOJiEnMfBMjOmr8yB2eHCpfUF6bTc0ZCImvFJzim8+dUBAMBfhnTF+AEdWv5gSYKz/SWoGb4QZZN/RO2VT8MZ1w0aRy0icpch8cMRiF85BsZ9KwCnpZVaQBQcTELO4VSjHQ3T28VgwZh0GFiQjoia8UVBKV7YvB8AcPfgTph86YWvHxMRibBkPYCK336Fylv/A2uPWyA0OuhP7UDs5w8i6Z+DEfXNXGgrCgMVPlFQKaDKWuC3TK0wN9rRMDESb4zPQKRBG/DXISJl+aHR+rExGe0w9ZpugZm+lTRwdB4CR+chqK0rRsTeD2HasxzammOI3LUEkbuWwN7xCljT74at+0hAa7j41yQKAgUkIV6BmSaptTkxfVUuDldY0C7GiEUTMxEfwYJ0RHR+uSer8diaPDhcAsN6J2PG8NZZPyai2sI8eCrMg/4Mw9GvYMp9H4bDW2A4/h0Mx7+DOyIZ1rTbYen3W7jjUgP++kSBpKAk5OLZnG48tiYPe4tqkRChx+KJmUiJMcodFhGFuMLSOkxflQuLw43LUuPx7KggrB/TaGFPHQp76lBoak7AtOffMO35AFpzESJ//isif/4r7F2uhSX9bs9maBp29xR6+FtZz1eQ7mgVogxaLJyQgdREFqQjovM7VmnB1BU5qLY6kdk+Bi/fGvz1Y+6YDjBf9ijMg6fDcHgLIvLeh+HIV74vV1QKrP3ugrXfnRAxfiySJWplTELg2VDoxU37sbWwDIb6gnR9U1iQjojOr7TWhikrclBaZ0eP5Ei8Pk7m9WNaPezdR8HefRQ0VYcQseffMO39ENq6IkT9+Doif3rTMypy5f1A3CWAxLVuJC/VX+4hhMDCrw9ibV4RtBLw4i39kN05Xu6wiCjEVVkcmLIyB8errOgYZ8LiCZmIC6H1Y+64rqi7YibKJm9H9Yi3YO94BSThhvHgJmD5RCT8awgidiyGZC6RO1RSMdUnIe/9eAzv/+QpSPfUjb1xbc8kmSMiolBncbjw0Opc/FpqRnKUAYsnZiI5OkTXj2mNsPUag6qxH6H8zi9hGXAfYIqDtuYoor9/CUnLLkHMxj9Bf2wbIAJ/tSHR+ag6CVm9+yQW1xeke+i67rglnQXpiOj87PUL2HNO1iDWpMOiiZnoFB/R/ANDgCuxF+qungM8ko+aYa/DkTIIktsJ06/rEL/mN0j497WI2PkuJGuF3KGSSqg2Cfl8fwleqi9I94fLOuOu7E4yR0REoc7lFnhmwz78cLgSEXpPRdyeyVFyh+U/fQRsabehcuInKP/NJlgyJsGtj4Ku8gCitz2LpH8ORszmadCd/JGjI9Sqwj8JuYA/kB8OV+Dp9fvgFsD4/u3xf1d1DXxcRKQoQgjM21KAz/eXQq+VsGBMOjI7xMod1kVzJfdD7bUvovz3O1Bz3UtwJGdActlg2r8KCavGIeE/N8C0+x+QbNVyh0oKFP5JiFcLNwXKa7Sh0A292+DxYT1ZkI6ImrX4fwexJucUNBLw/M1puCw1Qe6QAkoYomFN/x0qb9+AiolrYUn7DYTOBF15PmL+97SngN4Xj0JXvEvuUElBVHWJ7oGy0zYUuqkPC9IRUbOWbT+K9370LGCfObwXhvZKljmiViRJcKZkoTYlC3VXPQNj/kpE5L4PXcV+ROz9DyL2/geONv1hzfgdrD3HAIYwnI6ikKGckZBmnKy2YuqKHFQ12lBIr1VN84noAq1qtIB9+rXdMSazvcwRBY8wxsHa/x5U3Pk5KsatgrX3OAiNAfqS3Yj58nHP6MhXs6At3SN3qBSmVPEpXN6oIF33pBDYUIiIwsKmfcV4aXPDAvbfDVbpAnZJgrPDpagZvghlv/8JtVc+BWdcV2gctYjIXYbED0cgfuVYGPNXAE6L3NFSGFF8ElJrc2LaylwcqbCgfawRi0JsQyEiCk3fHizHMxvyIQBMGMAF7F4iIhGWrD+h4rdfo/LW/8DW42YIjQ76Uz8hdsuDSPrnYER9Mxfail/lDpXCgKLXhFgdLjz8cR7yi2uRGKnH4on90ZYF6YioGbuOV+HxT/bA5RYY0acNHhvKBexnkDRwdB4CR+ch0NQVwbT3Q5jylkNbexyRu5YgctcS2DteCWv672DrPhLQGuSOmEKQYpMQp1tg1qf78Mux+oJ04zPRJSE8NhQiIvnsL67Fg6tzYXO6cWW3BMwZxQXszXFHpcA8eBrMg/4Cw5GtMOW9D8Phz2E4/i0Mx7+FOyIZ1rTfwJL+W7hju8gdLoUQRSYhbiHw/Gf5+PrXMhh1Grw2Lh19UqLlDouIQtyRCgumrsxBrc2FAR1iMX90Py5g94dGC3vXYbB3HQZNzXGY9vwbpj3/gdZchMif30LEz3+Fo8u1sKT/zlNIT6PIjyDyg+J+A4QQeGPrAXy6p7i+IF0aBnWKlzssIgpxxTU2TFmxG+VmB3q1icLr4zJg0nMB+4Vyx3SE+bLHYB78IAyHNiMibzkMR7+C4chWGI5shSuqHaz97oS1351wR3eQO1ySieKSkH/8cBQf/HwcAPD0jX1wTQ8WpCOi86usr4h7stqGzvEmLJqQiRiT4rpHeWj1sPe4CfYeN0FTeRARe/4N094Poa07hagfX0fkT2/C3nU4LOm/g6PLtYDEkSc18fuvzGazYe7cudi0aRNMJhPuuece3HPPPWc9ds+ePZg9ezb279+Pnj17Yu7cucjIyLjooBtzpF4HY+luONtlY+WuE/h/2w4B8BSkuzk9JaCvRUQXJtT6jcbq7E5MX5WLg2VmtI02YPHE/kiK4iLK1uCO74a6K2eh7rJHYfx1g2ftyInvYTz4GYwHP4Mruj3cUe0htAZAa6z/1wChNUBoGv7f8K/nmIbbjA33aTzfNz5e0hkBbQKkOofvfmgNnBaSkd/v/Msvv4zc3FwsW7YMJ06cwBNPPIEOHTpg5MiRTY4zm824//77MXr0aLz00kv44IMP8MADD2Dz5s2IjIwMWAOsmZMRfd1fsHHbQczfshcAcM/lXViQjiiEhFq/4WV1uPDox3uw51QN4uor4naIMwX8deg0WiNsvcfC1nsstOUFMOW9D1P+CmhrT0Jbe7LVX/708XEhaRqSGI3h3ImN9szEBloDhKbhmDMTKD2gOU+ydNbnNKpmRMivJMRsNuOjjz7CkiVLkJ6ejvT0dBQUFGD58uVndCbr16+H0WjE448/DkmSMGvWLHz99dfYuHEjxo8fH9BGfF1QiqfX7/Ndz/+nK1MD+vxEdOFCtd9wugWmffALfjxSiUi9Fm9OyET3JG5BHmyuxF6ou3ou6i5/EvpTP0FymCG57IDb5vnXZW/07+m3Nf1ecttOO77hGO9tGrcdwmmDhIbip5JwA04rJKdVxneiKaHRNUqITk9azjMKpDnzNugMQGwsTFYBt+bM52h4XOPX0Z8xstTSGm3+8CsJ2bdvH5xOJ7Kysny3ZWdn4+2334bb7YZG05C57dq1C9nZ2b5r6yVJwqBBg7Bz506/OpPm2ry3qAYP/Gc3nG6B4X08Bek0KrmczvveqG37ArW2Gwh+2wPxOnL0Gy2Jfd6mAmzaUwSDVsKr4/oho32MX88frkL278cQAWeXq1v1JSQJSEqKQXlpNYTL2ZC8uGyAu2myA5fjrIlOk6TG7T32/InP+Y5pSKDsTWN1OwG3E5LTHLD2X+w1okJrhKX/72G+6unzHufP75ZfSUhJSQkSEhJgMDTMlyYnJ8Nms6GyshKJiYlNju3Zs2eTxyclJaGgoMCfl0RS0vk7hvVfH4LF4cLVvZLx1t2DYdCpYwirsebeI6VSa7uB8Gq7HP2G53Hnfo9qrA58knsKWo2ExXcNwoj0dn4/f7gLp9+hQEtKjpU7hDMJAbjsgNPW6F8b4LSf9m/9/S77mbe19LEtPd7tbBKi5LIh0nwMkcmB+93xKwmxWCxNOhIAvu/tdnuLjj39uOaUldVAiHPff+eAFKR3iMV1XeNQXVnn13OHO29W39x7pDRqbTcQ/LZ7X+9iyNFvAM33HS/ekoYeHeLRI1aP0tIav58/XPHvJ9TbLgEw1n/BU1xFA+Aiqo1ccLuFu+mIjdsBd1Q7oJm/F3/6Db+SEKPReEZn4P3eZDK16NjTj2uOEDjvm9YhLgL9e7RFaWko/1K1rubeI6VSa7uB8Gq7HP0G0Px7NLxPGyQnx6i27win36FAU2vb/W+3BtBGQGhP2208gO+dX3MXKSkpqKiogNPZMERTUlICk8mE2NjYM44tLS1tcltpaSnatm17EeESUbhhv0FE5+JXEpKWlgadToedO3f6btuxYwcyMzObLC4DgAEDBuCXX36BqE+7hBD4+eefMWDAgIuPmojCBvsNIjoXv5KQiIgIjB07FnPmzMHu3buxZcsWLF26FJMmTQLgObuxWj2XOI0cORLV1dV44YUXUFhYiBdeeAEWiwWjRo0KfCuIKGSx3yCic/H7UpIZM2YgPT0dkydPxty5czF16lSMGDECADBkyBCsX78eABAdHY133nkHO3bswPjx47Fr1y68++67rbLhEBGFNvYbRHQ2khChvTynuUVjkgTVLi5Ta9vV2m4g+G33vl44Yt9xdmptN6Detodyv6G+TTWIiIgoJDAJISIiIlkwCSEiIiJZMAkhIiIiWTAJISIiIlkwCSEiIiJZMAkhIiIiWTAJISIiIln4VUVXDpLUsvubO06J1Np2tbYbCH7bw/k9Zt9xdmptN6DetodyvxHyO6YSERGRMnE6hoiIiGTBJISIiIhkwSSEiIiIZMEkhIiIiGTBJISIiIhkwSSEiIiIZMEkhIiIiGTBJISIiIhkwSSEiIiIZMEkhIiIiGQRFkmIzWbDzJkzMXjwYAwZMgRLly4957F79uzBbbfdhgEDBmDChAnIzc0NYqSB50/bt27dijFjxiArKwujR4/G559/HsRIA8ufdnsdO3YMWVlZ+OGHH4IQYevxp+35+fm488470b9/f4wePRrff/99ECMNfWrtO9TabwDq7TvCtt8QYeDZZ58Vo0ePFrm5uWLTpk0iKytLbNiw4Yzj6urqxFVXXSVeeuklUVhYKJ577jlx5ZVXirq6OhmiDoyWtn3v3r0iPT1dLFu2TBw6dEi8//77Ij09Xezdu1eGqC9eS9vd2L333it69+4tvv/++yBF2Tpa2vbq6mpx5ZVXiqeeekocOnRIvPnmmyI7O1uUlpbKEHVoUmvfodZ+Qwj19h3h2m+EfBJSV1cnMjMzm/xyvPXWW+J3v/vdGcd+9NFHYujQocLtdgshhHC73WL48OFi5cqVQYs3kPxp+4IFC8S9997b5LZ77rlHvPbaa60eZ6D5026vNWvWiDvuuCPsOxJ/2r5s2TJxww03CKfT6btt/PjxYuvWrUGJNdSpte9Qa78hhHr7jnDuN0J+Ombfvn1wOp3Iysry3ZadnY1du3bB7XY3OXbXrl3Izs6GVF9HWJIkDBo0CDt37gxmyAHjT9vHjRuHRx999IznqKmpafU4A82fdgNARUUFFixYgGeffTaYYbYKf9q+fft2DBs2DFqt1nfbypUrce211wYt3lCm1r5Drf0GoN6+I5z7jZBPQkpKSpCQkACDweC7LTk5GTabDZWVlWcc27Zt2ya3JSUl4dSpU8EINeD8aXuPHj3Qt29f3/cFBQX47rvvcMUVVwQr3IDxp90A8NJLL2HcuHHo1atXEKNsHf60/ejRo0hMTMTTTz+Nq666Crfffjt27NgR5IhDl1r7DrX2G4B6+45w7jdCPgmxWCxN3lgAvu/tdnuLjj39uHDhT9sbKy8vx9SpUzFo0CAMGzasVWNsDf60+9tvv8WOHTvw5z//OWjxtSZ/2m42m/Huu++iTZs2WLJkCS655BLce++9OHnyZNDiDWVq7TvU2m8A6u07wrnfCPkkxGg0nvEmer83mUwtOvb048KFP233Ki0txeTJkyGEwMKFC6HRhPyP+AwtbbfVasUzzzyD2bNnh+3P+HT+/My1Wi3S0tIwbdo09OvXD4899hi6du2KNWvWBC3eUKbWvkOt/Qag3r4jnPsNnSyv6oeUlBRUVFTA6XRCp/OEW1JSApPJhNjY2DOOLS0tbXJbaWnpGcOs4cKftgNAUVERJk2aBAB47733kJiYGNR4A6Wl7d69ezeOHj2KadOmNXn8H//4R4wdOzYs53n9+Zm3adMG3bt3b3Jb165dORJST619h1r7DUC9fUc49xshn+6mpaVBp9M1WSC2Y8cOZGZmnpGtDxgwAL/88guEEAAAIQR+/vlnDBgwIJghB4w/bTebzbjvvvug0Wjw/vvvIyUlJcjRBk5L292/f39s2rQJH3/8se8LAJ5//nlMnz49yFEHhj8/84EDByI/P7/JbQcOHEDHjh2DEWrIU2vfodZ+A1Bv3xHW/YYs1+T46emnnxY333yz2LVrl9i8ebMYNGiQ+Oyzz4QQQhQXFwuLxSKEEKKmpkZcfvnl4rnnnhMFBQXiueeeE1dddVXYXusvRMvb/tprr4n+/fuLXbt2ieLiYt9XdXW1nOFfsJa2+3ThfJmdV0vbfuzYMTFw4ECxcOFCcejQIfHGG2+IgQMHilOnTskZfkhRa9+h1n5DCPX2HeHab4RFEmI2m8Xjjz8uBg4cKIYMGSL+8Y9/+O7r3bt3k2v5d+3aJcaOHSsyMzPFxIkTRV5engwRB05L237jjTeK3r17n/H1xBNPyBT5xfHnZ95YuHckQvjX9p9++kmMGzdOZGRkiDFjxojt27fLEHHoUmvfodZ+Qwj19h3h2m9IQtSPPxIREREFUcivCSEiIiJlYhJCREREsmASQkRERLJgEkJERESyYBJCREREsmASQkRERLJgEkJERESyYBJCREREsmASQkRERLJgEkJERESyYBJCREREsvj/giC2ib3qHzMAAAAASUVORK5CYII=\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\"\"\"\n",
    "expected_gained_learn = nn.learn(learn, num_epochs=5, categorical=True, need_preparations=False)\n",
    "learn_analyser = ClassificationErrorAnalyser(expected_gained_learn, 10, border_step=0.1)\n",
    "show_plots_classification(learn_analyser, \"learn\")\n",
    "#\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test\n",
      " border = 0.5\n",
      " recall = 0.8054438247147339\n",
      " precision = 0.9347618184472051\n",
      " accuracy = 0.97499\n",
      " F-score = 0.8652978884488697\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "expected_gained_test = nn.test(test, categorical=True, need_preparation=False)\n",
    "test_analyser = ClassificationErrorAnalyser(expected_gained_test, 10, border_step=0.1)\n",
    "show_plots_classification(test_analyser, \"test\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving weights...\n"
     ]
    }
   ],
   "source": [
    "nn.save_net(\"./classification_weights\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
