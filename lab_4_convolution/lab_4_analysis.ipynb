{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from neunet.neunet import NeuralNet\n",
    "from neunet.layer import *\n",
    "from analysis.classification_error_analyser import ClassificationErrorAnalyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def prepare_df(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n",
    "    df[columns] = df[columns].apply(lambda s: s.apply(lambda x: x / 255))\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_target_to_vector(df: pd.DataFrame, column_name: str) -> pd.DataFrame:\n",
    "    target = df[column_name]\n",
    "    new_target = np.zeros((len(df.index), 10))\n",
    "    for label, val in target.items():\n",
    "        new_target[label, int(val)] = 1.0\n",
    "    new_target_df = pd.DataFrame(new_target, columns=[\"is_0\", \"is_1\", \"is_2\", \"is_3\", \"is_4\", \"is_5\", \"is_6\", \"is_7\", \"is_8\", \"is_9\"])\n",
    "    df = df.drop([column_name], axis=1)\n",
    "    return df.join(new_target_df)\n",
    "\n",
    "def show_plots_classification(analyser: ClassificationErrorAnalyser, msg: str):\n",
    "    recall = analyser.recall()\n",
    "    precision = analyser.precision()\n",
    "    accuracy = analyser.accuracy()\n",
    "    f_score = analyser.f_score()\n",
    "\n",
    "    border = 0.5\n",
    "    ind = analyser.get_index_by_border(border)\n",
    "\n",
    "    print(f\"\\n{msg}\\n border = {border}\\n recall = {recall[ind]}\\n precision = {precision[ind]}\\n \"\n",
    "          f\"accuracy = {accuracy[ind]}\\n F-score = {f_score[ind]}\\n\")\n",
    "\n",
    "    tpr = analyser.tpr()\n",
    "    fpr = analyser.fpr()\n",
    "    fnr = analyser.fnr()\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    plt.title('test', fontsize=15)\n",
    "    axs[0].plot(fpr, fpr, fpr, tpr)\n",
    "    axs[0].grid(True)\n",
    "    axs[0].set_title(label='ROC', fontsize=10)\n",
    "    axs[1].plot(fpr, fpr, fpr, fnr)\n",
    "    axs[1].grid(True)\n",
    "    axs[1].set_title(label='DET', fontsize=10)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "dataset_train_path = \"MNIST_dataset/mnist_train.csv\"\n",
    "dataset_test_path = \"MNIST_dataset/mnist_test.csv\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "       1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  1x10  ...  is_0  is_1  \\\n0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n1      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   1.0   0.0   \n2      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n3      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   1.0   \n4      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n...    ...  ...  ...  ...  ...  ...  ...  ...  ...   ...  ...   ...   ...   \n59995  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n59996  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n59997  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n59998  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n59999  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n\n       is_2  is_3  is_4  is_5  is_6  is_7  is_8  is_9  \n0       0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0  \n1       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n2       0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0  \n3       0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n4       0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0  \n...     ...   ...   ...   ...   ...   ...   ...   ...  \n59995   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0  \n59996   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  \n59997   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0  \n59998   0.0   0.0   0.0   0.0   1.0   0.0   0.0   0.0  \n59999   0.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0  \n\n[60000 rows x 794 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1x1</th>\n      <th>1x2</th>\n      <th>1x3</th>\n      <th>1x4</th>\n      <th>1x5</th>\n      <th>1x6</th>\n      <th>1x7</th>\n      <th>1x8</th>\n      <th>1x9</th>\n      <th>1x10</th>\n      <th>...</th>\n      <th>is_0</th>\n      <th>is_1</th>\n      <th>is_2</th>\n      <th>is_3</th>\n      <th>is_4</th>\n      <th>is_5</th>\n      <th>is_6</th>\n      <th>is_7</th>\n      <th>is_8</th>\n      <th>is_9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>59995</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>59996</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>59997</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>59998</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>59999</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>60000 rows × 794 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = pd.read_csv(dataset_train_path)\n",
    "learn[learn.columns] = learn[learn.columns].astype(float)\n",
    "columns_list = list(learn.columns[1:])\n",
    "columns_list.append(learn.columns[0])\n",
    "learn = learn[columns_list]\n",
    "learn = prepare_df(learn, columns_list[:-1])\n",
    "learn = convert_target_to_vector(learn, learn.columns[-1])\n",
    "learn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "      1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  1x10  ...  is_0  is_1  \\\n0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   1.0   \n3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   1.0   0.0   \n4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n...   ...  ...  ...  ...  ...  ...  ...  ...  ...   ...  ...   ...   ...   \n9995  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n9996  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n9997  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n9998  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n9999  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   0.0   0.0   \n\n      is_2  is_3  is_4  is_5  is_6  is_7  is_8  is_9  \n0      0.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0  \n1      1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n2      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n3      0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n4      0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0  \n...    ...   ...   ...   ...   ...   ...   ...   ...  \n9995   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n9996   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0  \n9997   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0  \n9998   0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0  \n9999   0.0   0.0   0.0   0.0   1.0   0.0   0.0   0.0  \n\n[10000 rows x 794 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1x1</th>\n      <th>1x2</th>\n      <th>1x3</th>\n      <th>1x4</th>\n      <th>1x5</th>\n      <th>1x6</th>\n      <th>1x7</th>\n      <th>1x8</th>\n      <th>1x9</th>\n      <th>1x10</th>\n      <th>...</th>\n      <th>is_0</th>\n      <th>is_1</th>\n      <th>is_2</th>\n      <th>is_3</th>\n      <th>is_4</th>\n      <th>is_5</th>\n      <th>is_6</th>\n      <th>is_7</th>\n      <th>is_8</th>\n      <th>is_9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9995</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>9996</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>9997</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>9998</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>9999</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows × 794 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(dataset_test_path)\n",
    "test = test[columns_list]\n",
    "test = prepare_df(test, columns_list[:-1])\n",
    "test = convert_target_to_vector(test, test.columns[-1])\n",
    "test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "hidden_1 = 120\n",
    "hidden_2 = 84\n",
    "#nn = NeuralNet(dir_name='./classification_weights')\n",
    "#\"\"\"\n",
    "nn = NeuralNet(learn_rate=0.003,  layers=[\n",
    "    ConvLayer((1, 28, 28), dims_filter=(1, 3, 3), num_filters=6, start_weight_multiplier=0.1,\n",
    "              funcs=[\"ReLu\"] * 2304, deviation=0.0),\n",
    "    PoolLayer(\"avg\", (6, 26, 26)),\n",
    "    ConvLayer((6, 13, 13), dims_filter=(6, 4, 4), num_filters=16, start_weight_multiplier=0.1,\n",
    "              funcs=[\"ReLu\"] * 576, deviation=0.0),\n",
    "    PoolLayer(\"avg\", (16, 10, 10)),\n",
    "    DenseLayer(dims=(hidden_1, 400), funcs=[\"sigmoid\"] * hidden_1, deviation=0.5),\n",
    "    DenseLayer(dims=(hidden_2, hidden_1), funcs=[\"sigmoid\"] * hidden_2, deviation=0.5),\n",
    "    DenseLayer(dims=(10, hidden_2), funcs=[\"sigmoid\"], deviation=0.5)\n",
    "])\n",
    "#\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.1282342  -0.01378655  0.03097528  0.16437574 -0.09145006  0.23155345\n",
      "  0.27576417  0.20048078 -0.0739896  -0.04595697], error = [ 0.8717658   0.01378655 -0.03097528 -0.16437574  0.09145006 -0.23155345\n",
      " -0.27576417 -0.20048078  0.0739896   0.04595697]\n",
      "learn time: 70.89597868919373\n",
      "2000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.12535207 -0.01561583  0.05535829 -0.0551423   0.00175064  0.46521438\n",
      "  0.13338122 -0.12824216  0.05138836  0.20707225], error = [-0.12535207  0.01561583 -0.05535829  0.0551423  -0.00175064  0.53478562\n",
      " -0.13338122  0.12824216 -0.05138836 -0.20707225]\n",
      "learn time: 147.19863176345825\n",
      "3000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.13297403 -0.03615836  0.0828201  -0.07369798  0.0796303   0.07255247\n",
      "  0.2513225   0.16581617 -0.04373969  0.02001849], error = [-0.13297403  0.03615836 -0.0828201   0.07369798 -0.0796303  -0.07255247\n",
      " -0.2513225  -0.16581617  0.04373969  0.97998151]\n",
      "learn time: 223.4474058151245\n",
      "4000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.10106211  0.15107312  0.19443041  0.28314078 -0.01394556 -0.09480117\n",
      " -0.15433762  0.40815795  0.09004487  0.07343687], error = [-0.10106211 -0.15107312 -0.19443041 -0.28314078  0.01394556  0.09480117\n",
      "  0.15433762  0.59184205 -0.09004487 -0.07343687]\n",
      "learn time: 299.7681860923767\n",
      "5000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.13767087 -0.27787649  0.21075268  0.02856714  0.38137707 -0.19496677\n",
      "  0.1655268   0.41263095 -0.05503128  0.10230804], error = [-0.13767087  0.27787649 -0.21075268 -0.02856714 -0.38137707  0.19496677\n",
      " -0.1655268   0.58736905  0.05503128 -0.10230804]\n",
      "learn time: 376.0220386981964\n",
      "6000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.12558324  0.02211717  0.12648405 -0.02754104  0.02807118  0.12705451\n",
      "  0.71816282 -0.17184686 -0.00161375  0.05450111], error = [-0.12558324 -0.02211717 -0.12648405  0.02754104 -0.02807118 -0.12705451\n",
      "  0.28183718  0.17184686  0.00161375 -0.05450111]\n",
      "learn time: 452.49754071235657\n",
      "7000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.10538857  0.07263978  0.05399004  0.19700426  0.1890482   0.22852828\n",
      "  0.02720974 -0.00455281  0.12809528  0.28362095], error = [-0.10538857 -0.07263978 -0.05399004 -0.19700426 -0.1890482  -0.22852828\n",
      " -0.02720974  0.00455281  0.87190472 -0.28362095]\n",
      "learn time: 528.0542557239532\n",
      "8000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.18518642  0.03195599 -0.07621779  0.10707427 -0.00084644 -0.03312467\n",
      "  0.11824445  0.08491537  0.09366865 -0.0620738 ], error = [ 0.81481358 -0.03195599  0.07621779 -0.10707427  0.00084644  0.03312467\n",
      " -0.11824445 -0.08491537 -0.09366865  0.0620738 ]\n",
      "learn time: 603.7342917919159\n",
      "9000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.15001424 -0.07102392  0.2898173  -0.21455504  0.2513863  -0.1580518\n",
      "  0.45733244 -0.01567778  0.24590795  0.08181286], error = [-0.15001424  0.07102392 -0.2898173   0.21455504 -0.2513863   0.1580518\n",
      "  0.54266756  0.01567778 -0.24590795 -0.08181286]\n",
      "learn time: 679.5700798034668\n",
      "10000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.10350386 -0.04548127  0.07645775  0.67833871 -0.05836569  0.06672112\n",
      "  0.200313    0.29156389  0.00271512 -0.03759693], error = [-0.10350386  0.04548127 -0.07645775  0.32166129  0.05836569 -0.06672112\n",
      " -0.200313   -0.29156389 -0.00271512  0.03759693]\n",
      "learn time: 755.2859618663788\n",
      "11000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.09800941 -0.01833419  0.03963229  0.04269655  0.16444646 -0.03669667\n",
      "  0.94562672 -0.06155878 -0.0675917  -0.05573045], error = [-0.09800941  0.01833419 -0.03963229 -0.04269655 -0.16444646  0.03669667\n",
      "  0.05437328  0.06155878  0.0675917   0.05573045]\n",
      "learn time: 831.0293161869049\n",
      "12000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.16660147 -0.02689841  0.27934092  0.07901594 -0.08902917  0.22867574\n",
      " -0.10769588  0.77655174  0.09885813 -0.02529575], error = [-0.16660147  0.02689841 -0.27934092 -0.07901594  0.08902917 -0.22867574\n",
      "  0.10769588  0.22344826 -0.09885813  0.02529575]\n",
      "learn time: 906.5586104393005\n",
      "13000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.1005277   0.12249792 -0.11511063 -0.02430928  0.10847532  0.26285151\n",
      " -0.15983266  0.5216452  -0.06407784  0.18612346], error = [-0.1005277  -0.12249792  0.11511063  0.02430928 -0.10847532 -0.26285151\n",
      "  0.15983266  0.4783548   0.06407784 -0.18612346]\n",
      "learn time: 982.5884628295898\n",
      "14000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.05155425  0.69171083 -0.16697456  0.01972723 -0.00286796 -0.03902646\n",
      "  0.12490722  0.00344991 -0.05699012  0.16430501], error = [-0.05155425  0.30828917  0.16697456 -0.01972723  0.00286796  0.03902646\n",
      " -0.12490722 -0.00344991  0.05699012 -0.16430501]\n",
      "learn time: 1058.3708095550537\n",
      "15000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.14910847  0.01016996 -0.06470022  0.05682673  0.14154991  0.27502307\n",
      "  0.22388822 -0.20188849 -0.05122849  0.23870041], error = [-0.14910847 -0.01016996  0.06470022 -0.05682673 -0.14154991  0.72497693\n",
      " -0.22388822  0.20188849  0.05122849 -0.23870041]\n",
      "learn time: 1134.2513315677643\n",
      "16000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.1669423  -0.05953944 -0.06090886 -0.10510585 -0.01244201  0.13562034\n",
      "  0.07171334  0.0528429   0.60471498  0.17617999], error = [-0.1669423   0.05953944  0.06090886  0.10510585  0.01244201 -0.13562034\n",
      " -0.07171334 -0.0528429   0.39528502 -0.17617999]\n",
      "learn time: 1210.1044428348541\n",
      "17000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.13524389  0.0467998   0.11212483  0.55629996  0.11045695  0.24880066\n",
      "  0.04828651  0.08781387 -0.19379736 -0.00516021], error = [-0.13524389 -0.0467998  -0.11212483  0.44370004 -0.11045695 -0.24880066\n",
      " -0.04828651 -0.08781387  0.19379736  0.00516021]\n",
      "learn time: 1285.8920929431915\n",
      "18000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.11266231 -0.13000138  0.0971882   0.0124144   0.43012449 -0.09717146\n",
      "  0.28770441  0.11428298 -0.07648796  0.39750176], error = [-0.11266231  0.13000138 -0.0971882  -0.0124144   0.56987551  0.09717146\n",
      " -0.28770441 -0.11428298  0.07648796 -0.39750176]\n",
      "learn time: 1361.7650799751282\n",
      "19000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.10227649  0.01653409  0.33048974  0.01433907 -0.06577487 -0.13156651\n",
      " -0.12708101  0.13776338  0.41194245  0.00674211], error = [-0.10227649 -0.01653409 -0.33048974 -0.01433907  0.06577487  0.13156651\n",
      "  0.12708101 -0.13776338  0.58805755 -0.00674211]\n",
      "learn time: 1437.2217426300049\n",
      "20000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.30298462 -0.12240548 -0.19368054  0.08853477  0.07527145  0.59312552\n",
      "  0.19310601 -0.1965021   0.16213139  0.06611535], error = [-0.30298462  0.12240548  0.19368054 -0.08853477 -0.07527145  0.40687448\n",
      " -0.19310601  0.1965021  -0.16213139 -0.06611535]\n",
      "learn time: 1512.8034238815308\n",
      "21000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.08422073  0.14166161  0.12391344  0.0405519  -0.1166251   0.11416707\n",
      " -0.21466136  0.46732816  0.07145551  0.33809406], error = [-0.08422073 -0.14166161 -0.12391344 -0.0405519   0.1166251  -0.11416707\n",
      "  0.21466136  0.53267184 -0.07145551 -0.33809406]\n",
      "learn time: 1588.568099975586\n",
      "22000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.04101342  0.71375877 -0.12634983  0.58287857  0.11589579 -0.06371313\n",
      " -0.17964854  0.09542032 -0.20084917 -0.07782646], error = [-0.04101342 -0.71375877  0.12634983  0.41712143 -0.11589579  0.06371313\n",
      "  0.17964854 -0.09542032  0.20084917  0.07782646]\n",
      "learn time: 1664.085744857788\n",
      "23000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.12049654 -0.1498302  -0.06453454  0.14202801  0.08957651 -0.13802533\n",
      " -0.00981522  0.97767322 -0.06497635  0.08644605], error = [-0.12049654  0.1498302   0.06453454 -0.14202801 -0.08957651  0.13802533\n",
      "  0.00981522  0.02232678  0.06497635 -0.08644605]\n",
      "learn time: 1739.6412258148193\n",
      "24000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.0691384  -0.01317817  0.27772668  0.25962175  0.02050846  0.09177751\n",
      " -0.09863623  0.04728971  0.35524602 -0.05202903], error = [-0.0691384   0.01317817 -0.27772668 -0.25962175 -0.02050846 -0.09177751\n",
      "  0.09863623 -0.04728971  0.64475398  0.05202903]\n",
      "learn time: 1815.312801361084\n",
      "25000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.03283599  0.29806109  0.02248055  0.20949266  0.01291643 -0.0111932\n",
      " -0.10640241  0.28482646  0.07010064  0.28101527], error = [-0.03283599 -0.29806109 -0.02248055  0.79050734 -0.01291643  0.0111932\n",
      "  0.10640241 -0.28482646 -0.07010064 -0.28101527]\n",
      "learn time: 1890.9634504318237\n",
      "26000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.04874013  0.11673597  0.09785752 -0.09224485  0.55841208  0.04777814\n",
      "  0.0136483   0.0645375   0.05798635  0.27795246], error = [-0.04874013 -0.11673597 -0.09785752  0.09224485  0.44158792 -0.04777814\n",
      " -0.0136483  -0.0645375  -0.05798635 -0.27795246]\n",
      "learn time: 1966.6016085147858\n",
      "27000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.24378986 -0.05234331 -0.04819647  0.14872759  0.23473444  0.32769717\n",
      "  0.05578367 -0.24134666  0.09174349  0.13744532], error = [-0.24378986  0.05234331  0.04819647 -0.14872759 -0.23473444  0.67230283\n",
      " -0.05578367  0.24134666 -0.09174349 -0.13744532]\n",
      "learn time: 2042.6758968830109\n",
      "28000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.02551237  0.80747144 -0.02215604 -0.05835309 -0.06466767  0.13941825\n",
      "  0.03962701  0.08503154  0.17618772  0.02253523], error = [-0.02551237  0.19252856  0.02215604  0.05835309  0.06466767 -0.13941825\n",
      " -0.03962701 -0.08503154 -0.17618772 -0.02253523]\n",
      "learn time: 2118.4174284934998\n",
      "29000: expected = [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.04747141  0.09814092  0.75622256  0.32425725  0.05848123 -0.04008769\n",
      "  0.0406634  -0.00964867  0.05116593 -0.05816773], error = [-0.04747141 -0.09814092  0.24377744 -0.32425725 -0.05848123  0.04008769\n",
      " -0.0406634   0.00964867 -0.05116593  0.05816773]\n",
      "learn time: 2194.397109270096\n",
      "30000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.04639433  0.16203993 -0.13539747  0.98182379 -0.13105327 -0.05997109\n",
      "  0.11812169  0.15923983  0.02104142 -0.0247408 ], error = [-0.04639433 -0.16203993  0.13539747  0.01817621  0.13105327  0.05997109\n",
      " -0.11812169 -0.15923983 -0.02104142  0.0247408 ]\n",
      "learn time: 2270.209203004837\n",
      "31000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.2657104  -0.07436821 -0.01170804 -0.021558    0.17205539  0.55058116\n",
      "  0.38648009 -0.07184882  0.02236703 -0.19593658], error = [-0.2657104   0.07436821  0.01170804  0.021558   -0.17205539 -0.55058116\n",
      "  0.61351991  0.07184882 -0.02236703  0.19593658]\n",
      "learn time: 2345.7941393852234\n",
      "32000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.05902675  0.10518169  0.11819793 -0.03922403  0.06269524  0.16534452\n",
      "  0.10570225  0.05469456  0.5099496  -0.22004187], error = [-0.05902675 -0.10518169 -0.11819793  0.03922403 -0.06269524 -0.16534452\n",
      " -0.10570225 -0.05469456  0.4900504   0.22004187]\n",
      "learn time: 2421.506047487259\n",
      "33000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [0.04337941 0.01226913 0.02797759 0.29575426 0.11763582 0.46352795\n",
      " 0.10210499 0.05894477 0.02309119 0.0810707 ], error = [-0.04337941 -0.01226913 -0.02797759  0.70424574 -0.11763582 -0.46352795\n",
      " -0.10210499 -0.05894477 -0.02309119 -0.0810707 ]\n",
      "learn time: 2497.201493501663\n",
      "34000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.08168149 -0.05838492  0.21434565  0.01009187  0.4319855   0.02689617\n",
      "  0.06241313  0.01243188 -0.17374056  0.63764588], error = [-0.08168149  0.05838492 -0.21434565 -0.01009187  0.5680145  -0.02689617\n",
      " -0.06241313 -0.01243188  0.17374056 -0.63764588]\n",
      "learn time: 2572.9460763931274\n",
      "35000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.01069051  0.92144002 -0.12869948  0.0489608   0.09819935 -0.02055697\n",
      " -0.03783313 -0.02347719 -0.06224078 -0.06602699], error = [-0.01069051  0.07855998  0.12869948 -0.0489608  -0.09819935  0.02055697\n",
      "  0.03783313  0.02347719  0.06224078  0.06602699]\n",
      "learn time: 2648.499849796295\n",
      "36000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.04528326  0.05610815 -0.13171329 -0.00633151  0.23761399 -0.1118397\n",
      " -0.04993904 -0.03881619  0.39823915  0.60869973], error = [-0.04528326 -0.05610815  0.13171329  0.00633151 -0.23761399  0.1118397\n",
      "  0.04993904  0.03881619 -0.39823915  0.39130027]\n",
      "learn time: 2724.1513409614563\n",
      "37000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.0690239  -0.07090005 -0.00197049  0.07242162  0.58881469  0.20799513\n",
      "  0.46040383 -0.23166349  0.03426473 -0.0111675 ], error = [-0.0690239   0.07090005  0.00197049 -0.07242162  0.41118531 -0.20799513\n",
      " -0.46040383  0.23166349 -0.03426473  0.0111675 ]\n",
      "learn time: 2799.8026580810547\n",
      "38000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.04257711  0.01778652  0.04136889 -0.01903156  0.03230984  0.17606555\n",
      "  0.06797816  0.21342445  0.33988653  0.15789614], error = [-0.04257711 -0.01778652 -0.04136889  0.01903156 -0.03230984 -0.17606555\n",
      " -0.06797816 -0.21342445  0.66011347 -0.15789614]\n",
      "learn time: 2875.404906272888\n",
      "39000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.03615288 -0.11628833 -0.09589643  0.21536239  0.41677202  0.07227232\n",
      "  0.06254294  0.04300809 -0.07326623  0.36891378], error = [-0.03615288  0.11628833  0.09589643 -0.21536239 -0.41677202 -0.07227232\n",
      " -0.06254294 -0.04300809  0.07326623  0.63108622]\n",
      "learn time: 2951.18572640419\n",
      "40000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.03501837  0.11727404 -0.16417311  0.0416799  -0.04612697  0.23192439\n",
      "  0.11838214  0.66672688  0.08844405  0.27587422], error = [-0.03501837 -0.11727404  0.16417311 -0.0416799   0.04612697 -0.23192439\n",
      " -0.11838214  0.33327312 -0.08844405 -0.27587422]\n",
      "learn time: 3027.005617618561\n",
      "41000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.06192768 -0.16552629  0.04948651 -0.12099586  0.3181182   0.30379858\n",
      " -0.09767357  0.00489882  0.07221976  0.14849355], error = [-0.06192768  0.16552629 -0.04948651  0.12099586 -0.3181182   0.69620142\n",
      "  0.09767357 -0.00489882 -0.07221976 -0.14849355]\n",
      "learn time: 3102.558982849121\n",
      "42000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 1.26080284e-02  1.03084186e+00 -7.28832719e-02  4.76479708e-02\n",
      " -2.52868182e-02 -1.02480788e-01  8.25912554e-02  6.42843774e-02\n",
      " -8.88203003e-03 -8.35097060e-04], error = [-0.01260803 -0.03084186  0.07288327 -0.04764797  0.02528682  0.10248079\n",
      " -0.08259126 -0.06428438  0.00888203  0.0008351 ]\n",
      "learn time: 3178.3909204006195\n",
      "43000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.04137105 -0.05404837  0.16783564  0.04472129  0.09777395  0.0107502\n",
      " -0.05206242  0.28994635  0.08181775  0.43274762], error = [-0.04137105  0.05404837 -0.16783564 -0.04472129 -0.09777395 -0.0107502\n",
      "  0.05206242 -0.28994635 -0.08181775  0.56725238]\n",
      "learn time: 3253.916024208069\n",
      "44000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.66286268  0.21859903  0.049668    0.15663679 -0.12803669  0.15119187\n",
      " -0.14739116  0.01929599 -0.17784324  0.14151571], error = [ 0.33713732 -0.21859903 -0.049668   -0.15663679  0.12803669 -0.15119187\n",
      "  0.14739116 -0.01929599  0.17784324 -0.14151571]\n",
      "learn time: 3329.436730146408\n",
      "45000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.02167717 -0.03399996  0.04473466  0.58158637  0.01022597  0.22697629\n",
      "  0.07334008 -0.12958762  0.15394439  0.066976  ], error = [-0.02167717  0.03399996 -0.04473466  0.41841363 -0.01022597 -0.22697629\n",
      " -0.07334008  0.12958762 -0.15394439 -0.066976  ]\n",
      "learn time: 3405.083417415619\n",
      "46000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.03320283 -0.02698147  0.10444392 -0.0452452   0.07640114 -0.00454491\n",
      " -0.04448416  0.03728167  0.07110542  0.91691041], error = [-0.03320283  0.02698147 -0.10444392  0.0452452  -0.07640114  0.00454491\n",
      "  0.04448416 -0.03728167 -0.07110542  0.08308959]\n",
      "learn time: 3480.64400100708\n",
      "47000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.64418562  0.05795925  0.00645216  0.05816782 -0.05158823  0.11047996\n",
      "  0.01631376  0.05288565 -0.07445024  0.08566491], error = [ 0.35581438 -0.05795925 -0.00645216 -0.05816782  0.05158823 -0.11047996\n",
      " -0.01631376 -0.05288565  0.07445024 -0.08566491]\n",
      "learn time: 3556.931853055954\n",
      "48000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.0217662  -0.02602002  0.13431978 -0.17378479  0.66484998  0.26357295\n",
      "  0.05442381  0.07986025 -0.0529506  -0.03271949], error = [-0.0217662   0.02602002 -0.13431978  0.17378479  0.33515002 -0.26357295\n",
      " -0.05442381 -0.07986025  0.0529506   0.03271949]\n",
      "learn time: 3632.4057092666626\n",
      "49000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.02114151  0.05596195 -0.05841605  0.01582608  0.59462671 -0.08739843\n",
      "  0.21846152  0.05861007  0.04699856  0.20979177], error = [-0.02114151 -0.05596195  0.05841605 -0.01582608  0.40537329  0.08739843\n",
      " -0.21846152 -0.05861007 -0.04699856 -0.20979177]\n",
      "learn time: 3708.1711750030518\n",
      "50000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.009846    0.1886909   0.34225314  0.17096644  0.13098205  0.27786027\n",
      " -0.15079494 -0.13110828  0.24778918 -0.30610706], error = [-0.009846   -0.1886909  -0.34225314  0.82903356 -0.13098205 -0.27786027\n",
      "  0.15079494  0.13110828 -0.24778918  0.30610706]\n",
      "learn time: 3785.0633516311646\n",
      "51000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.04287785  0.01759633  0.01713977 -0.1263649  -0.13534314  0.13906376\n",
      " -0.0011067   1.03382286 -0.04587849  0.16842665], error = [-0.04287785 -0.01759633 -0.01713977  0.1263649   0.13534314 -0.13906376\n",
      "  0.0011067  -0.03382286  0.04587849 -0.16842665]\n",
      "learn time: 3861.0014567375183\n",
      "52000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.01702348  0.01327262  0.04185976  0.15933637 -0.06871821  0.02107946\n",
      "  0.91585132 -0.07548075  0.0921419   0.00492445], error = [-0.01702348 -0.01327262 -0.04185976 -0.15933637  0.06871821 -0.02107946\n",
      "  0.08414868  0.07548075 -0.0921419  -0.00492445]\n",
      "learn time: 3936.7470932006836\n",
      "53000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 1.29682369e-01 -1.65837096e-01  1.17569393e-01 -7.51370348e-02\n",
      "  1.13211031e-01  1.15109681e+00 -6.47629994e-02 -5.20641371e-02\n",
      " -1.94456415e-04 -2.20008416e-01], error = [-1.29682369e-01  1.65837096e-01 -1.17569393e-01  7.51370348e-02\n",
      " -1.13211031e-01 -1.51096809e-01  6.47629994e-02  5.20641371e-02\n",
      "  1.94456415e-04  2.20008416e-01]\n",
      "learn time: 4012.6906776428223\n",
      "54000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.05030628 -0.05701662 -0.08625212  0.03816659  0.11718374  0.59685779\n",
      " -0.03579822 -0.08818125  0.30457865  0.12224705], error = [-0.05030628  0.05701662  0.08625212 -0.03816659 -0.11718374  0.40314221\n",
      "  0.03579822  0.08818125 -0.30457865 -0.12224705]\n",
      "learn time: 4088.6179399490356\n",
      "55000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.01154208  0.89200305 -0.00476457  0.08201571  0.09536107 -0.00297036\n",
      "  0.0194375   0.04915293 -0.03259021  0.00218284], error = [-0.01154208  0.10799695  0.00476457 -0.08201571 -0.09536107  0.00297036\n",
      " -0.0194375  -0.04915293  0.03259021 -0.00218284]\n",
      "learn time: 4164.043959140778\n",
      "56000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.00694353  1.00642352 -0.02837593  0.06590624  0.09568661 -0.01200745\n",
      " -0.02291747  0.11408942 -0.11495154  0.07660321], error = [-0.00694353 -0.00642352  0.02837593 -0.06590624 -0.09568661  0.01200745\n",
      "  0.02291747 -0.11408942  0.11495154 -0.07660321]\n",
      "learn time: 4239.629960536957\n",
      "57000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.83302807 -0.16923924 -0.04375194  0.12312361 -0.01778948 -0.08567601\n",
      " -0.0944412  -0.04649051 -0.01463345 -0.03022507], error = [ 0.16697193  0.16923924  0.04375194 -0.12312361  0.01778948  0.08567601\n",
      "  0.0944412   0.04649051  0.01463345  0.03022507]\n",
      "learn time: 4315.234642982483\n",
      "58000: expected = [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.05874201  0.04235053  0.46538011  0.09746419 -0.01323437 -0.29107342\n",
      " -0.07486767  0.17092483  0.56914286 -0.01275423], error = [-0.05874201 -0.04235053  0.53461989 -0.09746419  0.01323437  0.29107342\n",
      "  0.07486767 -0.17092483 -0.56914286  0.01275423]\n",
      "learn time: 4390.869877099991\n",
      "59000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.02954274  0.13227665 -0.09589291 -0.04952882 -0.01374168  0.01311596\n",
      "  1.06656061  0.04789011 -0.13983473 -0.00603342], error = [-0.02954274 -0.13227665  0.09589291  0.04952882  0.01374168 -0.01311596\n",
      " -0.06656061 -0.04789011  0.13983473  0.00603342]\n",
      "learn time: 4466.6361293792725\n",
      "60000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.03985699 -0.24171431  0.30151761  0.32631975 -0.01709484  0.78312217\n",
      " -0.00475251  0.15191887  0.04423607 -0.07024495], error = [-0.03985699  0.24171431 -0.30151761 -0.32631975  0.01709484  0.21687783\n",
      "  0.00475251 -0.15191887 -0.04423607  0.07024495]\n",
      "learn time: 4542.525108337402\n",
      "61000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.61926726  0.07122337 -0.05073367 -0.00359727 -0.01090444  0.09377457\n",
      "  0.02511215  0.10371849  0.10844428 -0.0563012 ], error = [ 0.38073274 -0.07122337  0.05073367  0.00359727  0.01090444 -0.09377457\n",
      " -0.02511215 -0.10371849 -0.10844428  0.0563012 ]\n",
      "learn time: 4618.218085765839\n",
      "62000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.18524777 -0.09666765  0.04131459 -0.07319786  0.01566009  0.53183136\n",
      "  0.2220249   0.00126864 -0.08425627  0.0684746 ], error = [-0.18524777  0.09666765 -0.04131459  0.07319786 -0.01566009  0.46816864\n",
      " -0.2220249  -0.00126864  0.08425627 -0.0684746 ]\n",
      "learn time: 4693.816561698914\n",
      "63000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.4038198   0.04550911  0.14791997 -0.02390114  0.12997391  0.05168685\n",
      "  0.06669207 -0.10899673  0.05410394  0.20316914], error = [-0.4038198  -0.04550911 -0.14791997  0.02390114 -0.12997391 -0.05168685\n",
      " -0.06669207  0.10899673 -0.05410394  0.79683086]\n",
      "learn time: 4769.864669084549\n",
      "64000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.01353269 -0.11963919  0.02302941  0.30765983  0.01919843 -0.11739982\n",
      " -0.01041829  0.84367993  0.03894984 -0.09216851], error = [-0.01353269  0.11963919 -0.02302941 -0.30765983 -0.01919843  0.11739982\n",
      "  0.01041829  0.15632007 -0.03894984  0.09216851]\n",
      "learn time: 4845.749418258667\n",
      "65000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.03060107 -0.15206029  0.03316038  0.14934469  0.13012195  0.12108499\n",
      "  0.2066995   0.3397559  -0.09613435  0.26158071], error = [-0.03060107  0.15206029 -0.03316038 -0.14934469 -0.13012195 -0.12108499\n",
      " -0.2066995   0.6602441   0.09613435 -0.26158071]\n",
      "learn time: 4921.530097484589\n",
      "66000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.05022557 -0.00191409 -0.00319333  0.02657631  0.00585909  0.04030113\n",
      "  1.0685051   0.00239162 -0.07889832 -0.06657451], error = [-0.05022557  0.00191409  0.00319333 -0.02657631 -0.00585909 -0.04030113\n",
      " -0.0685051  -0.00239162  0.07889832  0.06657451]\n",
      "learn time: 4997.648615121841\n",
      "67000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.00793257 -0.01598193  0.03420735  0.19889782  0.18569878  0.24523908\n",
      " -0.03426043 -0.03282965  0.28506436  0.05115282], error = [-0.00793257  0.01598193 -0.03420735 -0.19889782 -0.18569878 -0.24523908\n",
      "  0.03426043  0.03282965  0.71493564 -0.05115282]\n",
      "learn time: 5073.2059762477875\n",
      "68000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.83387961  0.03135188  0.07339503 -0.07518467  0.06426116 -0.07092242\n",
      " -0.07820616  0.00319073  0.11765839 -0.03333859], error = [ 0.16612039 -0.03135188 -0.07339503  0.07518467 -0.06426116  0.07092242\n",
      "  0.07820616 -0.00319073 -0.11765839  0.03333859]\n",
      "learn time: 5149.118563175201\n",
      "69000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.06427494  0.07972463 -0.04614834 -0.05585315  0.15536562 -0.03486654\n",
      "  0.77754481 -0.15059869  0.20086648  0.06762036], error = [-0.06427494 -0.07972463  0.04614834  0.05585315 -0.15536562  0.03486654\n",
      "  0.22245519  0.15059869 -0.20086648 -0.06762036]\n",
      "learn time: 5224.821985006332\n",
      "70000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.01059585 -0.09412732 -0.16355105  1.09458202  0.00762808  0.05593488\n",
      "  0.12057938  0.10122759 -0.07618109 -0.07822744], error = [-0.01059585  0.09412732  0.16355105 -0.09458202 -0.00762808 -0.05593488\n",
      " -0.12057938 -0.10122759  0.07618109  0.07822744]\n",
      "learn time: 5300.654104709625\n",
      "71000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.01501837  0.09641682 -0.07612103  0.11586283  0.0326256  -0.12052019\n",
      "  1.18103469 -0.00396421 -0.07423995 -0.0249919 ], error = [-0.01501837 -0.09641682  0.07612103 -0.11586283 -0.0326256   0.12052019\n",
      " -0.18103469  0.00396421  0.07423995  0.0249919 ]\n",
      "learn time: 5376.243463754654\n",
      "72000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.09279976 -0.02851747  0.24845663  0.12329897 -0.04130159  0.05858123\n",
      " -0.01573572  0.81390621  0.20830152 -0.17986281], error = [-0.09279976  0.02851747 -0.24845663 -0.12329897  0.04130159 -0.05858123\n",
      "  0.01573572  0.18609379 -0.20830152  0.17986281]\n",
      "learn time: 5452.072510719299\n",
      "73000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.03024622 -0.03548325  0.01049709 -0.09847625  0.056035    0.29481015\n",
      " -0.12103033  0.86936328 -0.12714183  0.13116926], error = [-0.03024622  0.03548325 -0.01049709  0.09847625 -0.056035   -0.29481015\n",
      "  0.12103033  0.13063672  0.12714183 -0.13116926]\n",
      "learn time: 5527.945714712143\n",
      "74000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.00198113  1.10317458 -0.11749753 -0.11094919 -0.00616967  0.00488089\n",
      "  0.09863788 -0.04756772 -0.04984659  0.09278601], error = [-0.00198113 -0.10317458  0.11749753  0.11094919  0.00616967 -0.00488089\n",
      " -0.09863788  0.04756772  0.04984659 -0.09278601]\n",
      "learn time: 5603.9110560417175\n",
      "75000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.06550887 -0.04431536 -0.13594568  0.00902854  0.17132306  0.48994711\n",
      "  0.17814869 -0.03739572 -0.02590617  0.02817847], error = [-0.06550887  0.04431536  0.13594568 -0.00902854 -0.17132306  0.51005289\n",
      " -0.17814869  0.03739572  0.02590617 -0.02817847]\n",
      "learn time: 5679.285253047943\n",
      "76000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.02718009 -0.00140296  0.03081533 -0.04806865 -0.04732594 -0.05326789\n",
      " -0.03512399 -0.03715268  1.00755421 -0.00684618], error = [-0.02718009  0.00140296 -0.03081533  0.04806865  0.04732594  0.05326789\n",
      "  0.03512399  0.03715268 -0.00755421  0.00684618]\n",
      "learn time: 5754.786130666733\n",
      "77000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.09279324 -0.04401058  0.14224706  0.48444692  0.20890287  0.13369979\n",
      "  0.02772484  0.17288847 -0.15966701  0.00613075], error = [-0.09279324  0.04401058 -0.14224706  0.51555308 -0.20890287 -0.13369979\n",
      " -0.02772484 -0.17288847  0.15966701 -0.00613075]\n",
      "learn time: 5830.671563863754\n",
      "78000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.02470094  0.05721628  0.15226768 -0.01418827  0.55334885 -0.03269493\n",
      "  0.13004896 -0.03048755 -0.02320043  0.41246166], error = [-0.02470094 -0.05721628 -0.15226768  0.01418827  0.44665115  0.03269493\n",
      " -0.13004896  0.03048755  0.02320043 -0.41246166]\n",
      "learn time: 5906.965786933899\n",
      "79000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.01457047  0.02549393  0.25538234  0.01313788  0.01971877 -0.16672714\n",
      " -0.18790754  0.09912276  0.67996246 -0.08655483], error = [-0.01457047 -0.02549393 -0.25538234 -0.01313788 -0.01971877  0.16672714\n",
      "  0.18790754 -0.09912276  0.32003754  0.08655483]\n",
      "learn time: 5982.47460103035\n",
      "80000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.27711825  0.03809599 -0.10946575 -0.16373212  0.07042688  0.89564605\n",
      " -0.00278388 -0.02157264  0.03130052 -0.09836859], error = [-0.27711825 -0.03809599  0.10946575  0.16373212 -0.07042688  0.10435395\n",
      "  0.00278388  0.02157264 -0.03130052  0.09836859]\n",
      "learn time: 6058.273810863495\n",
      "81000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.01790784  0.03355667  0.05305559 -0.06400604 -0.08383941  0.00802178\n",
      " -0.08593648  0.59037333 -0.05672088  0.40847036], error = [-0.01790784 -0.03355667 -0.05305559  0.06400604  0.08383941 -0.00802178\n",
      "  0.08593648  0.40962667  0.05672088 -0.40847036]\n",
      "learn time: 6133.9817769527435\n",
      "82000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00433879  0.62028973 -0.14500912  0.79995749 -0.02146232 -0.14481267\n",
      "  0.06134773  0.05493034 -0.10380096 -0.11705857], error = [-0.00433879 -0.62028973  0.14500912  0.20004251  0.02146232  0.14481267\n",
      " -0.06134773 -0.05493034  0.10380096  0.11705857]\n",
      "learn time: 6209.402323484421\n",
      "83000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.04788926  0.01489125 -0.02700034  0.05202841 -0.03446231 -0.10084508\n",
      " -0.07716066  1.00836742  0.04781269  0.00595475], error = [-0.04788926 -0.01489125  0.02700034 -0.05202841  0.03446231  0.10084508\n",
      "  0.07716066 -0.00836742 -0.04781269 -0.00595475]\n",
      "learn time: 6285.04607129097\n",
      "84000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.01523547 -0.03724237  0.1410259   0.21191326  0.06816366 -0.04763062\n",
      " -0.01477235  0.03890192  0.59928301 -0.08394602], error = [-0.01523547  0.03724237 -0.1410259  -0.21191326 -0.06816366  0.04763062\n",
      "  0.01477235 -0.03890192  0.40071699  0.08394602]\n",
      "learn time: 6360.735436677933\n",
      "85000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00309009  0.22653335  0.02392087  0.1913288  -0.01047333 -0.027111\n",
      " -0.10513894  0.36345149  0.07945004  0.25724374], error = [-0.00309009 -0.22653335 -0.02392087  0.8086712   0.01047333  0.027111\n",
      "  0.10513894 -0.36345149 -0.07945004 -0.25724374]\n",
      "learn time: 6436.69557595253\n",
      "86000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.00500646  0.00596541  0.08603029 -0.01498569  0.77594584  0.00971188\n",
      " -0.02873612  0.09655864 -0.08037635  0.17094489], error = [-0.00500646 -0.00596541 -0.08603029  0.01498569  0.22405416 -0.00971188\n",
      "  0.02873612 -0.09655864  0.08037635 -0.17094489]\n",
      "learn time: 6512.101197004318\n",
      "87000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.1532063  -0.0428469  -0.06795248 -0.03251347  0.34159033  0.40604093\n",
      "  0.0757244  -0.01601981  0.0539182  -0.05156635], error = [-0.1532063   0.0428469   0.06795248  0.03251347 -0.34159033  0.59395907\n",
      " -0.0757244   0.01601981 -0.0539182   0.05156635]\n",
      "learn time: 6587.636021137238\n",
      "88000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 3.02256789e-03  8.82716333e-01 -9.29453394e-03 -7.64925563e-02\n",
      " -3.83953764e-02  1.65655183e-01  2.75052917e-02  5.39546316e-02\n",
      "  1.25440002e-01  3.22264508e-04], error = [-0.00302257  0.11728367  0.00929453  0.07649256  0.03839538 -0.16565518\n",
      " -0.02750529 -0.05395463 -0.12544    -0.00032226]\n",
      "learn time: 6663.233074188232\n",
      "89000: expected = [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.00628038  0.10356752  0.79223264  0.23699472  0.07555157  0.01342772\n",
      " -0.07901009  0.02250552  0.10869113 -0.07152601], error = [-0.00628038 -0.10356752  0.20776736 -0.23699472 -0.07555157 -0.01342772\n",
      "  0.07901009 -0.02250552 -0.10869113  0.07152601]\n",
      "learn time: 6738.800126314163\n",
      "90000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00988296  0.07653521 -0.0997223   1.13569562 -0.08362353 -0.12347729\n",
      "  0.08407639  0.01017658  0.04408776  0.1170954 ], error = [-0.00988296 -0.07653521  0.0997223  -0.13569562  0.08362353  0.12347729\n",
      " -0.08407639 -0.01017658 -0.04408776 -0.1170954 ]\n",
      "learn time: 6814.5340230464935\n",
      "91000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.19455636 -0.05393534 -0.03069644 -0.15317325  0.26613386  0.58225202\n",
      "  0.4092643   0.02319503 -0.03475402 -0.18904612], error = [-0.19455636  0.05393534  0.03069644  0.15317325 -0.26613386 -0.58225202\n",
      "  0.5907357  -0.02319503  0.03475402  0.18904612]\n",
      "learn time: 6890.113414049149\n",
      "92000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.01227864  0.06167297  0.09897382  0.01703147  0.03128357  0.05733053\n",
      "  0.07069876  0.01454549  0.81743024 -0.1630505 ], error = [-0.01227864 -0.06167297 -0.09897382 -0.01703147 -0.03128357 -0.05733053\n",
      " -0.07069876 -0.01454549  0.18256976  0.1630505 ]\n",
      "learn time: 6965.74040722847\n",
      "93000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.01105844 -0.02870921  0.00322118  0.33218596  0.0991782   0.54121642\n",
      "  0.04035369  0.02695827  0.07945661  0.01079567], error = [-0.01105844  0.02870921 -0.00322118  0.66781404 -0.0991782  -0.54121642\n",
      " -0.04035369 -0.02695827 -0.07945661 -0.01079567]\n",
      "learn time: 7041.499539852142\n",
      "94000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.02083235 -0.040529    0.07367889 -0.00710038  0.50256017 -0.02772941\n",
      "  0.06506446  0.0079271  -0.11851171  0.62882734], error = [-0.02083235  0.040529   -0.07367889  0.00710038  0.49743983  0.02772941\n",
      " -0.06506446 -0.0079271   0.11851171 -0.62882734]\n",
      "learn time: 7117.174373149872\n",
      "95000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.00171332  1.04754515 -0.0634394  -0.02103547  0.03467392  0.06014057\n",
      " -0.01678787 -0.08083192 -0.01859174 -0.08749656], error = [-0.00171332 -0.04754515  0.0634394   0.02103547 -0.03467392 -0.06014057\n",
      "  0.01678787  0.08083192  0.01859174  0.08749656]\n",
      "learn time: 7192.86884355545\n",
      "96000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.00933308  0.09612281 -0.05569785  0.02357587  0.18198099 -0.05511981\n",
      " -0.01023495 -0.08199813  0.23715612  0.71302236], error = [-0.00933308 -0.09612281  0.05569785 -0.02357587 -0.18198099  0.05511981\n",
      "  0.01023495  0.08199813 -0.23715612  0.28697764]\n",
      "learn time: 7268.174766540527\n",
      "97000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.01006734 -0.04088863 -0.02278669  0.01021749  0.86822285  0.16416891\n",
      "  0.23889671 -0.07976736  0.02382688 -0.18005203], error = [-0.01006734  0.04088863  0.02278669 -0.01021749  0.13177715 -0.16416891\n",
      " -0.23889671  0.07976736 -0.02382688  0.18005203]\n",
      "learn time: 7343.80202460289\n",
      "98000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.01344773 -0.03350128  0.02719721  0.01943389  0.00161111  0.12298085\n",
      "  0.03331671  0.10932003  0.54885114  0.18008503], error = [-0.01344773  0.03350128 -0.02719721 -0.01943389 -0.00161111 -0.12298085\n",
      " -0.03331671 -0.10932003  0.45114886 -0.18008503]\n",
      "learn time: 7419.2970242500305\n",
      "99000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.00609967 -0.10983269 -0.08561991  0.14973719  0.4942248   0.0536985\n",
      "  0.01451157 -0.0044027   0.07321818  0.30839061], error = [-0.00609967  0.10983269  0.08561991 -0.14973719 -0.4942248  -0.0536985\n",
      " -0.01451157  0.0044027  -0.07321818  0.69160939]\n",
      "learn time: 7494.79158616066\n",
      "100000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.00969438  0.05005265 -0.04849629 -0.02596336 -0.02941888  0.21704495\n",
      "  0.0381339   0.87671815 -0.0194713   0.2218174 ], error = [-0.00969438 -0.05005265  0.04849629  0.02596336  0.02941888 -0.21704495\n",
      " -0.0381339   0.12328185  0.0194713  -0.2218174 ]\n",
      "learn time: 7570.467236280441\n",
      "101000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.02241804 -0.07783825  0.05353515 -0.0767904   0.31315036  0.4584424\n",
      " -0.09964701  0.02665188 -0.05701989  0.24470932], error = [-0.02241804  0.07783825 -0.05353515  0.0767904  -0.31315036  0.5415576\n",
      "  0.09964701 -0.02665188  0.05701989 -0.24470932]\n",
      "learn time: 7646.614457607269\n",
      "102000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.00256398  1.0681779  -0.09442388  0.06774581 -0.04124067 -0.09312025\n",
      "  0.04296804  0.0771779   0.04725646 -0.06593793], error = [-0.00256398 -0.0681779   0.09442388 -0.06774581  0.04124067  0.09312025\n",
      " -0.04296804 -0.0771779  -0.04725646  0.06593793]\n",
      "learn time: 7722.208807229996\n",
      "103000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.0098751  -0.02468401  0.06854653  0.13996966  0.08483487  0.08966867\n",
      " -0.05288438  0.23278891 -0.01656032  0.64589201], error = [-0.0098751   0.02468401 -0.06854653 -0.13996966 -0.08483487 -0.08966867\n",
      "  0.05288438 -0.23278891  0.01656032  0.35410799]\n",
      "learn time: 7797.858823060989\n",
      "104000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 8.40969913e-01  1.36543760e-01  1.49990670e-04  3.49322296e-02\n",
      " -1.06437772e-01  6.30304005e-02 -6.59254484e-02  3.71176260e-02\n",
      " -6.22890959e-02  1.25849645e-01], error = [ 1.59030087e-01 -1.36543760e-01 -1.49990670e-04 -3.49322296e-02\n",
      "  1.06437772e-01 -6.30304005e-02  6.59254484e-02 -3.71176260e-02\n",
      "  6.22890959e-02 -1.25849645e-01]\n",
      "learn time: 7873.441700696945\n",
      "105000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00682988 -0.0197039   0.06036211  0.69186722 -0.02836713  0.20557127\n",
      " -0.02813278 -0.07420804  0.1702482   0.07178135], error = [-0.00682988  0.0197039  -0.06036211  0.30813278  0.02836713 -0.20557127\n",
      "  0.02813278  0.07420804 -0.1702482  -0.07178135]\n",
      "learn time: 7948.9329833984375\n",
      "106000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.01101649 -0.05649603  0.15049685 -0.01849066  0.03188477  0.03540642\n",
      " -0.06838656  0.04273448 -0.12466125  1.0508943 ], error = [-0.01101649  0.05649603 -0.15049685  0.01849066 -0.03188477 -0.03540642\n",
      "  0.06838656 -0.04273448  0.12466125 -0.0508943 ]\n",
      "learn time: 8024.5926859378815\n",
      "107000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.81812103  0.03729269  0.06417266 -0.00738092 -0.03828637  0.03809494\n",
      "  0.02501856  0.05014965 -0.04082278  0.05818907], error = [ 0.18187897 -0.03729269 -0.06417266  0.00738092  0.03828637 -0.03809494\n",
      " -0.02501856 -0.05014965  0.04082278 -0.05818907]\n",
      "learn time: 8100.765620946884\n",
      "108000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.00576539 -0.0868492   0.09836738 -0.13109269  0.85943603  0.19722941\n",
      " -0.00337351  0.07041082  0.02752873 -0.1190154 ], error = [-0.00576539  0.0868492  -0.09836738  0.13109269  0.14056397 -0.19722941\n",
      "  0.00337351 -0.07041082 -0.02752873  0.1190154 ]\n",
      "learn time: 8176.413361549377\n",
      "109000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.00672684  0.08067512 -0.00752942 -0.00471014  0.78089242 -0.02553106\n",
      "  0.10081091  0.02781916 -0.01406645  0.14764609], error = [-0.00672684 -0.08067512  0.00752942  0.00471014  0.21910758  0.02553106\n",
      " -0.10081091 -0.02781916  0.01406645 -0.14764609]\n",
      "learn time: 8251.938837766647\n",
      "110000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00319004  0.15167327  0.27892571  0.27152834  0.07960119  0.20508148\n",
      " -0.11441578 -0.06316821  0.19704861 -0.16043707], error = [-0.00319004 -0.15167327 -0.27892571  0.72847166 -0.07960119 -0.20508148\n",
      "  0.11441578  0.06316821 -0.19704861  0.16043707]\n",
      "learn time: 8327.494397878647\n",
      "111000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.0201219   0.00857364  0.05672542 -0.17820433 -0.06089894  0.06259524\n",
      " -0.03224477  1.13455612 -0.01096515  0.05984108], error = [-0.0201219  -0.00857364 -0.05672542  0.17820433  0.06089894 -0.06259524\n",
      "  0.03224477 -0.13455612  0.01096515 -0.05984108]\n",
      "learn time: 8403.006306648254\n",
      "112000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 7.05250848e-03  4.04585074e-03 -6.57754304e-04  1.70090217e-01\n",
      " -9.95939093e-02  2.86584295e-02  9.99237489e-01 -3.19986537e-02\n",
      "  7.26451347e-02  1.18074906e-02], error = [-0.00705251 -0.00404585  0.00065775 -0.17009022  0.09959391 -0.02865843\n",
      "  0.00076251  0.03199865 -0.07264513 -0.01180749]\n",
      "learn time: 8478.841455698013\n",
      "113000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.07477294  0.03420148  0.21660139 -0.07165778 -0.12534     1.22689589\n",
      "  0.01860108 -0.10960356 -0.01222505 -0.09185103], error = [-0.07477294 -0.03420148 -0.21660139  0.07165778  0.12534    -0.22689589\n",
      " -0.01860108  0.10960356  0.01222505  0.09185103]\n",
      "learn time: 8554.436475276947\n",
      "114000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.01639072  0.01285878 -0.13393228  0.08065309 -0.02041459  0.62447143\n",
      " -0.02885923  0.00824491  0.35511769  0.07377916], error = [-0.01639072 -0.01285878  0.13393228 -0.08065309  0.02041459  0.37552857\n",
      "  0.02885923 -0.00824491 -0.35511769 -0.07377916]\n",
      "learn time: 8630.254068136215\n",
      "115000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.00202508  0.89854964 -0.01448687  0.06848636  0.05194653 -0.01555916\n",
      "  0.0532326   0.00432985 -0.03765605  0.03347347], error = [-0.00202508  0.10145036  0.01448687 -0.06848636 -0.05194653  0.01555916\n",
      " -0.0532326  -0.00432985  0.03765605 -0.03347347]\n",
      "learn time: 8706.017395973206\n",
      "116000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.00143142  1.02916608 -0.01172853  0.07216679  0.01925368 -0.00370367\n",
      "  0.0215313   0.04678886 -0.09332699  0.11690764], error = [-0.00143142 -0.02916608  0.01172853 -0.07216679 -0.01925368  0.00370367\n",
      " -0.0215313  -0.04678886  0.09332699 -0.11690764]\n",
      "learn time: 8782.000097990036\n",
      "117000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.90976911 -0.15611816 -0.01918039  0.07082887 -0.06410789 -0.02209775\n",
      " -0.05030751 -0.00093895  0.05614733 -0.03458822], error = [ 0.09023089  0.15611816  0.01918039 -0.07082887  0.06410789  0.02209775\n",
      "  0.05030751  0.00093895 -0.05614733  0.03458822]\n",
      "learn time: 8857.94938659668\n",
      "118000: expected = [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.0264277  -0.01715095  0.62009872  0.07840851 -0.00355321 -0.12755228\n",
      " -0.04860669  0.10309146  0.56932583 -0.04963969], error = [-0.0264277   0.01715095  0.37990128 -0.07840851  0.00355321  0.12755228\n",
      "  0.04860669 -0.10309146 -0.56932583  0.04963969]\n",
      "learn time: 8933.709506988525\n",
      "119000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.02212343  0.12799345 -0.04546827 -0.01111039 -0.02324237  0.00979128\n",
      "  1.03184222  0.02195512 -0.10440044 -0.03137992], error = [-0.02212343 -0.12799345  0.04546827  0.01111039  0.02324237 -0.00979128\n",
      " -0.03184222 -0.02195512  0.10440044  0.03137992]\n",
      "learn time: 9009.495739936829\n",
      "120000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.02224442 -0.15694138  0.28630792  0.22849844 -0.07613849  0.85773027\n",
      " -0.03187698  0.04990421 -0.14142964  0.09525693], error = [-0.02224442  0.15694138 -0.28630792 -0.22849844  0.07613849  0.14226973\n",
      "  0.03187698 -0.04990421  0.14142964 -0.09525693]\n",
      "learn time: 9085.365167856216\n",
      "121000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.80046099  0.05277696 -0.01396891 -0.03292818  0.05432085  0.08265265\n",
      "  0.0205607   0.05067612  0.01022453 -0.04802529], error = [ 0.19953901 -0.05277696  0.01396891  0.03292818 -0.05432085 -0.08265265\n",
      " -0.0205607  -0.05067612 -0.01022453  0.04802529]\n",
      "learn time: 9161.368226766586\n",
      "122000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.1461741   0.00351253  0.09270247 -0.03830475 -0.00719558  0.51851479\n",
      "  0.28700316  0.07890238 -0.07391513  0.01418189], error = [-0.1461741  -0.00351253 -0.09270247  0.03830475  0.00719558  0.48148521\n",
      " -0.28700316 -0.07890238  0.07391513 -0.01418189]\n",
      "learn time: 9237.203137159348\n",
      "123000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.45814146  0.02518127  0.16905801 -0.09227776  0.1418256   0.02197761\n",
      "  0.02065985 -0.0904924   0.00436928  0.31039838], error = [-0.45814146 -0.02518127 -0.16905801  0.09227776 -0.1418256  -0.02197761\n",
      " -0.02065985  0.0904924  -0.00436928  0.68960162]\n",
      "learn time: 9313.013338804245\n",
      "124000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.0065645  -0.06522851 -0.02230957  0.26032479  0.05827838 -0.06803917\n",
      "  0.0225636   0.87606711  0.03310323 -0.17737667], error = [-0.0065645   0.06522851  0.02230957 -0.26032479 -0.05827838  0.06803917\n",
      " -0.0225636   0.12393289 -0.03310323  0.17737667]\n",
      "learn time: 9389.128000974655\n",
      "125000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.01316351 -0.11204359  0.00862335  0.16893809  0.03439967  0.21135484\n",
      "  0.14189582  0.34614363 -0.18804524  0.29628291], error = [-0.01316351  0.11204359 -0.00862335 -0.16893809 -0.03439967 -0.21135484\n",
      " -0.14189582  0.65385637  0.18804524 -0.29628291]\n",
      "learn time: 9464.749418497086\n",
      "126000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.02800076  0.0201409   0.00984855  0.06119741 -0.03196525  0.01339455\n",
      "  1.05055656  0.00212393 -0.07578803 -0.04302027], error = [-0.02800076 -0.0201409  -0.00984855 -0.06119741  0.03196525 -0.01339455\n",
      " -0.05055656 -0.00212393  0.07578803  0.04302027]\n",
      "learn time: 9540.590946435928\n",
      "127000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.00224169 -0.0186889  -0.01455816  0.23510235  0.21019647  0.20051273\n",
      "  0.00336974 -0.0164278   0.37304965 -0.0758502 ], error = [-0.00224169  0.0186889   0.01455816 -0.23510235 -0.21019647 -0.20051273\n",
      " -0.00336974  0.0164278   0.62695035  0.0758502 ]\n",
      "learn time: 9616.08680176735\n",
      "128000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.91014045  0.00120038  0.02710443 -0.05383271  0.08272342 -0.09957395\n",
      " -0.06958451 -0.00497643  0.05715449 -0.01018034], error = [ 0.08985955 -0.00120038 -0.02710443  0.05383271 -0.08272342  0.09957395\n",
      "  0.06958451  0.00497643 -0.05715449  0.01018034]\n",
      "learn time: 9692.020131349564\n",
      "129000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.03536233  0.00520737 -0.02018567 -0.0735264   0.14448416 -0.03779505\n",
      "  0.82043251 -0.11550798  0.10317554  0.08759078], error = [-0.03536233 -0.00520737  0.02018567  0.0735264  -0.14448416  0.03779505\n",
      "  0.17956749  0.11550798 -0.10317554 -0.08759078]\n",
      "learn time: 9767.501824617386\n",
      "130000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00413061 -0.04597121 -0.11850699  1.1574851   0.00815325  0.0810038\n",
      "  0.06756606  0.06627151 -0.10079026 -0.05612582], error = [-0.00413061  0.04597121  0.11850699 -0.1574851  -0.00815325 -0.0810038\n",
      " -0.06756606 -0.06627151  0.10079026  0.05612582]\n",
      "learn time: 9843.176250219345\n",
      "131000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 1.17435069e-02  9.39551861e-02 -5.85253643e-02  8.69947693e-02\n",
      "  2.15920483e-04 -9.23231991e-02  1.16690232e+00 -2.95202916e-02\n",
      " -4.57028167e-02 -4.88564127e-02], error = [-0.01174351 -0.09395519  0.05852536 -0.08699477 -0.00021592  0.0923232\n",
      " -0.16690232  0.02952029  0.04570282  0.04885641]\n",
      "learn time: 9918.894161462784\n",
      "132000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.08463805 -0.09342127  0.26992086  0.05453067 -0.00852588  0.07525293\n",
      "  0.00257345  0.75448223  0.17712419 -0.21288379], error = [-0.08463805  0.09342127 -0.26992086 -0.05453067  0.00852588 -0.07525293\n",
      " -0.00257345  0.24551777 -0.17712419  0.21288379]\n",
      "learn time: 9994.846557855606\n",
      "133000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.01690871 -0.0364448   0.0278162  -0.08156765  0.0548387   0.28097285\n",
      " -0.06835428  0.90494558 -0.09640718  0.13519912], error = [-0.01690871  0.0364448  -0.0278162   0.08156765 -0.0548387  -0.28097285\n",
      "  0.06835428  0.09505442  0.09640718 -0.13519912]\n",
      "learn time: 10070.891125679016\n",
      "134000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 5.80175367e-04  1.14111500e+00 -7.06844186e-02 -1.51722439e-01\n",
      " -7.10125689e-03  2.67374869e-02  7.16185011e-02 -4.96309309e-02\n",
      " -3.70322081e-02  1.15296680e-01], error = [-0.00058018 -0.141115    0.07068442  0.15172244  0.00710126 -0.02673749\n",
      " -0.0716185   0.04963093  0.03703221 -0.11529668]\n",
      "learn time: 10146.471381902695\n",
      "135000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.04333286 -0.00953927 -0.11832602 -0.02781497  0.07591655  0.66080838\n",
      "  0.21174493  0.04946479  0.02495662 -0.01843409], error = [-0.04333286  0.00953927  0.11832602  0.02781497 -0.07591655  0.33919162\n",
      " -0.21174493 -0.04946479 -0.02495662  0.01843409]\n",
      "learn time: 10222.144693613052\n",
      "136000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.01377876  0.00834876 -0.02344015 -0.00940894 -0.02645036 -0.07404513\n",
      " -0.07251003  0.01816976  1.17627449 -0.11168609], error = [-0.01377876 -0.00834876  0.02344015  0.00940894  0.02645036  0.07404513\n",
      "  0.07251003 -0.01816976 -0.17627449  0.11168609]\n",
      "learn time: 10297.740117073059\n",
      "137000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.06642354 -0.06085671  0.14691603  0.56079091  0.18260728  0.04497312\n",
      "  0.05613132  0.09554519 -0.08433456  0.02841569], error = [-0.06642354  0.06085671 -0.14691603  0.43920909 -0.18260728 -0.04497312\n",
      " -0.05613132 -0.09554519  0.08433456 -0.02841569]\n",
      "learn time: 10373.567880868912\n",
      "138000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.0116103   0.05141628  0.15405174 -0.02655145  0.66412649 -0.07216568\n",
      "  0.08170628 -0.03622506 -0.03223816  0.34632728], error = [-0.0116103  -0.05141628 -0.15405174  0.02655145  0.33587351  0.07216568\n",
      " -0.08170628  0.03622506  0.03223816 -0.34632728]\n",
      "learn time: 10449.524161815643\n",
      "139000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.00791457  0.04321778  0.1526126   0.04032483  0.02038553 -0.08700458\n",
      " -0.14560614  0.13421224  0.79402706 -0.14474475], error = [-0.00791457 -0.04321778 -0.1526126  -0.04032483 -0.02038553  0.08700458\n",
      "  0.14560614 -0.13421224  0.20597294  0.14474475]\n",
      "learn time: 10525.204614162445\n",
      "140000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.14324525  0.04107698 -0.09153267 -0.20514536  0.0744774   1.05417798\n",
      " -0.00309841  0.03386099 -0.00143405 -0.05623404], error = [-0.14324525 -0.04107698  0.09153267  0.20514536 -0.0744774  -0.05417798\n",
      "  0.00309841 -0.03386099  0.00143405  0.05623404]\n",
      "learn time: 10601.137593746185\n",
      "141000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.01115435 -0.02625306  0.01135281 -0.00312297 -0.02891982 -0.05218695\n",
      " -0.03076504  0.60469565 -0.04429843  0.3980223 ], error = [-0.01115435  0.02625306 -0.01135281  0.00312297  0.02891982  0.05218695\n",
      "  0.03076504  0.39530435  0.04429843 -0.3980223 ]\n",
      "learn time: 10676.767815828323\n",
      "142000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00207243  0.5844828  -0.13163115  0.86866007 -0.03162534 -0.17759953\n",
      "  0.08191094  0.00913467 -0.07684168 -0.12355661], error = [-0.00207243 -0.5844828   0.13163115  0.13133993  0.03162534  0.17759953\n",
      " -0.08191094 -0.00913467  0.07684168  0.12355661]\n",
      "learn time: 10752.416043281555\n",
      "143000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 2.74930478e-02  1.66726340e-02 -6.53015372e-02  8.99006765e-02\n",
      " -4.89960662e-05 -7.14893134e-02 -7.24037943e-02  9.65232883e-01\n",
      " -6.19360611e-02  2.10677852e-02], error = [-2.74930478e-02 -1.66726340e-02  6.53015372e-02 -8.99006765e-02\n",
      "  4.89960662e-05  7.14893134e-02  7.24037943e-02  3.47671167e-02\n",
      "  6.19360611e-02 -2.10677852e-02]\n",
      "learn time: 10827.944627285004\n",
      "144000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.011379   -0.03283428  0.08952705  0.16226571  0.03518716 -0.01186131\n",
      "  0.00705894  0.00964212  0.73739285 -0.0790679 ], error = [-0.011379    0.03283428 -0.08952705 -0.16226571 -0.03518716  0.01186131\n",
      " -0.00705894 -0.00964212  0.26260715  0.0790679 ]\n",
      "learn time: 10903.630960702896\n",
      "145000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00153508  0.20259253  0.0173193   0.20668305 -0.0038134  -0.02263484\n",
      " -0.10789952  0.40583829  0.07314906  0.23937405], error = [-0.00153508 -0.20259253 -0.0173193   0.79331695  0.0038134   0.02263484\n",
      "  0.10789952 -0.40583829 -0.07314906 -0.23937405]\n",
      "learn time: 10979.736948013306\n",
      "146000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.00210664 -0.01253257  0.04115026 -0.00129702  0.84705406  0.00138246\n",
      " -0.00993451  0.11133028 -0.13872698  0.12171186], error = [-0.00210664  0.01253257 -0.04115026  0.00129702  0.15294594 -0.00138246\n",
      "  0.00993451 -0.11133028  0.13872698 -0.12171186]\n",
      "learn time: 11055.552748203278\n",
      "147000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.0920628  -0.04807476 -0.12996867 -0.04304318  0.3047233   0.4243273\n",
      "  0.14439273  0.04554809  0.1079116  -0.06863952], error = [-0.0920628   0.04807476  0.12996867  0.04304318 -0.3047233   0.5756727\n",
      " -0.14439273 -0.04554809 -0.1079116   0.06863952]\n",
      "learn time: 11131.595663785934\n",
      "148000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.00147252  0.90572819 -0.00180355 -0.07019776 -0.03014666  0.15786459\n",
      "  0.01830207  0.04018036  0.09796246 -0.01013475], error = [-0.00147252  0.09427181  0.00180355  0.07019776  0.03014666 -0.15786459\n",
      " -0.01830207 -0.04018036 -0.09796246  0.01013475]\n",
      "learn time: 11207.630069971085\n",
      "149000: expected = [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.0028676   0.07306057  0.77970076  0.20455797  0.06570568  0.01894096\n",
      " -0.08811209  0.01173226  0.1279158  -0.04782157], error = [-0.0028676  -0.07306057  0.22029924 -0.20455797 -0.06570568 -0.01894096\n",
      "  0.08811209 -0.01173226 -0.1279158   0.04782157]\n",
      "learn time: 11283.342599153519\n",
      "150000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00483863  0.04661636 -0.07018527  1.14399162 -0.07415675 -0.09284074\n",
      "  0.05389775 -0.02596575  0.03375585  0.14669906], error = [-0.00483863 -0.04661636  0.07018527 -0.14399162  0.07415675  0.09284074\n",
      " -0.05389775  0.02596575 -0.03375585 -0.14669906]\n",
      "learn time: 11359.259999275208\n",
      "151000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.12643374 -0.03077527 -0.0017755  -0.19274663  0.12671895  0.65411965\n",
      "  0.46285329  0.03793426 -0.03236935 -0.08306834], error = [-0.12643374  0.03077527  0.0017755   0.19274663 -0.12671895 -0.65411965\n",
      "  0.53714671 -0.03793426  0.03236935  0.08306834]\n",
      "learn time: 11435.037615776062\n",
      "152000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 6.00827609e-03  3.18738455e-02  6.37419882e-02  3.11453227e-02\n",
      "  6.79229186e-02 -1.43164568e-02  5.06575191e-02  4.17462726e-04\n",
      "  9.19640451e-01 -1.35555890e-01], error = [-0.00600828 -0.03187385 -0.06374199 -0.03114532 -0.06792292  0.01431646\n",
      " -0.05065752 -0.00041746  0.08035955  0.13555589]\n",
      "learn time: 11511.287193059921\n",
      "153000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00749902 -0.04685199  0.00221884  0.34874765  0.08165309  0.59011928\n",
      "  0.03549437  0.04391054  0.05528615 -0.00686322], error = [-0.00749902  0.04685199 -0.00221884  0.65125235 -0.08165309 -0.59011928\n",
      " -0.03549437 -0.04391054 -0.05528615  0.00686322]\n",
      "learn time: 11587.205487251282\n",
      "154000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.00823282 -0.0053864  -0.00372858 -0.00210031  0.60482207 -0.03159111\n",
      "  0.03978487  0.01457161 -0.07677039  0.53487701], error = [-0.00823282  0.0053864   0.00372858  0.00210031  0.39517793  0.03159111\n",
      " -0.03978487 -0.01457161  0.07677039 -0.53487701]\n",
      "learn time: 11662.885503530502\n",
      "155000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 8.25543612e-04  1.05559437e+00 -3.81707221e-02 -2.64898990e-02\n",
      "  1.81695885e-02  8.34839632e-02 -2.12117436e-02 -6.84584272e-02\n",
      " -4.10720452e-03 -9.77608080e-02], error = [-0.00082554 -0.05559437  0.03817072  0.0264899  -0.01816959 -0.08348396\n",
      "  0.02121174  0.06845843  0.0041072   0.09776081]\n",
      "learn time: 11738.804880619049\n",
      "156000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.00506737  0.05889116 -0.01560199 -0.01416457  0.14652278 -0.04543808\n",
      "  0.00569557 -0.08426961  0.17965745  0.72070774], error = [-0.00506737 -0.05889116  0.01560199  0.01416457 -0.14652278  0.04543808\n",
      " -0.00569557  0.08426961 -0.17965745  0.27929226]\n",
      "learn time: 11814.68904209137\n",
      "157000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.00406106 -0.05349562 -0.03812088  0.01634185  0.88987777  0.10439685\n",
      "  0.17796749  0.00680813  0.01693239 -0.17675186], error = [-0.00406106  0.05349562  0.03812088 -0.01634185  0.11012223 -0.10439685\n",
      " -0.17796749 -0.00680813 -0.01693239  0.17675186]\n",
      "learn time: 11890.487143993378\n",
      "158000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.00987753 -0.03177556  0.01643003  0.03179718  0.00886056  0.08097096\n",
      "  0.04088143  0.05380542  0.63106833  0.16717374], error = [-0.00987753  0.03177556 -0.01643003 -0.03179718 -0.00886056 -0.08097096\n",
      " -0.04088143 -0.05380542  0.36893167 -0.16717374]\n",
      "learn time: 11966.406590223312\n",
      "159000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.00219689 -0.0671684  -0.07048739  0.11096458  0.52443329  0.05025009\n",
      "  0.00810094 -0.02671869  0.11186948  0.27916281], error = [-0.00219689  0.0671684   0.07048739 -0.11096458 -0.52443329 -0.05025009\n",
      " -0.00810094  0.02671869 -0.11186948  0.72083719]\n",
      "learn time: 12042.174743175507\n",
      "160000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.00636     0.04381965 -0.04889703 -0.08492239 -0.01595408  0.22215138\n",
      "  0.01121412  0.99512464 -0.07086953  0.11430305], error = [-0.00636    -0.04381965  0.04889703  0.08492239  0.01595408 -0.22215138\n",
      " -0.01121412  0.00487536  0.07086953 -0.11430305]\n",
      "learn time: 12118.178591966629\n",
      "161000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.01433716 -0.0398627   0.05463114 -0.08797661  0.29114449  0.54689396\n",
      " -0.10110733  0.04958818 -0.11466299  0.30270291], error = [-0.01433716  0.0398627  -0.05463114  0.08797661 -0.29114449  0.45310604\n",
      "  0.10110733 -0.04958818  0.11466299 -0.30270291]\n",
      "learn time: 12193.966679573059\n",
      "162000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.00119786  1.0822169  -0.10019002  0.04708016 -0.04212987 -0.07463784\n",
      "  0.04224577  0.08061973  0.05457283 -0.06939125], error = [-0.00119786 -0.0822169   0.10019002 -0.04708016  0.04212987  0.07463784\n",
      " -0.04224577 -0.08061973 -0.05457283  0.06939125]\n",
      "learn time: 12269.891952037811\n",
      "163000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.00470418 -0.02751988  0.02919561  0.17817325  0.08039063  0.08876618\n",
      " -0.03145828  0.20303966 -0.11566815  0.75246278], error = [-0.00470418  0.02751988 -0.02919561 -0.17817325 -0.08039063 -0.08876618\n",
      "  0.03145828 -0.20303966  0.11566815  0.24753722]\n",
      "learn time: 12345.595673322678\n",
      "164000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.89157782  0.08048505  0.00873964  0.00926129 -0.07701519  0.03654868\n",
      " -0.10017618  0.03911424 -0.00326238  0.10931963], error = [ 0.10842218 -0.08048505 -0.00873964 -0.00926129  0.07701519 -0.03654868\n",
      "  0.10017618 -0.03911424  0.00326238 -0.10931963]\n",
      "learn time: 12421.510994195938\n",
      "165000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00482239 -0.00748768  0.06985533  0.75630512 -0.01467219  0.18790787\n",
      " -0.04834632 -0.07312442  0.11315973  0.10615678], error = [-0.00482239  0.00748768 -0.06985533  0.24369488  0.01467219 -0.18790787\n",
      "  0.04834632  0.07312442 -0.11315973 -0.10615678]\n",
      "learn time: 12497.215071678162\n",
      "166000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.00557295 -0.0386207   0.12194424 -0.00213592  0.00555294  0.04379753\n",
      " -0.07279505  0.00922687 -0.17159565  1.0864767 ], error = [-0.00557295  0.0386207  -0.12194424  0.00213592 -0.00555294 -0.04379753\n",
      "  0.07279505 -0.00922687  0.17159565 -0.0864767 ]\n",
      "learn time: 12573.06925034523\n",
      "167000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.87260502  0.0180017   0.07402995 -0.02900075 -0.01495034  0.0256746\n",
      "  0.03427298  0.03553355 -0.01453783  0.05464629], error = [ 0.12739498 -0.0180017  -0.07402995  0.02900075  0.01495034 -0.0256746\n",
      " -0.03427298 -0.03553355  0.01453783 -0.05464629]\n",
      "learn time: 12648.68693780899\n",
      "168000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.00303543 -0.09203401  0.08266169 -0.09845167  0.94235101  0.1468186\n",
      " -0.00710889  0.06472785  0.0673272  -0.13536736], error = [-0.00303543  0.09203401 -0.08266169  0.09845167  0.05764899 -0.1468186\n",
      "  0.00710889 -0.06472785 -0.0673272   0.13536736]\n",
      "learn time: 12724.555750370026\n",
      "169000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.00405734  0.04471308 -0.01382822 -0.03027729  0.88499939  0.00487954\n",
      "  0.05397346 -0.01606974 -0.02016377  0.12902774], error = [-0.00405734 -0.04471308  0.01382822  0.03027729  0.11500061 -0.00487954\n",
      " -0.05397346  0.01606974  0.02016377 -0.12902774]\n",
      "learn time: 12800.293318510056\n",
      "170000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00223651  0.10806839  0.22029203  0.40691397  0.03139685  0.15407204\n",
      " -0.06203368 -0.04669976  0.1399829  -0.06168856], error = [-0.00223651 -0.10806839 -0.22029203  0.59308603 -0.03139685 -0.15407204\n",
      "  0.06203368  0.04669976 -0.1399829   0.06168856]\n",
      "learn time: 12875.772555112839\n",
      "171000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.0146902   0.00263421  0.08506487 -0.15329355 -0.01239457  0.0146837\n",
      " -0.0315519   1.15797932  0.02578299  0.01009126], error = [-0.0146902  -0.00263421 -0.08506487  0.15329355  0.01239457 -0.0146837\n",
      "  0.0315519  -0.15797932 -0.02578299 -0.01009126]\n",
      "learn time: 12951.742410898209\n",
      "172000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.00609373 -0.01472948 -0.00544389  0.13697616 -0.0901957   0.04473941\n",
      "  1.02898929 -0.02792805  0.02918892  0.01150958], error = [-0.00609373  0.01472948  0.00544389 -0.13697616  0.0901957  -0.04473941\n",
      " -0.02898929  0.02792805 -0.02918892 -0.01150958]\n",
      "learn time: 13027.572125673294\n",
      "173000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.04636652  0.05279561  0.23259749 -0.04368485 -0.17759082  1.11211268\n",
      "  0.04985105 -0.0756773   0.03549974 -0.08606156], error = [-0.04636652 -0.05279561 -0.23259749  0.04368485  0.17759082 -0.11211268\n",
      " -0.04985105  0.0756773  -0.03549974  0.08606156]\n",
      "learn time: 13103.123010158539\n",
      "174000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.00979456  0.04643387 -0.13788844  0.0471351  -0.01511119  0.61175075\n",
      " -0.00967013  0.01066852  0.3727662   0.03776528], error = [-0.00979456 -0.04643387  0.13788844 -0.0471351   0.01511119  0.38824925\n",
      "  0.00967013 -0.01066852 -0.3727662  -0.03776528]\n",
      "learn time: 13179.035473585129\n",
      "175000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 7.70322398e-04  9.06476448e-01 -1.44915456e-02  6.86444962e-02\n",
      "  5.15943618e-02 -1.69382054e-02  4.07353713e-02 -1.77428552e-02\n",
      " -3.45431405e-02  2.85551708e-02], error = [-0.00077032  0.09352355  0.01449155 -0.0686445  -0.05159436  0.01693821\n",
      " -0.04073537  0.01774286  0.03454314 -0.02855517]\n",
      "learn time: 13254.96465587616\n",
      "176000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 6.06892234e-04  1.03290107e+00 -6.95724570e-03  7.10828807e-02\n",
      " -1.06231565e-02 -1.74578677e-03  3.21013190e-02  2.05825468e-02\n",
      " -9.07327597e-02  1.27911054e-01], error = [-0.00060689 -0.03290107  0.00695725 -0.07108288  0.01062316  0.00174579\n",
      " -0.03210132 -0.02058255  0.09073276 -0.12791105]\n",
      "learn time: 13330.690550088882\n",
      "177000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.93665573 -0.12403284 -0.02631833  0.05384885 -0.09628766  0.00344928\n",
      " -0.04850758  0.0100759   0.03662049 -0.02598194], error = [ 0.06334427  0.12403284  0.02631833 -0.05384885  0.09628766 -0.00344928\n",
      "  0.04850758 -0.0100759  -0.03662049  0.02598194]\n",
      "learn time: 13406.591221094131\n",
      "178000: expected = [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.01805943 -0.01844873  0.73024478 -0.02123399  0.04423733 -0.09242288\n",
      " -0.044093    0.07800418  0.53938256 -0.08777389], error = [-0.01805943  0.01844873  0.26975522  0.02123399 -0.04423733  0.09242288\n",
      "  0.044093   -0.07800418 -0.53938256  0.08777389]\n",
      "learn time: 13482.496910572052\n",
      "179000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.01667508  0.09426602  0.00255268  0.0233631  -0.01604411 -0.01155041\n",
      "  0.97844291  0.03079854 -0.05405267 -0.04983437], error = [-0.01667508 -0.09426602 -0.00255268 -0.0233631   0.01604411  0.01155041\n",
      "  0.02155709 -0.03079854  0.05405267  0.04983437]\n",
      "learn time: 13558.33052945137\n",
      "180000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.01817487 -0.08205422  0.24977976  0.19235177 -0.12411501  0.90507749\n",
      " -0.03525362 -0.01687927 -0.13986827  0.10117467], error = [-0.01817487  0.08205422 -0.24977976 -0.19235177  0.12411501  0.09492251\n",
      "  0.03525362  0.01687927  0.13986827 -0.10117467]\n",
      "learn time: 13634.152954816818\n",
      "181000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.86429841  0.03946574 -0.03117835 -0.03392864  0.05098312  0.0904553\n",
      "  0.04175687  0.01538205 -0.01117421 -0.02503164], error = [ 0.13570159 -0.03946574  0.03117835  0.03392864 -0.05098312 -0.0904553\n",
      " -0.04175687 -0.01538205  0.01117421  0.02503164]\n",
      "learn time: 13709.911125183105\n",
      "182000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.11892068  0.03238566  0.1073086  -0.04698907  0.04999994  0.47727861\n",
      "  0.27622117  0.07508426 -0.04255094  0.00096052], error = [-0.11892068 -0.03238566 -0.1073086   0.04698907 -0.04999994  0.52272139\n",
      " -0.27622117 -0.07508426  0.04255094 -0.00096052]\n",
      "learn time: 13785.699189662933\n",
      "183000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.50352661  0.01831299  0.13733786 -0.09509367  0.15317627  0.00509195\n",
      " -0.0060428  -0.08234029 -0.04647814  0.36789378], error = [-0.50352661 -0.01831299 -0.13733786  0.09509367 -0.15317627 -0.00509195\n",
      "  0.0060428   0.08234029  0.04647814  0.63210622]\n",
      "learn time: 13861.695388317108\n",
      "184000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.00591909 -0.04752124  0.01345283  0.19450809  0.05183942 -0.04436788\n",
      "  0.02814987  0.97945625 -0.00702339 -0.22714964], error = [-0.00591909  0.04752124 -0.01345283 -0.19450809 -0.05183942  0.04436788\n",
      " -0.02814987  0.02054375  0.00702339  0.22714964]\n",
      "learn time: 13937.483845233917\n",
      "185000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.00756498 -0.11041311  0.04705554  0.13033667  0.01535505  0.26654929\n",
      "  0.08900842  0.37721853 -0.22657725  0.31631012], error = [-0.00756498  0.11041311 -0.04705554 -0.13033667 -0.01535505 -0.26654929\n",
      " -0.08900842  0.62278147  0.22657725 -0.31631012]\n",
      "learn time: 14013.125751018524\n",
      "186000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 1.87155419e-02  1.51469814e-02 -3.57427727e-03  7.28532788e-02\n",
      " -3.13175944e-02 -4.34566843e-03  1.06650127e+00 -4.31864485e-04\n",
      " -6.23310027e-02 -2.82747285e-02], error = [-0.01871554 -0.01514698  0.00357428 -0.07285328  0.03131759  0.00434567\n",
      " -0.06650127  0.00043186  0.062331    0.02827473]\n",
      "learn time: 14088.967474222183\n",
      "187000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.00111877 -0.0109812  -0.03979317  0.2586404   0.19566135  0.14914048\n",
      "  0.02287337 -0.02058679  0.42935046 -0.08737674], error = [-0.00111877  0.0109812   0.03979317 -0.2586404  -0.19566135 -0.14914048\n",
      " -0.02287337  0.02058679  0.57064954  0.08737674]\n",
      "learn time: 14164.760429620743\n",
      "188000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 9.33452519e-01 -3.26735093e-03  1.55124329e-02 -4.90417204e-02\n",
      "  7.48417595e-02 -9.07862304e-02 -3.80870554e-02 -1.74771628e-03\n",
      "  4.31614360e-02  6.88318504e-04], error = [ 0.06654748  0.00326735 -0.01551243  0.04904172 -0.07484176  0.09078623\n",
      "  0.03808706  0.00174772 -0.04316144 -0.00068832]\n",
      "learn time: 14240.42212176323\n",
      "189000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.02186879  0.00698843  0.00954934 -0.08334446  0.13088266 -0.0372266\n",
      "  0.88389495 -0.07664264  0.0684453   0.07090414], error = [-0.02186879 -0.00698843 -0.00954934  0.08334446 -0.13088266  0.0372266\n",
      "  0.11610505  0.07664264 -0.0684453  -0.07090414]\n",
      "learn time: 14315.942855596542\n",
      "190000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00280045 -0.02678299 -0.08243346  1.1543353  -0.01268443  0.08118993\n",
      "  0.05978144  0.05042455 -0.09469769 -0.05134367], error = [-0.00280045  0.02678299  0.08243346 -0.1543353   0.01268443 -0.08118993\n",
      " -0.05978144 -0.05042455  0.09469769  0.05134367]\n",
      "learn time: 14391.615208864212\n",
      "191000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.00873319  0.07339469 -0.0333313   0.09263006 -0.01647513 -0.08877736\n",
      "  1.11639673 -0.02389611 -0.02923891 -0.05985594], error = [-0.00873319 -0.07339469  0.0333313  -0.09263006  0.01647513  0.08877736\n",
      " -0.11639673  0.02389611  0.02923891  0.05985594]\n",
      "learn time: 14467.377341032028\n",
      "192000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.0577318  -0.10762629  0.24531174  0.01648485  0.01546368  0.09634893\n",
      "  0.0370941   0.75327586  0.09626873 -0.23674659], error = [-0.0577318   0.10762629 -0.24531174 -0.01648485 -0.01546368 -0.09634893\n",
      " -0.0370941   0.24672414 -0.09626873  0.23674659]\n",
      "learn time: 14543.551830530167\n",
      "193000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.01172821 -0.01227628  0.03659778 -0.08888478  0.04879678  0.24608868\n",
      " -0.02885485  0.9299704  -0.06593996  0.12354697], error = [-0.01172821  0.01227628 -0.03659778  0.08888478 -0.04879678 -0.24608868\n",
      "  0.02885485  0.0700296   0.06593996 -0.12354697]\n",
      "learn time: 14619.35578417778\n",
      "194000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 3.15373544e-04  1.13996573e+00 -4.40550425e-02 -1.51539906e-01\n",
      " -2.04879471e-02  2.71447850e-02  5.48798276e-02 -5.87906992e-02\n",
      " -1.70862423e-02  1.38337151e-01], error = [-0.00031537 -0.13996573  0.04405504  0.15153991  0.02048795 -0.02714478\n",
      " -0.05487983  0.0587907   0.01708624 -0.13833715]\n",
      "learn time: 14695.116457939148\n",
      "195000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.03105302 -0.00717357 -0.11506363 -0.04795435  0.05309877  0.74528128\n",
      "  0.21555952  0.08703286  0.03354775 -0.03561593], error = [-0.03105302  0.00717357  0.11506363  0.04795435 -0.05309877  0.25471872\n",
      " -0.21555952 -0.08703286 -0.03354775  0.03561593]\n",
      "learn time: 14771.004580497742\n",
      "196000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.00917802  0.00279803 -0.06036428  0.00923111 -0.01718174 -0.09438939\n",
      " -0.05420988  0.04575458  1.22041419 -0.11474767], error = [-0.00917802 -0.00279803  0.06036428 -0.00923111  0.01718174  0.09438939\n",
      "  0.05420988 -0.04575458 -0.22041419  0.11474767]\n",
      "learn time: 14846.719161272049\n",
      "197000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.06329502 -0.0461774   0.13852175  0.65621916  0.15678    -0.01041437\n",
      "  0.06119918  0.05627335 -0.08012591  0.04102567], error = [-0.06329502  0.0461774  -0.13852175  0.34378084 -0.15678     0.01041437\n",
      " -0.06119918 -0.05627335  0.08012591 -0.04102567]\n",
      "learn time: 14922.230804920197\n",
      "198000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.00684588  0.02101955  0.16851042 -0.0306513   0.72218    -0.10608924\n",
      "  0.05014494 -0.02545415 -0.04973613  0.32975173], error = [-0.00684588 -0.02101955 -0.16851042  0.0306513   0.27782     0.10608924\n",
      " -0.05014494  0.02545415  0.04973613 -0.32975173]\n",
      "learn time: 14998.536801099777\n",
      "199000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.00504136  0.05034908  0.11465409  0.04267608  0.01808975 -0.04435721\n",
      " -0.11842858  0.15627767  0.84992794 -0.14007014], error = [-0.00504136 -0.05034908 -0.11465409 -0.04267608 -0.01808975  0.04435721\n",
      "  0.11842858 -0.15627767  0.15007206  0.14007014]\n",
      "learn time: 15074.554792881012\n",
      "200000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.07823556  0.04398025 -0.10461953 -0.20141294  0.06858453  1.13251475\n",
      " -0.00325966  0.05737841 -0.03665956 -0.02556182], error = [-0.07823556 -0.04398025  0.10461953  0.20141294 -0.06858453 -0.13251475\n",
      "  0.00325966 -0.05737841  0.03665956  0.02556182]\n",
      "learn time: 15152.132363319397\n",
      "201000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 8.70762389e-03 -3.83453862e-02  3.35093643e-03  1.11157299e-02\n",
      "  4.23015805e-03 -6.23880163e-02  1.65646938e-04  6.20064866e-01\n",
      " -2.67448965e-02  3.60570917e-01], error = [-8.70762389e-03  3.83453862e-02 -3.35093643e-03 -1.11157299e-02\n",
      " -4.23015805e-03  6.23880163e-02 -1.65646938e-04  3.79935134e-01\n",
      "  2.67448965e-02 -3.60570917e-01]\n",
      "learn time: 15231.866907596588\n",
      "202000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00155125  0.57103195 -0.11962301  0.8854242  -0.01857324 -0.19726084\n",
      "  0.0613436  -0.01690079 -0.08865911 -0.10624756], error = [-0.00155125 -0.57103195  0.11962301  0.1145758   0.01857324  0.19726084\n",
      " -0.0613436   0.01690079  0.08865911  0.10624756]\n",
      "learn time: 15307.245122909546\n",
      "203000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.02057615  0.0292978  -0.08922154  0.12491496 -0.00238129 -0.07742946\n",
      " -0.05938646  0.95631987 -0.09787498 -0.00704226], error = [-0.02057615 -0.0292978   0.08922154 -0.12491496  0.00238129  0.07742946\n",
      "  0.05938646  0.04368013  0.09787498  0.00704226]\n",
      "learn time: 15383.011357069016\n",
      "204000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.00951646 -0.01290332  0.09647588  0.11110568 -0.01225351  0.0394309\n",
      "  0.00865348 -0.02430695  0.81153779 -0.05765628], error = [-0.00951646  0.01290332 -0.09647588 -0.11110568  0.01225351 -0.0394309\n",
      " -0.00865348  0.02430695  0.18846221  0.05765628]\n",
      "learn time: 15458.77010345459\n",
      "205000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00107829  0.18098332  0.02469121  0.22409853 -0.0062066  -0.00905786\n",
      " -0.10258015  0.4248127   0.0698966   0.22688316], error = [-0.00107829 -0.18098332 -0.02469121  0.77590147  0.0062066   0.00905786\n",
      "  0.10258015 -0.4248127  -0.0698966  -0.22688316]\n",
      "learn time: 15534.872317552567\n",
      "206000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.0013525  -0.01085792  0.00401197  0.00303685  0.89398765  0.01210739\n",
      "  0.012196    0.09615335 -0.154665    0.09223879], error = [-0.0013525   0.01085792 -0.00401197 -0.00303685  0.10601235 -0.01210739\n",
      " -0.012196   -0.09615335  0.154665   -0.09223879]\n",
      "learn time: 15610.9224588871\n",
      "207000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.06437348 -0.05230185 -0.15222879 -0.02937447  0.2295699   0.45405373\n",
      "  0.19024127  0.05503502  0.12726976 -0.03276971], error = [-0.06437348  0.05230185  0.15222879  0.02937447 -0.2295699   0.54594627\n",
      " -0.19024127 -0.05503502 -0.12726976  0.03276971]\n",
      "learn time: 15686.551511526108\n",
      "208000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.00096997  0.92517285 -0.00327921 -0.05133573 -0.02433244  0.14054046\n",
      "  0.00172882  0.03520372  0.07702649 -0.0162778 ], error = [-0.00096997  0.07482715  0.00327921  0.05133573  0.02433244 -0.14054046\n",
      " -0.00172882 -0.03520372 -0.07702649  0.0162778 ]\n",
      "learn time: 15762.524102687836\n",
      "209000: expected = [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.00194428  0.05343358  0.77204307  0.19525889  0.04375481  0.02055158\n",
      " -0.10652216  0.01929942  0.13013072 -0.03098257], error = [-0.00194428 -0.05343358  0.22795693 -0.19525889 -0.04375481 -0.02055158\n",
      "  0.10652216 -0.01929942 -0.13013072  0.03098257]\n",
      "learn time: 15838.854811668396\n",
      "210000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00344603  0.01629745 -0.05874279  1.15641016 -0.05525091 -0.07684917\n",
      "  0.04155929 -0.03636523  0.02234794  0.14906522], error = [-0.00344603 -0.01629745  0.05874279 -0.15641016  0.05525091  0.07684917\n",
      " -0.04155929  0.03636523 -0.02234794 -0.14906522]\n",
      "learn time: 15914.740119218826\n",
      "211000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 8.96613085e-02  4.49912634e-04  1.29188552e-02 -2.00687872e-01\n",
      "  5.50674519e-02  6.35666201e-01  5.18033532e-01  3.66921968e-02\n",
      " -2.71164903e-02 -2.51650402e-02], error = [-8.96613085e-02 -4.49912634e-04 -1.29188552e-02  2.00687872e-01\n",
      " -5.50674519e-02 -6.35666201e-01  4.81966468e-01 -3.66921968e-02\n",
      "  2.71164903e-02  2.51650402e-02]\n",
      "learn time: 15990.800605773926\n",
      "212000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.00383817  0.0152571   0.03145342  0.05107298  0.08171793 -0.06551452\n",
      "  0.04591881  0.00140658  0.97626358 -0.11375034], error = [-0.00383817 -0.0152571  -0.03145342 -0.05107298 -0.08171793  0.06551452\n",
      " -0.04591881 -0.00140658  0.02373642  0.11375034]\n",
      "learn time: 16066.567091226578\n",
      "213000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00607287 -0.05185383  0.00472925  0.36085822  0.07121423  0.62888516\n",
      "  0.01621175  0.05326526  0.03549419 -0.02295856], error = [-0.00607287  0.05185383 -0.00472925  0.63914178 -0.07121423 -0.62888516\n",
      " -0.01621175 -0.05326526 -0.03549419  0.02295856]\n",
      "learn time: 16142.597128152847\n",
      "214000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.00454741  0.00540472 -0.01683508 -0.00441034  0.66510225 -0.03978057\n",
      "  0.04143924  0.02303098 -0.05791001  0.46778778], error = [-0.00454741 -0.00540472  0.01683508  0.00441034  0.33489775  0.03978057\n",
      " -0.04143924 -0.02303098  0.05791001 -0.46778778]\n",
      "learn time: 16218.31854391098\n",
      "215000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 5.38959759e-04  1.04963059e+00 -3.43091562e-02 -2.44248514e-02\n",
      "  1.51802628e-02  9.42453048e-02 -1.29253076e-02 -4.68515040e-02\n",
      "  4.59889957e-03 -1.07094190e-01], error = [-0.00053896 -0.04963059  0.03430916  0.02442485 -0.01518026 -0.0942453\n",
      "  0.01292531  0.0468515  -0.0045989   0.10709419]\n",
      "learn time: 16294.331330060959\n",
      "216000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.00351753  0.0377453   0.01251554 -0.0197344   0.13291575 -0.02259029\n",
      "  0.00592217 -0.08313492  0.16804011  0.69015413], error = [-0.00351753 -0.0377453  -0.01251554  0.0197344  -0.13291575  0.02259029\n",
      " -0.00592217  0.08313492 -0.16804011  0.30984587]\n",
      "learn time: 16370.177666664124\n",
      "217000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.00236022 -0.05539029 -0.03802214  0.02867768  0.89781777  0.05349121\n",
      "  0.15285073  0.03661986  0.01776017 -0.14811784], error = [-0.00236022  0.05539029  0.03802214 -0.02867768  0.10218223 -0.05349121\n",
      " -0.15285073 -0.03661986 -0.01776017  0.14811784]\n",
      "learn time: 16446.117825746536\n",
      "218000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.00799597 -0.03523852  0.00460479  0.04526639  0.0206473   0.0501492\n",
      "  0.04443143  0.02876774  0.68240427  0.1542696 ], error = [-0.00799597  0.03523852 -0.00460479 -0.04526639 -0.0206473  -0.0501492\n",
      " -0.04443143 -0.02876774  0.31759573 -0.1542696 ]\n",
      "learn time: 16522.194390773773\n",
      "219000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.00118164 -0.03787339 -0.08119183  0.11197713  0.53925908  0.0564456\n",
      "  0.00597633 -0.02409843  0.10952386  0.2878965 ], error = [-0.00118164  0.03787339  0.08119183 -0.11197713 -0.53925908 -0.0564456\n",
      " -0.00597633  0.02409843 -0.10952386  0.7121035 ]\n",
      "learn time: 16598.213667154312\n",
      "220000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.004541    0.04895819 -0.08326547 -0.0870531  -0.01750924  0.22517811\n",
      "  0.00882826  1.0439394  -0.09397979  0.07511229], error = [-0.004541   -0.04895819  0.08326547  0.0870531   0.01750924 -0.22517811\n",
      " -0.00882826 -0.0439394   0.09397979 -0.07511229]\n",
      "learn time: 16674.00631761551\n",
      "221000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.01136602 -0.00994561  0.07173512 -0.09811196  0.27064186  0.61138897\n",
      " -0.10758264  0.06039688 -0.15869144  0.33488465], error = [-0.01136602  0.00994561 -0.07173512  0.09811196 -0.27064186  0.38861103\n",
      "  0.10758264 -0.06039688  0.15869144 -0.33488465]\n",
      "learn time: 16750.205391407013\n",
      "222000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 7.39821907e-04  1.08898943e+00 -1.08528931e-01  4.12383958e-02\n",
      " -3.80024875e-02 -6.26243061e-02  4.01548013e-02  7.26109289e-02\n",
      "  6.10879006e-02 -7.18977511e-02], error = [-0.00073982 -0.08898943  0.10852893 -0.0412384   0.03800249  0.06262431\n",
      " -0.0401548  -0.07261093 -0.0610879   0.07189775]\n",
      "learn time: 16825.83180642128\n",
      "223000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.00308515 -0.00862057 -0.0021651   0.16613131  0.12314661  0.05611534\n",
      " -0.019782    0.14148999 -0.1484233   0.79810184], error = [-0.00308515  0.00862057  0.0021651  -0.16613131 -0.12314661 -0.05611534\n",
      "  0.019782   -0.14148999  0.1484233   0.20189816]\n",
      "learn time: 16901.631013154984\n",
      "224000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.92016529  0.06100935 -0.00691556  0.02598968 -0.04862385  0.03580378\n",
      " -0.12352878  0.04963634  0.01612407  0.07071002], error = [ 0.07983471 -0.06100935  0.00691556 -0.02598968  0.04862385 -0.03580378\n",
      "  0.12352878 -0.04963634 -0.01612407 -0.07071002]\n",
      "learn time: 16978.098009109497\n",
      "225000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00417855 -0.00106438  0.05911978  0.81058007  0.01186713  0.15961276\n",
      " -0.05532076 -0.07710476  0.0618518   0.11203563], error = [-0.00417855  0.00106438 -0.05911978  0.18941993 -0.01186713 -0.15961276\n",
      "  0.05532076  0.07710476 -0.0618518  -0.11203563]\n",
      "learn time: 17053.77376961708\n",
      "226000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.00329356 -0.03372211  0.11002463 -0.01032257 -0.02332052  0.03321751\n",
      " -0.06516398 -0.02683297 -0.15691098  1.12530726], error = [-0.00329356  0.03372211 -0.11002463  0.01032257  0.02332052 -0.03321751\n",
      "  0.06516398  0.02683297  0.15691098 -0.12530726]\n",
      "learn time: 17129.527817964554\n",
      "227000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.90224473  0.00326413  0.07130867 -0.04006764 -0.00534944  0.01489788\n",
      "  0.03585584  0.01847322  0.01713133  0.05662725], error = [ 0.09775527 -0.00326413 -0.07130867  0.04006764  0.00534944 -0.01489788\n",
      " -0.03585584 -0.01847322 -0.01713133 -0.05662725]\n",
      "learn time: 17205.413488149643\n",
      "228000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.0020047  -0.08620212  0.06973628 -0.08335025  0.9817805   0.11417544\n",
      "  0.0085268   0.05890134  0.08777833 -0.12864949], error = [-0.0020047   0.08620212 -0.06973628  0.08335025  0.0182195  -0.11417544\n",
      " -0.0085268  -0.05890134 -0.08777833  0.12864949]\n",
      "learn time: 17281.370080709457\n",
      "229000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.00318349  0.02316134 -0.01168132 -0.03764094  0.93790025  0.00852337\n",
      "  0.02696248 -0.04017467 -0.0099338   0.10974818], error = [-0.00318349 -0.02316134  0.01168132  0.03764094  0.06209975 -0.00852337\n",
      " -0.02696248  0.04017467  0.0099338  -0.10974818]\n",
      "learn time: 17357.337258577347\n",
      "230000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00185985  0.07984042  0.16839561  0.51118487 -0.00743628  0.12777213\n",
      " -0.03513099 -0.04193136  0.09279582 -0.0091858 ], error = [-0.00185985 -0.07984042 -0.16839561  0.48881513  0.00743628 -0.12777213\n",
      "  0.03513099  0.04193136 -0.09279582  0.0091858 ]\n",
      "learn time: 17433.25923871994\n",
      "231000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.01205267  0.00293402  0.09421766 -0.11418863  0.01973537 -0.00670375\n",
      " -0.03083344  1.15172098  0.02805452 -0.02024222], error = [-0.01205267 -0.00293402 -0.09421766  0.11418863 -0.01973537  0.00670375\n",
      "  0.03083344 -0.15172098 -0.02805452  0.02024222]\n",
      "learn time: 17509.266109228134\n",
      "232000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 6.10570937e-03 -2.68788784e-02 -7.45424847e-04  1.10693065e-01\n",
      " -7.30576044e-02  5.65397379e-02  1.03456352e+00 -2.74059941e-02\n",
      "  4.89820356e-03  1.40765936e-02], error = [-0.00610571  0.02687888  0.00074542 -0.11069307  0.0730576  -0.05653974\n",
      " -0.03456352  0.02740599 -0.0048982  -0.01407659]\n",
      "learn time: 17585.538888692856\n",
      "233000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.03722081  0.01904109  0.23630574 -0.02355017 -0.16662966  1.01429918\n",
      "  0.04396721 -0.0314858   0.06623896 -0.05340599], error = [-0.03722081 -0.01904109 -0.23630574  0.02355017  0.16662966 -0.01429918\n",
      " -0.04396721  0.0314858  -0.06623896  0.05340599]\n",
      "learn time: 17661.74995160103\n",
      "234000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.00707928  0.03344607 -0.14197414  0.02226634  0.01848394  0.62623489\n",
      " -0.00986498 -0.01064923  0.35162357  0.03356301], error = [-0.00707928 -0.03344607  0.14197414 -0.02226634 -0.01848394  0.37376511\n",
      "  0.00986498  0.01064923 -0.35162357 -0.03356301]\n",
      "learn time: 17737.451675653458\n",
      "235000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 3.92060572e-04  9.18640755e-01 -1.19432216e-02  6.88833351e-02\n",
      "  5.50517945e-02 -1.94797376e-02  2.63338387e-02 -2.83128708e-02\n",
      " -4.20419733e-02  1.87181783e-02], error = [-0.00039206  0.08135925  0.01194322 -0.06888334 -0.05505179  0.01947974\n",
      " -0.02633384  0.02831287  0.04204197 -0.01871818]\n",
      "learn time: 17813.678387641907\n",
      "236000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 3.35783370e-04  1.02800945e+00 -1.96054925e-02  6.64204163e-02\n",
      " -1.76402391e-02 -7.22183745e-03  3.55046964e-02  1.65347475e-02\n",
      " -7.54551183e-02  1.29527844e-01], error = [-0.00033578 -0.02800945  0.01960549 -0.06642042  0.01764024  0.00722184\n",
      " -0.0355047  -0.01653475  0.07545512 -0.12952784]\n",
      "learn time: 17889.76355433464\n",
      "237000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.95218384 -0.08283972 -0.02683485  0.05671029 -0.10999957  0.00968486\n",
      " -0.07572379  0.00726923  0.00885003 -0.01344995], error = [ 0.04781616  0.08283972  0.02683485 -0.05671029  0.10999957 -0.00968486\n",
      "  0.07572379 -0.00726923 -0.00885003  0.01344995]\n",
      "learn time: 17965.99883580208\n",
      "238000: expected = [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.01458952 -0.02129915  0.80702406 -0.09017242  0.07197468 -0.06748255\n",
      " -0.03518385  0.06842571  0.4866801  -0.09953803], error = [-0.01458952  0.02129915  0.19297594  0.09017242 -0.07197468  0.06748255\n",
      "  0.03518385 -0.06842571 -0.4866801   0.09953803]\n",
      "learn time: 18041.977636814117\n",
      "239000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.01259418  0.07393208  0.02254273  0.04715326 -0.00808353 -0.03471398\n",
      "  0.95385315  0.03501645 -0.02494678 -0.05698067], error = [-0.01259418 -0.07393208 -0.02254273 -0.04715326  0.00808353  0.03471398\n",
      "  0.04614685 -0.03501645  0.02494678  0.05698067]\n",
      "learn time: 18118.023249149323\n",
      "240000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.01482515 -0.03206361  0.20738406  0.16676188 -0.12161247  0.93385276\n",
      " -0.01889954 -0.04617752 -0.11274674  0.0902709 ], error = [-0.01482515  0.03206361 -0.20738406 -0.16676188  0.12161247  0.06614724\n",
      "  0.01889954  0.04617752  0.11274674 -0.0902709 ]\n",
      "learn time: 18193.98807811737\n",
      "241000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.89751594  0.02924317 -0.04447761 -0.02160053  0.03647999  0.0887041\n",
      "  0.05285991  0.01561548 -0.02554992 -0.01254599], error = [ 0.10248406 -0.02924317  0.04447761  0.02160053 -0.03647999 -0.0887041\n",
      " -0.05285991 -0.01561548  0.02554992  0.01254599]\n",
      "learn time: 18269.883268117905\n",
      "242000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.09409572  0.03373852  0.11741644 -0.06029199  0.09511868  0.46357226\n",
      "  0.24941268  0.06800207 -0.03770076  0.00055061], error = [-0.09409572 -0.03373852 -0.11741644  0.06029199 -0.09511868  0.53642774\n",
      " -0.24941268 -0.06800207  0.03770076 -0.00055061]\n",
      "learn time: 18345.808137893677\n",
      "243000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.52935109  0.01554     0.092959   -0.07415768  0.15470091 -0.00557659\n",
      " -0.02523169 -0.05468619 -0.08747771  0.39020294], error = [-0.52935109 -0.01554    -0.092959    0.07415768 -0.15470091  0.00557659\n",
      "  0.02523169  0.05468619  0.08747771  0.60979706]\n",
      "learn time: 18421.78298306465\n",
      "244000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.00573359 -0.05282358  0.04664157  0.15067711  0.0328525  -0.03278701\n",
      "  0.03691599  1.08021663 -0.02034618 -0.25943774], error = [-0.00573359  0.05282358 -0.04664157 -0.15067711 -0.0328525   0.03278701\n",
      " -0.03691599 -0.08021663  0.02034618  0.25943774]\n",
      "learn time: 18497.573397636414\n",
      "245000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.00556029 -0.11662586  0.09786358  0.08858414  0.00969707  0.2596278\n",
      "  0.07102908  0.3888125  -0.22635238  0.34236609], error = [-0.00556029  0.11662586 -0.09786358 -0.08858414 -0.00969707 -0.2596278\n",
      " -0.07102908  0.6111875   0.22635238 -0.34236609]\n",
      "learn time: 18573.423337459564\n",
      "246000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.01374895  0.01103577 -0.00833898  0.06939934 -0.02014578 -0.01329834\n",
      "  1.08222022 -0.01079861 -0.04352009 -0.02674277], error = [-0.01374895 -0.01103577  0.00833898 -0.06939934  0.02014578  0.01329834\n",
      " -0.08222022  0.01079861  0.04352009  0.02674277]\n",
      "learn time: 18649.253748893738\n",
      "247000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.00068875 -0.00819524 -0.05097986  0.25906004  0.18987673  0.12057947\n",
      "  0.01405756 -0.01612384  0.47198501 -0.07313976], error = [-0.00068875  0.00819524  0.05097986 -0.25906004 -0.18987673 -0.12057947\n",
      " -0.01405756  0.01612384  0.52801499  0.07313976]\n",
      "learn time: 18725.058728933334\n",
      "248000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 9.46867417e-01 -1.01839496e-02  2.00258564e-02 -3.41720479e-02\n",
      "  6.17621773e-02 -9.22482120e-02 -7.92717360e-03  7.13561291e-03\n",
      "  4.98835277e-02 -2.31898005e-04], error = [ 0.05313258  0.01018395 -0.02002586  0.03417205 -0.06176218  0.09224821\n",
      "  0.00792717 -0.00713561 -0.04988353  0.0002319 ]\n",
      "learn time: 18800.698615550995\n",
      "249000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.01381067 -0.00955306  0.01239781 -0.09982481  0.10684999 -0.02628941\n",
      "  0.92788769 -0.04673433  0.07442363  0.05960163], error = [-0.01381067  0.00955306 -0.01239781  0.09982481 -0.10684999  0.02628941\n",
      "  0.07211231  0.04673433 -0.07442363 -0.05960163]\n",
      "learn time: 18876.370168209076\n",
      "250000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00257854 -0.01348685 -0.06678766  1.12321193 -0.02417188  0.07414663\n",
      "  0.077302    0.03878477 -0.07171417 -0.05021674], error = [-0.00257854  0.01348685  0.06678766 -0.12321193  0.02417188 -0.07414663\n",
      " -0.077302   -0.03878477  0.07171417  0.05021674]\n",
      "learn time: 18952.16557073593\n",
      "251000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.00628528  0.06647929 -0.03451893  0.10210266 -0.01784959 -0.09203862\n",
      "  1.0762267  -0.00961693 -0.02420891 -0.06493621], error = [-0.00628528 -0.06647929  0.03451893 -0.10210266  0.01784959  0.09203862\n",
      " -0.0762267   0.00961693  0.02420891  0.06493621]\n",
      "learn time: 19027.92044711113\n",
      "252000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.03520827 -0.10439719  0.21831491  0.00727101  0.03422757  0.12169226\n",
      "  0.04292351  0.77855093  0.04193429 -0.25359971], error = [-0.03520827  0.10439719 -0.21831491 -0.00727101 -0.03422757 -0.12169226\n",
      " -0.04292351  0.22144907 -0.04193429  0.25359971]\n",
      "learn time: 19103.664338350296\n",
      "253000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 8.92778933e-03 -9.18944640e-04  3.67292455e-02 -8.70400026e-02\n",
      "  2.88830675e-02  2.16875816e-01 -1.62640830e-02  9.40882975e-01\n",
      " -5.20169095e-02  1.23959561e-01], error = [-0.00892779  0.00091894 -0.03672925  0.08704    -0.02888307 -0.21687582\n",
      "  0.01626408  0.05911703  0.05201691 -0.12395956]\n",
      "learn time: 19179.272555828094\n",
      "254000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 2.07684744e-04  1.13486758e+00 -2.71785478e-02 -1.48364549e-01\n",
      " -3.26833146e-02  2.44537682e-02  4.71825978e-02 -6.72712577e-02\n",
      " -3.73417335e-04  1.51926457e-01], error = [-0.00020768 -0.13486758  0.02717855  0.14836455  0.03268331 -0.02445377\n",
      " -0.0471826   0.06727126  0.00037342 -0.15192646]\n",
      "learn time: 19254.94772028923\n",
      "255000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.02500571 -0.01093208 -0.11190051 -0.05943093  0.05827122  0.79122988\n",
      "  0.20197942  0.10122593  0.0358277  -0.06152861], error = [-0.02500571  0.01093208  0.11190051  0.05943093 -0.05827122  0.20877012\n",
      " -0.20197942 -0.10122593 -0.0358277   0.06152861]\n",
      "learn time: 19330.573956012726\n",
      "256000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.00704854 -0.00134827 -0.07322693  0.01092251 -0.01793963 -0.10253817\n",
      " -0.03814532  0.04627815  1.2268562  -0.09430115], error = [-0.00704854  0.00134827  0.07322693 -0.01092251  0.01793963  0.10253817\n",
      "  0.03814532 -0.04627815 -0.2268562   0.09430115]\n",
      "learn time: 19406.564430475235\n",
      "257000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.06450692 -0.0377262   0.12392465  0.73057015  0.12769485 -0.05209864\n",
      "  0.05736069  0.04471901 -0.07549202  0.05427807], error = [-0.06450692  0.0377262  -0.12392465  0.26942985 -0.12769485  0.05209864\n",
      " -0.05736069 -0.04471901  0.07549202 -0.05427807]\n",
      "learn time: 19482.469005584717\n",
      "258000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.00469001  0.00620241  0.16550431 -0.03970564  0.76448205 -0.11718935\n",
      "  0.03534543 -0.02119878 -0.06210144  0.31380031], error = [-0.00469001 -0.00620241 -0.16550431  0.03970564  0.23551795  0.11718935\n",
      " -0.03534543  0.02119878  0.06210144 -0.31380031]\n",
      "learn time: 19559.736812591553\n",
      "259000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.00351563  0.04882943  0.09643041  0.03213303  0.01646566 -0.01805367\n",
      " -0.09646213  0.16222581  0.87839132 -0.12334988], error = [-0.00351563 -0.04882943 -0.09643041 -0.03213303 -0.01646566  0.01805367\n",
      "  0.09646213 -0.16222581  0.12160868  0.12334988]\n",
      "learn time: 19638.99062895775\n",
      "260000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.04826549  0.04173483 -0.12537243 -0.18874577  0.06900308  1.17433876\n",
      " -0.00305442  0.06479273 -0.05245295 -0.01854326], error = [-0.04826549 -0.04173483  0.12537243  0.18874577 -0.06900308 -0.17433876\n",
      "  0.00305442 -0.06479273  0.05245295  0.01854326]\n",
      "learn time: 19716.428647756577\n",
      "261000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.0070636  -0.05268244  0.00544962  0.01516779  0.02766905 -0.05662155\n",
      "  0.01652471  0.65490057 -0.02680544  0.32341102], error = [-0.0070636   0.05268244 -0.00544962 -0.01516779 -0.02766905  0.05662155\n",
      " -0.01652471  0.34509943  0.02680544 -0.32341102]\n",
      "learn time: 19792.54909133911\n",
      "262000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00135167  0.54087023 -0.1088525   0.87898929 -0.01644539 -0.2015458\n",
      "  0.04153181 -0.02473421 -0.10058833 -0.07792714], error = [-0.00135167 -0.54087023  0.1088525   0.12101071  0.01644539  0.2015458\n",
      " -0.04153181  0.02473421  0.10058833  0.07792714]\n",
      "learn time: 19868.28690814972\n",
      "263000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.01663207  0.04355344 -0.0855458   0.1201409   0.00967359 -0.08102795\n",
      " -0.04950715  0.9560933  -0.09229299 -0.03564388], error = [-0.01663207 -0.04355344  0.0855458  -0.1201409  -0.00967359  0.08102795\n",
      "  0.04950715  0.0439067   0.09229299  0.03564388]\n",
      "learn time: 19944.177322149277\n",
      "264000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 8.01111134e-03  4.16564369e-04  1.04042554e-01  6.61125063e-02\n",
      " -4.49047717e-02  8.30342116e-02  6.34587860e-03 -4.92781989e-02\n",
      "  8.44392460e-01 -3.85700012e-02], error = [-0.00801111 -0.00041656 -0.10404255 -0.06611251  0.04490477 -0.08303421\n",
      " -0.00634588  0.0492782   0.15560754  0.03857   ]\n",
      "learn time: 20019.859890937805\n",
      "265000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00084759  0.15757561  0.02417297  0.23652464 -0.01063252  0.00112036\n",
      " -0.089475    0.43495365  0.07789054  0.21680396], error = [-0.00084759 -0.15757561 -0.02417297  0.76347536  0.01063252 -0.00112036\n",
      "  0.089475   -0.43495365 -0.07789054 -0.21680396]\n",
      "learn time: 20095.742179870605\n",
      "266000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.00099922 -0.01224689 -0.00980633  0.00402923  0.9238654   0.0297724\n",
      "  0.01989715  0.09114772 -0.1563495   0.06566242], error = [-0.00099922  0.01224689  0.00980633 -0.00402923  0.0761346  -0.0297724\n",
      " -0.01989715 -0.09114772  0.1563495  -0.06566242]\n",
      "learn time: 20171.80150294304\n",
      "267000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 4.68787079e-02 -5.50107939e-02 -1.53570811e-01 -1.49368984e-02\n",
      "  1.62556684e-01  4.86643759e-01  2.12017958e-01  5.32440951e-02\n",
      "  1.27545034e-01  4.67371760e-04], error = [-4.68787079e-02  5.50107939e-02  1.53570811e-01  1.49368984e-02\n",
      " -1.62556684e-01  5.13356241e-01 -2.12017958e-01 -5.32440951e-02\n",
      " -1.27545034e-01 -4.67371760e-04]\n",
      "learn time: 20247.603660345078\n",
      "268000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 7.12584765e-04  9.51195712e-01 -6.58783549e-03 -3.04522511e-02\n",
      " -2.08800653e-02  1.16132461e-01 -1.84087936e-02  3.21937269e-02\n",
      "  5.56424508e-02 -1.21477170e-02], error = [-0.00071258  0.04880429  0.00658784  0.03045225  0.02088007 -0.11613246\n",
      "  0.01840879 -0.03219373 -0.05564245  0.01214772]\n",
      "learn time: 20323.22043824196\n",
      "269000: expected = [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.00153131  0.04592656  0.76481429  0.19090676  0.02734264  0.01418642\n",
      " -0.12266868  0.02678225  0.12779607 -0.01212393], error = [-0.00153131 -0.04592656  0.23518571 -0.19090676 -0.02734264 -0.01418642\n",
      "  0.12266868 -0.02678225 -0.12779607  0.01212393]\n",
      "learn time: 20399.35002398491\n",
      "270000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00303855 -0.00418354 -0.05373862  1.16540454 -0.04224894 -0.06828779\n",
      "  0.04535674 -0.03965749  0.01318048  0.13550591], error = [-0.00303855  0.00418354  0.05373862 -0.16540454  0.04224894  0.06828779\n",
      " -0.04535674  0.03965749 -0.01318048 -0.13550591]\n",
      "learn time: 20475.367426395416\n",
      "271000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.07159985  0.01549796  0.01450656 -0.19307415  0.03235741  0.57765975\n",
      "  0.55775903  0.03595189 -0.02387153  0.00589054], error = [-0.07159985 -0.01549796 -0.01450656  0.19307415 -0.03235741 -0.57765975\n",
      "  0.44224097 -0.03595189  0.02387153 -0.00589054]\n",
      "learn time: 20551.517342805862\n",
      "272000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.00274713  0.00760175  0.00867323  0.07086865  0.07825894 -0.09575599\n",
      "  0.05300154  0.00479252  1.01063049 -0.10225618], error = [-0.00274713 -0.00760175 -0.00867323 -0.07086865 -0.07825894  0.09575599\n",
      " -0.05300154 -0.00479252 -0.01063049  0.10225618]\n",
      "learn time: 20627.551512241364\n",
      "273000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 5.18775496e-03 -4.78738879e-02  1.45476817e-04  3.72070561e-01\n",
      "  6.51682529e-02  6.43493825e-01  1.27941457e-03  5.24930039e-02\n",
      "  3.38243831e-02 -3.16265835e-02], error = [-5.18775496e-03  4.78738879e-02 -1.45476817e-04  6.27929439e-01\n",
      " -6.51682529e-02 -6.43493825e-01 -1.27941457e-03 -5.24930039e-02\n",
      " -3.38243831e-02  3.16265835e-02]\n",
      "learn time: 20703.481256484985\n",
      "274000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.00298025  0.00389214 -0.01662192 -0.01076447  0.71119525 -0.04225073\n",
      "  0.03943861  0.03173083 -0.03890514  0.42172418], error = [-0.00298025 -0.00389214  0.01662192  0.01076447  0.28880475  0.04225073\n",
      " -0.03943861 -0.03173083  0.03890514 -0.42172418]\n",
      "learn time: 20779.394782543182\n",
      "275000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 3.90204456e-04  1.04706472e+00 -3.45995606e-02 -2.15807587e-02\n",
      "  2.36855692e-02  9.31002928e-02  3.25181375e-04 -3.12661696e-02\n",
      "  4.61941324e-04 -1.14759617e-01], error = [-0.0003902  -0.04706472  0.03459956  0.02158076 -0.02368557 -0.09310029\n",
      " -0.00032518  0.03126617 -0.00046194  0.11475962]\n",
      "learn time: 20855.22870707512\n",
      "276000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 2.82405666e-03  4.24018445e-02  1.90208486e-02 -2.51792936e-02\n",
      "  1.21967212e-01  1.12194261e-02 -2.89685400e-04 -8.91903126e-02\n",
      "  1.73179081e-01  6.79891920e-01], error = [-2.82405666e-03 -4.24018445e-02 -1.90208486e-02  2.51792936e-02\n",
      " -1.21967212e-01 -1.12194261e-02  2.89685400e-04  8.91903126e-02\n",
      " -1.73179081e-01  3.20108080e-01]\n",
      "learn time: 20930.979981422424\n",
      "277000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.00166749 -0.04688354 -0.03043322  0.0398109   0.91338949  0.01083184\n",
      "  0.14850108  0.03789089  0.02534314 -0.13724486], error = [-0.00166749  0.04688354  0.03043322 -0.0398109   0.08661051 -0.01083184\n",
      " -0.14850108 -0.03789089 -0.02534314  0.13724486]\n",
      "learn time: 21007.019208192825\n",
      "278000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.00700109 -0.03550899 -0.00993825  0.055641    0.03189498  0.02616269\n",
      "  0.04628901  0.01544725  0.7212263   0.13888082], error = [-0.00700109  0.03550899  0.00993825 -0.055641   -0.03189498 -0.02616269\n",
      " -0.04628901 -0.01544725  0.2787737  -0.13888082]\n",
      "learn time: 21082.90171098709\n",
      "279000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.00079745 -0.01984127 -0.10030711  0.12626049  0.54421169  0.0593003\n",
      "  0.01398661 -0.01890178  0.10046997  0.30050326], error = [-0.00079745  0.01984127  0.10030711 -0.12626049 -0.54421169 -0.0593003\n",
      " -0.01398661  0.01890178 -0.10046997  0.69949674]\n",
      "learn time: 21159.18816423416\n",
      "280000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 3.40689280e-03  4.51052699e-02 -1.04274602e-01 -7.27417831e-02\n",
      " -2.00558784e-02  2.21906259e-01 -1.02325165e-04  1.05062009e+00\n",
      " -9.61080076e-02  8.11142465e-02], error = [-3.40689280e-03 -4.51052699e-02  1.04274602e-01  7.27417831e-02\n",
      "  2.00558784e-02 -2.21906259e-01  1.02325165e-04 -5.06200939e-02\n",
      "  9.61080076e-02 -8.11142465e-02]\n",
      "learn time: 21234.802180051804\n",
      "281000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.0099356   0.01020894  0.07366833 -0.09378567  0.25836839  0.65636306\n",
      " -0.10294231  0.06552709 -0.19265894  0.34718798], error = [-0.0099356  -0.01020894 -0.07366833  0.09378567 -0.25836839  0.34363694\n",
      "  0.10294231 -0.06552709  0.19265894 -0.34718798]\n",
      "learn time: 21310.82199072838\n",
      "282000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 5.14853674e-04  1.08964139e+00 -1.20542888e-01  4.47713825e-02\n",
      " -2.66896934e-02 -5.42465158e-02  3.81671872e-02  6.68417159e-02\n",
      "  7.09411642e-02 -7.94302685e-02], error = [-0.00051485 -0.08964139  0.12054289 -0.04477138  0.02668969  0.05424652\n",
      " -0.03816719 -0.06684172 -0.07094116  0.07943027]\n",
      "learn time: 21386.779400110245\n",
      "283000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.00226099  0.01242135 -0.01420138  0.14684888  0.16664302  0.02373619\n",
      " -0.02522872  0.07715251 -0.14141091  0.80858643], error = [-0.00226099 -0.01242135  0.01420138 -0.14684888 -0.16664302 -0.02373619\n",
      "  0.02522872 -0.07715251  0.14141091  0.19141357]\n",
      "learn time: 21462.46213078499\n",
      "284000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.93616658  0.05600454 -0.0240683   0.04439211 -0.03099311  0.02931011\n",
      " -0.11381597  0.06370349  0.02334834  0.03375585], error = [ 0.06383342 -0.05600454  0.0240683  -0.04439211  0.03099311 -0.02931011\n",
      "  0.11381597 -0.06370349 -0.02334834 -0.03375585]\n",
      "learn time: 21538.215566635132\n",
      "285000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00424719 -0.00196452  0.04830817  0.86594645  0.02813118  0.14311235\n",
      " -0.05630698 -0.07182538  0.01253881  0.10190219], error = [-0.00424719  0.00196452 -0.04830817  0.13405355 -0.02813118 -0.14311235\n",
      "  0.05630698  0.07182538 -0.01253881 -0.10190219]\n",
      "learn time: 21614.082084178925\n",
      "286000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.00218386 -0.03775636  0.098276   -0.00691342 -0.05031977  0.02441436\n",
      " -0.06953868 -0.05291662 -0.14831065  1.16916717], error = [-0.00218386  0.03775636 -0.098276    0.00691342  0.05031977 -0.02441436\n",
      "  0.06953868  0.05291662  0.14831065 -0.16916717]\n",
      "learn time: 21689.79076743126\n",
      "287000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.9208732  -0.00141996  0.0612406  -0.04039591 -0.00850303  0.00732936\n",
      "  0.0300067   0.0044168   0.03130664  0.05542834], error = [ 0.0791268   0.00141996 -0.0612406   0.04039591  0.00850303 -0.00732936\n",
      " -0.0300067  -0.0044168  -0.03130664 -0.05542834]\n",
      "learn time: 21765.649070739746\n",
      "288000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.00152764 -0.08063944  0.06156852 -0.0788559   1.00924236  0.08629103\n",
      "  0.01761872  0.05559105  0.09382943 -0.12816306], error = [-0.00152764  0.08063944 -0.06156852  0.0788559  -0.00924236 -0.08629103\n",
      " -0.01761872 -0.05559105 -0.09382943  0.12816306]\n",
      "learn time: 21842.611092805862\n",
      "289000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 2.99970301e-03  1.29906672e-02  4.61055633e-04 -4.49140526e-02\n",
      "  9.67895588e-01  3.60070129e-03 -6.23906024e-03 -5.55660966e-02\n",
      " -1.78710013e-03  1.06899107e-01], error = [-0.0029997  -0.01299067 -0.00046106  0.04491405  0.03210441 -0.0036007\n",
      "  0.00623906  0.0555661   0.0017871  -0.10689911]\n",
      "learn time: 21918.52109837532\n",
      "290000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00169822  0.07380349  0.12128672  0.59511533 -0.03724932  0.11962632\n",
      " -0.02025906 -0.03682191  0.05350535  0.00298598], error = [-0.00169822 -0.07380349 -0.12128672  0.40488467  0.03724932 -0.11962632\n",
      "  0.02025906  0.03682191 -0.05350535 -0.00298598]\n",
      "learn time: 21994.47496676445\n",
      "291000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.0094789  -0.00849846  0.09210703 -0.07677596  0.03684336 -0.01921972\n",
      " -0.02737578  1.14782093  0.01531814 -0.03848756], error = [-0.0094789   0.00849846 -0.09210703  0.07677596 -0.03684336  0.01921972\n",
      "  0.02737578 -0.14782093 -0.01531814  0.03848756]\n",
      "learn time: 22069.8767786026\n",
      "292000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 6.18124999e-03 -2.68704678e-02  6.98882370e-03  8.68909795e-02\n",
      " -6.32421843e-02  5.86985716e-02  1.03776715e+00 -3.09413294e-02\n",
      " -5.38279208e-04  1.83098043e-02], error = [-0.00618125  0.02687047 -0.00698882 -0.08689098  0.06324218 -0.05869857\n",
      " -0.03776715  0.03094133  0.00053828 -0.0183098 ]\n",
      "learn time: 22145.6422021389\n",
      "293000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.03436303  0.00833081  0.20870095 -0.00782691 -0.15090543  0.97327087\n",
      "  0.04307901 -0.00442192  0.05773749 -0.03029229], error = [-0.03436303 -0.00833081 -0.20870095  0.00782691  0.15090543  0.02672913\n",
      " -0.04307901  0.00442192 -0.05773749  0.03029229]\n",
      "learn time: 22221.674513578415\n",
      "294000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.0056568   0.01173822 -0.13861131  0.01036206  0.03807641  0.66449057\n",
      " -0.02698215 -0.02459072  0.31081916  0.05338998], error = [-0.0056568  -0.01173822  0.13861131 -0.01036206 -0.03807641  0.33550943\n",
      "  0.02698215  0.02459072 -0.31081916 -0.05338998]\n",
      "learn time: 22297.28625535965\n",
      "295000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 2.29165134e-04  9.33635374e-01 -1.10909667e-02  6.90769778e-02\n",
      "  5.73579379e-02 -1.64315926e-02  1.77029200e-02 -3.57624509e-02\n",
      " -4.96330465e-02  5.33538157e-03], error = [-0.00022917  0.06636463  0.01109097 -0.06907698 -0.05735794  0.01643159\n",
      " -0.01770292  0.03576245  0.04963305 -0.00533538]\n",
      "learn time: 22372.833857774734\n",
      "296000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 2.15059740e-04  1.02479947e+00 -3.77230901e-02  6.26889637e-02\n",
      " -1.55062102e-02 -1.76411466e-02  3.95296728e-02  2.65221283e-02\n",
      " -4.62876282e-02  1.16485450e-01], error = [-0.00021506 -0.02479947  0.03772309 -0.06268896  0.01550621  0.01764115\n",
      " -0.03952967 -0.02652213  0.04628763 -0.11648545]\n",
      "learn time: 22448.532240629196\n",
      "297000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.96452519 -0.05643172 -0.02829482  0.0582284  -0.10853726  0.03196933\n",
      " -0.08298947  0.00268486  0.0067545  -0.00698906], error = [ 0.03547481  0.05643172  0.02829482 -0.0582284   0.10853726 -0.03196933\n",
      "  0.08298947 -0.00268486 -0.0067545   0.00698906]\n",
      "learn time: 22524.235127210617\n",
      "298000: expected = [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.01355244 -0.02683293  0.86605103 -0.12617317  0.07884497 -0.04963653\n",
      " -0.02749869  0.05126848  0.42933077 -0.09188299], error = [-0.01355244  0.02683293  0.13394897  0.12617317 -0.07884497  0.04963653\n",
      "  0.02749869 -0.05126848 -0.42933077  0.09188299]\n",
      "learn time: 22600.067286252975\n",
      "299000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.00990705  0.06281109  0.03067532  0.06093426 -0.0036368  -0.05550961\n",
      "  0.95244492  0.03424531 -0.01126166 -0.05910221], error = [-0.00990705 -0.06281109 -0.03067532 -0.06093426  0.0036368   0.05550961\n",
      "  0.04755508 -0.03424531  0.01126166  0.05910221]\n",
      "learn time: 22675.73628592491\n",
      "300000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.01211625 -0.00234568  0.16542896  0.16122451 -0.09692862  0.93225967\n",
      "  0.0037766  -0.06058819 -0.08719055  0.08171456], error = [-0.01211625  0.00234568 -0.16542896 -0.16122451  0.09692862  0.06774033\n",
      " -0.0037766   0.06058819  0.08719055 -0.08171456]\n",
      "learn time: 22751.515657663345\n",
      "301000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.91840139  0.0216412  -0.04991104 -0.00745544  0.02481524  0.08093462\n",
      "  0.05343251  0.02976826 -0.03260674 -0.00595832], error = [ 0.08159861 -0.0216412   0.04991104  0.00745544 -0.02481524 -0.08093462\n",
      " -0.05343251 -0.02976826  0.03260674  0.00595832]\n",
      "learn time: 22827.256217241287\n",
      "302000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.07458264  0.02729408  0.12940861 -0.06299999  0.11857101  0.45461552\n",
      "  0.22654321  0.06956339 -0.0335803   0.00194887], error = [-0.07458264 -0.02729408 -0.12940861  0.06299999 -0.11857101  0.54538448\n",
      " -0.22654321 -0.06956339  0.0335803  -0.00194887]\n",
      "learn time: 22902.844511032104\n",
      "303000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.53685428  0.01420792  0.06821443 -0.05167809  0.1489256  -0.01593484\n",
      " -0.03683327 -0.02231529 -0.10814683  0.39630452], error = [-0.53685428 -0.01420792 -0.06821443  0.05167809 -0.1489256   0.01593484\n",
      "  0.03683327  0.02231529  0.10814683  0.60369548]\n",
      "learn time: 22978.616631507874\n",
      "304000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.00537876 -0.05504971  0.07723929  0.12263892  0.02143829 -0.03350772\n",
      "  0.03624408  1.1145702  -0.02227496 -0.25550271], error = [-0.00537876  0.05504971 -0.07723929 -0.12263892 -0.02143829  0.03350772\n",
      " -0.03624408 -0.1145702   0.02227496  0.25550271]\n",
      "learn time: 23054.432583093643\n",
      "305000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.00483624 -0.11625564  0.13264432  0.07339948  0.01068718  0.22332164\n",
      "  0.06678314  0.41574511 -0.21314552  0.32664537], error = [-0.00483624  0.11625564 -0.13264432 -0.07339948 -0.01068718 -0.22332164\n",
      " -0.06678314  0.58425489  0.21314552 -0.32664537]\n",
      "learn time: 23131.019672632217\n",
      "306000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.0107613   0.0090632  -0.01147849  0.05756398 -0.01078791 -0.01994461\n",
      "  1.09124325 -0.02069685 -0.01997658 -0.02576145], error = [-0.0107613  -0.0090632   0.01147849 -0.05756398  0.01078791  0.01994461\n",
      " -0.09124325  0.02069685  0.01997658  0.02576145]\n",
      "learn time: 23207.274017095566\n",
      "307000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 4.85857849e-04 -1.19669818e-02 -6.37853437e-02  2.49683089e-01\n",
      "  1.88839989e-01  1.08743661e-01  4.31463765e-03 -1.00233248e-02\n",
      "  5.07504318e-01 -5.92219872e-02], error = [-4.85857849e-04  1.19669818e-02  6.37853437e-02 -2.49683089e-01\n",
      " -1.88839989e-01 -1.08743661e-01 -4.31463765e-03  1.00233248e-02\n",
      "  4.92495682e-01  5.92219872e-02]\n",
      "learn time: 23282.809933662415\n",
      "308000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.95549667 -0.01180446  0.02998137 -0.02808546  0.04736635 -0.10722541\n",
      "  0.01857119  0.01618135  0.06437647  0.00172175], error = [ 0.04450333  0.01180446 -0.02998137  0.02808546 -0.04736635  0.10722541\n",
      " -0.01857119 -0.01618135 -0.06437647 -0.00172175]\n",
      "learn time: 23358.576410531998\n",
      "309000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.01002822 -0.03595501 -0.00599335 -0.12761104  0.08914498 -0.01293973\n",
      "  0.96380993 -0.04338662  0.12015971  0.04839254], error = [-0.01002822  0.03595501  0.00599335  0.12761104 -0.08914498  0.01293973\n",
      "  0.03619007  0.04338662 -0.12015971 -0.04839254]\n",
      "learn time: 23434.271159648895\n",
      "310000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00263155 -0.00266328 -0.04691779  1.08293754 -0.03011875  0.05776223\n",
      "  0.08014748  0.03058345 -0.03931617 -0.05179108], error = [-0.00263155  0.00266328  0.04691779 -0.08293754  0.03011875 -0.05776223\n",
      " -0.08014748 -0.03058345  0.03931617  0.05179108]\n",
      "learn time: 23510.23455929756\n",
      "311000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.00451012  0.05997282 -0.0438405   0.09642213 -0.01459833 -0.08566723\n",
      "  1.05311798  0.006598   -0.01415039 -0.06883599], error = [-0.00451012 -0.05997282  0.0438405  -0.09642213  0.01459833  0.08566723\n",
      " -0.05311798 -0.006598    0.01415039  0.06883599]\n",
      "learn time: 23585.958661556244\n",
      "312000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.02255437 -0.10106491  0.20352746  0.01636732  0.03747265  0.13158865\n",
      "  0.05574177  0.79038163  0.02619712 -0.24137107], error = [-0.02255437  0.10106491 -0.20352746 -0.01636732 -0.03747265 -0.13158865\n",
      " -0.05574177  0.20961837 -0.02619712  0.24137107]\n",
      "learn time: 23661.594566822052\n",
      "313000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.00702132  0.00371751  0.03466343 -0.08391802  0.00471733  0.18921467\n",
      " -0.00510257  0.94305044 -0.04172214  0.13401372], error = [-0.00702132 -0.00371751 -0.03466343  0.08391802 -0.00471733 -0.18921467\n",
      "  0.00510257  0.05694956  0.04172214 -0.13401372]\n",
      "learn time: 23737.596529960632\n",
      "314000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 1.45222942e-04  1.12251869e+00 -1.76567907e-02 -1.47825281e-01\n",
      " -3.76524385e-02  2.54557092e-02  5.31697403e-02 -7.62361014e-02\n",
      "  7.68302783e-03  1.60492701e-01], error = [-1.45222942e-04 -1.22518695e-01  1.76567907e-02  1.47825281e-01\n",
      "  3.76524385e-02 -2.54557092e-02 -5.31697403e-02  7.62361014e-02\n",
      " -7.68302783e-03 -1.60492701e-01]\n",
      "learn time: 23813.6090426445\n",
      "315000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.02178105 -0.01905027 -0.10982455 -0.05938575  0.05658621  0.81984554\n",
      "  0.18713168  0.1053696   0.03264593 -0.0635626 ], error = [-0.02178105  0.01905027  0.10982455  0.05938575 -0.05658621  0.18015446\n",
      " -0.18713168 -0.1053696  -0.03264593  0.0635626 ]\n",
      "learn time: 23889.324449300766\n",
      "316000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.00604071 -0.00490632 -0.07416311  0.00205317 -0.01806226 -0.10088597\n",
      " -0.02881878  0.03862986  1.21969038 -0.07321791], error = [-0.00604071  0.00490632  0.07416311 -0.00205317  0.01806226  0.10088597\n",
      "  0.02881878 -0.03862986 -0.21969038  0.07321791]\n",
      "learn time: 23965.227748155594\n",
      "317000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.06453507 -0.03270458  0.10675658  0.78239259  0.10747123 -0.08936478\n",
      "  0.05232088  0.04246953 -0.06383756  0.06889934], error = [-0.06453507  0.03270458 -0.10675658  0.21760741 -0.10747123  0.08936478\n",
      " -0.05232088 -0.04246953  0.06383756 -0.06889934]\n",
      "learn time: 24041.71546602249\n",
      "318000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.00353422  0.00381163  0.14484988 -0.04644949  0.80672087 -0.11679678\n",
      "  0.02734334 -0.01755075 -0.06930191  0.28603257], error = [-0.00353422 -0.00381163 -0.14484988  0.04644949  0.19327913  0.11679678\n",
      " -0.02734334  0.01755075  0.06930191 -0.28603257]\n",
      "learn time: 24117.28925871849\n",
      "319000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 2.74422802e-03  4.93301285e-02  8.16629445e-02  2.80607898e-02\n",
      "  1.23088659e-02 -3.34667576e-04 -8.04705269e-02  1.60208293e-01\n",
      "  8.88945873e-01 -1.09212678e-01], error = [-0.00274423 -0.04933013 -0.08166294 -0.02806079 -0.01230887  0.00033467\n",
      "  0.08047053 -0.16020829  0.11105413  0.10921268]\n",
      "learn time: 24192.86522078514\n",
      "320000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.03307564  0.03566628 -0.14282902 -0.18395473  0.07451072  1.20420382\n",
      "  0.00155338  0.06557043 -0.0538117  -0.02380873], error = [-0.03307564 -0.03566628  0.14282902  0.18395473 -0.07451072 -0.20420382\n",
      " -0.00155338 -0.06557043  0.0538117   0.02380873]\n",
      "learn time: 24268.888993263245\n",
      "321000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.00586623 -0.05622288  0.01161953  0.02053274  0.03332078 -0.04978864\n",
      "  0.0101967   0.69279558 -0.02484969  0.28056198], error = [-0.00586623  0.05622288 -0.01161953 -0.02053274 -0.03332078  0.04978864\n",
      " -0.0101967   0.30720442  0.02484969 -0.28056198]\n",
      "learn time: 24344.648731946945\n",
      "322000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00124989  0.50535731 -0.10026322  0.87115383 -0.02075718 -0.19716468\n",
      "  0.03004779 -0.02656128 -0.10616654 -0.05520226], error = [-0.00124989 -0.50535731  0.10026322  0.12884617  0.02075718  0.19716468\n",
      " -0.03004779  0.02656128  0.10616654  0.05520226]\n",
      "learn time: 24420.295917510986\n",
      "323000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.0136206   0.04750804 -0.07078296  0.09649458  0.02668511 -0.07818869\n",
      " -0.03375903  0.95360792 -0.06213715 -0.05186501], error = [-0.0136206  -0.04750804  0.07078296 -0.09649458 -0.02668511  0.07818869\n",
      "  0.03375903  0.04639208  0.06213715  0.05186501]\n",
      "learn time: 24495.922020196915\n",
      "324000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.006913    0.00816769  0.1053783   0.042039   -0.05976321  0.10248046\n",
      "  0.00509156 -0.06166994  0.86409842 -0.02837095], error = [-0.006913   -0.00816769 -0.1053783  -0.042039    0.05976321 -0.10248046\n",
      " -0.00509156  0.06166994  0.13590158  0.02837095]\n",
      "learn time: 24571.48923945427\n",
      "325000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00070829  0.13576007  0.02077937  0.25050168 -0.01039937  0.01000802\n",
      " -0.07675575  0.44508824  0.08158668  0.20903257], error = [-7.08289983e-04 -1.35760075e-01 -2.07793721e-02  7.49498322e-01\n",
      "  1.03993661e-02 -1.00080249e-02  7.67557514e-02 -4.45088238e-01\n",
      " -8.15866845e-02 -2.09032566e-01]\n",
      "learn time: 24647.205958366394\n",
      "326000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 7.88738016e-04 -1.32935170e-02 -1.01726087e-02  6.05638636e-03\n",
      "  9.42490941e-01  4.57193341e-02  2.04016414e-02  8.99723136e-02\n",
      " -1.55732228e-01  4.29154640e-02], error = [-0.00078874  0.01329352  0.01017261 -0.00605639  0.05750906 -0.04571933\n",
      " -0.02040164 -0.08997231  0.15573223 -0.04291546]\n",
      "learn time: 24722.93127679825\n",
      "327000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 3.43071696e-02 -5.90246819e-02 -1.50624941e-01 -2.56062980e-04\n",
      "  1.08405531e-01  5.19114032e-01  2.24205533e-01  4.87506283e-02\n",
      "  1.12159661e-01  3.23594755e-02], error = [-3.43071696e-02  5.90246819e-02  1.50624941e-01  2.56062980e-04\n",
      " -1.08405531e-01  4.80885968e-01 -2.24205533e-01 -4.87506283e-02\n",
      " -1.12159661e-01 -3.23594755e-02]\n",
      "learn time: 24798.565654993057\n",
      "328000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 5.42594092e-04  9.74703226e-01 -7.30626601e-03 -1.18894376e-02\n",
      " -1.61112443e-02  8.86505315e-02 -3.80387862e-02  3.10158886e-02\n",
      "  3.90732395e-02 -6.20880673e-03], error = [-0.00054259  0.02529677  0.00730627  0.01188944  0.01611124 -0.08865053\n",
      "  0.03803879 -0.03101589 -0.03907324  0.00620881]\n",
      "learn time: 24874.326120376587\n",
      "329000: expected = [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.00137694  0.03512638  0.76427696  0.1893785   0.01556331  0.0032148\n",
      " -0.12767021  0.03212508  0.11439287  0.00879108], error = [-0.00137694 -0.03512638  0.23572304 -0.1893785  -0.01556331 -0.0032148\n",
      "  0.12767021 -0.03212508 -0.11439287 -0.00879108]\n",
      "learn time: 24950.236535549164\n",
      "330000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00308812 -0.0145887  -0.05145348  1.1709174  -0.03010987 -0.06053301\n",
      "  0.05321828 -0.04297752 -0.00461311  0.12111816], error = [-0.00308812  0.0145887   0.05145348 -0.1709174   0.03010987  0.06053301\n",
      " -0.05321828  0.04297752  0.00461311 -0.12111816]\n",
      "learn time: 25025.89457178116\n",
      "331000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.06057685  0.01658941  0.01293689 -0.17919531  0.02041404  0.51274422\n",
      "  0.5897496   0.03742633 -0.03133377  0.02906217], error = [-0.06057685 -0.01658941 -0.01293689  0.17919531 -0.02041404 -0.51274422\n",
      "  0.4102504  -0.03742633  0.03133377 -0.02906217]\n",
      "learn time: 25101.548518657684\n",
      "332000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.00214589  0.00520112 -0.00657816  0.08137148  0.0631718  -0.10858937\n",
      "  0.06209118  0.00746686  1.03423143 -0.09568544], error = [-0.00214589 -0.00520112  0.00657816 -0.08137148 -0.0631718   0.10858937\n",
      " -0.06209118 -0.00746686 -0.03423143  0.09568544]\n",
      "learn time: 25177.43803715706\n",
      "333000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00459751 -0.04058242 -0.00600955  0.38447379  0.05559529  0.64645632\n",
      " -0.01102446  0.04691654  0.03741105 -0.02757205], error = [-0.00459751  0.04058242  0.00600955  0.61552621 -0.05559529 -0.64645632\n",
      "  0.01102446 -0.04691654 -0.03741105  0.02757205]\n",
      "learn time: 25253.16092276573\n",
      "334000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.00217876 -0.00411193 -0.02161863 -0.01886331  0.7601698  -0.04653762\n",
      "  0.03870389  0.03659687 -0.01044499  0.3803726 ], error = [-0.00217876  0.00411193  0.02161863  0.01886331  0.2398302   0.04653762\n",
      " -0.03870389 -0.03659687  0.01044499 -0.3803726 ]\n",
      "learn time: 25329.05746459961\n",
      "335000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 2.85032458e-04  1.04106304e+00 -3.28446975e-02 -2.55048313e-02\n",
      "  3.30206734e-02  8.74074801e-02  1.64171539e-02 -2.47979501e-02\n",
      " -9.52095038e-03 -1.12845587e-01], error = [-0.00028503 -0.04106304  0.0328447   0.02550483 -0.03302067 -0.08740748\n",
      " -0.01641715  0.02479795  0.00952095  0.11284559]\n",
      "learn time: 25404.69124007225\n",
      "336000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.00245124  0.0541268   0.00967972 -0.02319635  0.11635638  0.03531323\n",
      " -0.00501662 -0.09450606  0.18080626  0.68159458], error = [-0.00245124 -0.0541268  -0.00967972  0.02319635 -0.11635638 -0.03531323\n",
      "  0.00501662  0.09450606 -0.18080626  0.31840542]\n",
      "learn time: 25480.242721557617\n",
      "337000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.00131265 -0.04004327 -0.0244287   0.04297602  0.93592488 -0.02036675\n",
      "  0.15758514  0.02470695  0.03719946 -0.13810957], error = [-0.00131265  0.04004327  0.0244287  -0.04297602  0.06407512  0.02036675\n",
      " -0.15758514 -0.02470695 -0.03719946  0.13810957]\n",
      "learn time: 25556.261798858643\n",
      "338000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.00639376 -0.03264945 -0.01903247  0.0623411   0.03761445  0.01056803\n",
      "  0.04544762  0.00964545  0.74838483  0.12098392], error = [-0.00639376  0.03264945  0.01903247 -0.0623411  -0.03761445 -0.01056803\n",
      " -0.04544762 -0.00964545  0.25161517 -0.12098392]\n",
      "learn time: 25632.21216726303\n",
      "339000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.00062319 -0.01156289 -0.11885961  0.13963027  0.54477175  0.05942767\n",
      "  0.02345184 -0.0147187   0.09277335  0.30934933], error = [-6.23193046e-04  1.15628907e-02  1.18859615e-01 -1.39630266e-01\n",
      " -5.44771752e-01 -5.94276699e-02 -2.34518369e-02  1.47186968e-02\n",
      " -9.27733461e-02  6.90650674e-01]\n",
      "learn time: 25708.096180438995\n",
      "340000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.00272974  0.0299936  -0.1138655  -0.06420724 -0.01439642  0.20371402\n",
      " -0.00614018  1.05242162 -0.07590847  0.08558524], error = [-0.00272974 -0.0299936   0.1138655   0.06420724  0.01439642 -0.20371402\n",
      "  0.00614018 -0.05242162  0.07590847 -0.08558524]\n",
      "learn time: 25784.102428674698\n",
      "341000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.00889465  0.02280762  0.0655239  -0.08295243  0.25303584  0.6776989\n",
      " -0.08982946  0.07369112 -0.20673658  0.34103207], error = [-0.00889465 -0.02280762 -0.0655239   0.08295243 -0.25303584  0.3223011\n",
      "  0.08982946 -0.07369112  0.20673658 -0.34103207]\n",
      "learn time: 25860.05721259117\n",
      "342000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 3.76219296e-04  1.08564079e+00 -1.28548652e-01  4.27123591e-02\n",
      " -1.61997323e-02 -4.19456418e-02  3.42506292e-02  5.90840969e-02\n",
      "  8.34546123e-02 -8.38229544e-02], error = [-0.00037622 -0.08564079  0.12854865 -0.04271236  0.01619973  0.04194564\n",
      " -0.03425063 -0.0590841  -0.08345461  0.08382295]\n",
      "learn time: 25935.62701368332\n",
      "343000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.00170713  0.03163189 -0.00299874  0.14482025  0.17703208  0.00234194\n",
      " -0.06059304  0.04550386 -0.12183668  0.80663108], error = [-0.00170713 -0.03163189  0.00299874 -0.14482025 -0.17703208 -0.00234194\n",
      "  0.06059304 -0.04550386  0.12183668  0.19336892]\n",
      "learn time: 26011.116040229797\n",
      "344000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.94466973  0.05043633 -0.04372049  0.05555469 -0.01524111  0.04058138\n",
      " -0.11045692  0.06552078  0.00899917  0.01008319], error = [ 0.05533027 -0.05043633  0.04372049 -0.05555469  0.01524111 -0.04058138\n",
      "  0.11045692 -0.06552078 -0.00899917 -0.01008319]\n",
      "learn time: 26086.85604453087\n",
      "345000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00469771 -0.00611435  0.04140182  0.91215502  0.04121888  0.13450891\n",
      " -0.04770761 -0.06282729 -0.03398881  0.09323368], error = [-0.00469771  0.00611435 -0.04140182  0.08784498 -0.04121888 -0.13450891\n",
      "  0.04770761  0.06282729  0.03398881 -0.09323368]\n",
      "learn time: 26162.575382709503\n",
      "346000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.00156478 -0.04317983  0.08971818 -0.00435074 -0.05835008  0.01343428\n",
      " -0.06978759 -0.07203354 -0.14263728  1.19201558], error = [-0.00156478  0.04317983 -0.08971818  0.00435074  0.05835008 -0.01343428\n",
      "  0.06978759  0.07203354  0.14263728 -0.19201558]\n",
      "learn time: 26238.412947654724\n",
      "347000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.93369203 -0.00254171  0.04614558 -0.0355453  -0.01461085  0.00712808\n",
      "  0.02948234 -0.00424621  0.03430643  0.05151282], error = [ 0.06630797  0.00254171 -0.04614558  0.0355453   0.01461085 -0.00712808\n",
      " -0.02948234  0.00424621 -0.03430643 -0.05151282]\n",
      "learn time: 26314.23136305809\n",
      "348000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.00122167 -0.0720762   0.05230293 -0.07666259  1.03054973  0.06757996\n",
      "  0.01824193  0.0543603   0.08834906 -0.13027992], error = [-0.00122167  0.0720762  -0.05230293  0.07666259 -0.03054973 -0.06757996\n",
      " -0.01824193 -0.0543603  -0.08834906  0.13027992]\n",
      "learn time: 26390.12572169304\n",
      "349000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.00305519  0.00793657  0.01832322 -0.05302637  0.98959533 -0.00623856\n",
      " -0.02985746 -0.06913106  0.00339634  0.10931199], error = [-0.00305519 -0.00793657 -0.01832322  0.05302637  0.01040467  0.00623856\n",
      "  0.02985746  0.06913106 -0.00339634 -0.10931199]\n",
      "learn time: 26465.981734752655\n",
      "350000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 1.65322650e-03  7.60327179e-02  7.71870711e-02  6.57092259e-01\n",
      " -4.99423229e-02  1.12832175e-01 -8.27737940e-03 -2.93292891e-02\n",
      "  3.08205361e-02 -3.73114710e-04], error = [-0.00165323 -0.07603272 -0.07718707  0.34290774  0.04994232 -0.11283218\n",
      "  0.00827738  0.02932929 -0.03082054  0.00037311]\n",
      "learn time: 26541.937561750412\n",
      "351000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.00745715 -0.01760571  0.08823985 -0.04586808  0.04110696 -0.04692999\n",
      " -0.01939754  1.14280722  0.01251398 -0.04548636], error = [-0.00745715  0.01760571 -0.08823985  0.04586808 -0.04110696  0.04692999\n",
      "  0.01939754 -0.14280722 -0.01251398  0.04548636]\n",
      "learn time: 26617.77844429016\n",
      "352000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.00616287 -0.02388376  0.01514605  0.06604948 -0.05777534  0.05699345\n",
      "  1.0340432  -0.03465209  0.00660614  0.02346202], error = [-0.00616287  0.02388376 -0.01514605 -0.06604948  0.05777534 -0.05699345\n",
      " -0.0340432   0.03465209 -0.00660614 -0.02346202]\n",
      "learn time: 26693.87386727333\n",
      "353000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 3.28051752e-02  1.00385656e-02  1.72394362e-01  7.99908713e-04\n",
      " -1.38744196e-01  9.61129771e-01  4.67869099e-02  1.53345286e-03\n",
      "  3.56195282e-02 -2.00870363e-02], error = [-0.03280518 -0.01003857 -0.17239436 -0.00079991  0.1387442   0.03887023\n",
      " -0.04678691 -0.00153345 -0.03561953  0.02008704]\n",
      "learn time: 26769.67159318924\n",
      "354000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.00466352 -0.00389388 -0.13352018  0.00780298  0.03956798  0.70350528\n",
      " -0.04369424 -0.04039386  0.280488    0.08746   ], error = [-0.00466352  0.00389388  0.13352018 -0.00780298 -0.03956798  0.29649472\n",
      "  0.04369424  0.04039386 -0.280488   -0.08746   ]\n",
      "learn time: 26845.59938979149\n",
      "355000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 1.51889622e-04  9.46250890e-01 -1.21133600e-02  6.95674950e-02\n",
      "  5.52026031e-02 -1.02254067e-02  1.59852721e-02 -4.23119117e-02\n",
      " -5.06746255e-02  4.00620127e-04], error = [-0.00015189  0.05374911  0.01211336 -0.06956749 -0.0552026   0.01022541\n",
      " -0.01598527  0.04231191  0.05067463 -0.00040062]\n",
      "learn time: 26921.514817237854\n",
      "356000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 1.54715477e-04  1.02109471e+00 -5.05231412e-02  5.73697221e-02\n",
      " -5.92026714e-03 -2.68199610e-02  4.48961402e-02  4.08419833e-02\n",
      " -2.09334451e-02  9.83277232e-02], error = [-0.00015472 -0.02109471  0.05052314 -0.05736972  0.00592027  0.02681996\n",
      " -0.04489614 -0.04084198  0.02093345 -0.09832772]\n",
      "learn time: 26997.52750635147\n",
      "357000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.97279422 -0.03993027 -0.0304171   0.05274136 -0.09519862  0.04972993\n",
      " -0.07151631 -0.00140043  0.02259146 -0.00218857], error = [ 0.02720578  0.03993027  0.0304171  -0.05274136  0.09519862 -0.04972993\n",
      "  0.07151631  0.00140043 -0.02259146  0.00218857]\n",
      "learn time: 27073.392532110214\n",
      "358000: expected = [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.01392996 -0.03084315  0.91017479 -0.13526118  0.08338362 -0.04095141\n",
      " -0.0185354   0.03254701  0.36907653 -0.08210249], error = [-0.01392996  0.03084315  0.08982521  0.13526118 -0.08338362  0.04095141\n",
      "  0.0185354  -0.03254701 -0.36907653  0.08210249]\n",
      "learn time: 27149.78103494644\n",
      "359000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 8.15985045e-03  5.53488544e-02  3.34959486e-02  6.40638618e-02\n",
      " -7.57647799e-04 -6.67367883e-02  9.59027921e-01  3.09271346e-02\n",
      " -4.17972164e-03 -5.84253854e-02], error = [-0.00815985 -0.05534885 -0.03349595 -0.06406386  0.00075765  0.06673679\n",
      "  0.04097208 -0.03092713  0.00417972  0.05842539]\n",
      "learn time: 27225.713022470474\n",
      "360000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.01024099  0.01770397  0.13380033  0.16772607 -0.0740693   0.9120795\n",
      "  0.01680974 -0.07127899 -0.06548014  0.07993432], error = [-0.01024099 -0.01770397 -0.13380033 -0.16772607  0.0740693   0.0879205\n",
      " -0.01680974  0.07127899  0.06548014 -0.07993432]\n",
      "learn time: 27301.56818675995\n",
      "361000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 9.31176884e-01  1.72068797e-02 -4.95386309e-02  1.70869081e-04\n",
      "  1.51561864e-02  7.39548345e-02  4.99756547e-02  4.22918426e-02\n",
      " -3.38602066e-02 -2.18491192e-03], error = [ 0.06882312 -0.01720688  0.04953863 -0.00017087 -0.01515619 -0.07395483\n",
      " -0.04997565 -0.04229184  0.03386021  0.00218491]\n",
      "learn time: 27377.385011672974\n",
      "362000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.06132853  0.01358906  0.13666328 -0.05936428  0.12535193  0.44128989\n",
      "  0.21081305  0.07388348 -0.01882384  0.00494255], error = [-0.06132853 -0.01358906 -0.13666328  0.05936428 -0.12535193  0.55871011\n",
      " -0.21081305 -0.07388348  0.01882384 -0.00494255]\n",
      "learn time: 27453.526896238327\n",
      "363000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.53468167  0.00926134  0.05840487 -0.03571856  0.14384138 -0.01686608\n",
      " -0.04970703  0.00403961 -0.12041202  0.3928656 ], error = [-0.53468167 -0.00926134 -0.05840487  0.03571856 -0.14384138  0.01686608\n",
      "  0.04970703 -0.00403961  0.12041202  0.6071344 ]\n",
      "learn time: 27529.205047369003\n",
      "364000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.00498419 -0.05562159  0.09565443  0.10425041  0.01615477 -0.0452307\n",
      "  0.02685621  1.119453   -0.0242883  -0.23723662], error = [-0.00498419  0.05562159 -0.09565443 -0.10425041 -0.01615477  0.0452307\n",
      " -0.02685621 -0.119453    0.0242883   0.23723662]\n",
      "learn time: 27605.095906734467\n",
      "365000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.0044644  -0.10373014  0.14027743  0.07656597  0.00141444  0.19183414\n",
      "  0.05062898  0.47144332 -0.19257129  0.28569404], error = [-0.0044644   0.10373014 -0.14027743 -0.07656597 -0.00141444 -0.19183414\n",
      " -0.05062898  0.52855668  0.19257129 -0.28569404]\n",
      "learn time: 27680.674781799316\n",
      "366000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 9.25499533e-03  8.15530606e-03 -1.49436018e-02  4.39005992e-02\n",
      " -3.89551772e-03 -2.25180470e-02  1.09086117e+00 -2.88759845e-02\n",
      " -3.83105225e-04 -2.14399911e-02], error = [-0.009255   -0.00815531  0.0149436  -0.0439006   0.00389552  0.02251805\n",
      " -0.09086117  0.02887598  0.00038311  0.02143999]\n",
      "learn time: 27756.289014816284\n",
      "367000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 3.78185374e-04 -1.32291494e-02 -7.58642788e-02  2.34364300e-01\n",
      "  1.85017506e-01  1.03911516e-01  3.23511355e-03 -8.00431590e-03\n",
      "  5.40755184e-01 -4.74834379e-02], error = [-3.78185374e-04  1.32291494e-02  7.58642788e-02 -2.34364300e-01\n",
      " -1.85017506e-01 -1.03911516e-01 -3.23511355e-03  8.00431590e-03\n",
      "  4.59244816e-01  4.74834379e-02]\n",
      "learn time: 27832.058000802994\n",
      "368000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.96116124 -0.00880123  0.03597677 -0.02180244  0.03402895 -0.11839671\n",
      "  0.02677755  0.0218361   0.06488287  0.0030596 ], error = [ 0.03883876  0.00880123 -0.03597677  0.02180244 -0.03402895  0.11839671\n",
      " -0.02677755 -0.0218361  -0.06488287 -0.0030596 ]\n",
      "learn time: 27907.729332447052\n",
      "369000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.00813741 -0.0522452  -0.02612642 -0.14408671  0.07659918 -0.00329712\n",
      "  0.9923823  -0.04778976  0.16477374  0.04226184], error = [-0.00813741  0.0522452   0.02612642  0.14408671 -0.07659918  0.00329712\n",
      "  0.0076177   0.04778976 -0.16477374 -0.04226184]\n",
      "learn time: 27983.398485660553\n",
      "370000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00273554  0.0032179  -0.02935953  1.05076726 -0.03921646  0.03935365\n",
      "  0.06712393  0.02940697 -0.02087511 -0.04674329], error = [-0.00273554 -0.0032179   0.02935953 -0.05076726  0.03921646 -0.03935365\n",
      " -0.06712393 -0.02940697  0.02087511  0.04674329]\n",
      "learn time: 28059.14712047577\n",
      "371000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.0035312   0.05206236 -0.04620387  0.08709135 -0.00972467 -0.08205496\n",
      "  1.03747772  0.02081376 -0.00919222 -0.06476454], error = [-0.0035312  -0.05206236  0.04620387 -0.08709135  0.00972467  0.08205496\n",
      " -0.03747772 -0.02081376  0.00919222  0.06476454]\n",
      "learn time: 28134.722482204437\n",
      "372000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.01610463 -0.09872991  0.19745784  0.03736057  0.03327338  0.1303551\n",
      "  0.06299974  0.79078748  0.02982641 -0.22302527], error = [-0.01610463  0.09872991 -0.19745784 -0.03736057 -0.03327338 -0.1303551\n",
      " -0.06299974  0.20921252 -0.02982641  0.22302527]\n",
      "learn time: 28210.387694597244\n",
      "373000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.00562716  0.00732947  0.03414101 -0.08116815 -0.01682236  0.16114214\n",
      "  0.004428    0.94165196 -0.03186629  0.15006739], error = [-0.00562716 -0.00732947 -0.03414101  0.08116815  0.01682236 -0.16114214\n",
      " -0.004428    0.05834804  0.03186629 -0.15006739]\n",
      "learn time: 28286.39259505272\n",
      "374000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 1.05866553e-04  1.11034965e+00 -1.21357643e-02 -1.50117976e-01\n",
      " -3.66073087e-02  2.46317456e-02  6.12713174e-02 -8.29281232e-02\n",
      "  1.24192911e-02  1.60669392e-01], error = [-1.05866553e-04 -1.10349651e-01  1.21357643e-02  1.50117976e-01\n",
      "  3.66073087e-02 -2.46317456e-02 -6.12713174e-02  8.29281232e-02\n",
      " -1.24192911e-02 -1.60669392e-01]\n",
      "learn time: 28362.366482257843\n",
      "375000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.01961062 -0.0248522  -0.10953741 -0.04666564  0.04519901  0.8318052\n",
      "  0.17338625  0.10136058  0.02318251 -0.05043357], error = [-0.01961062  0.0248522   0.10953741  0.04666564 -0.04519901  0.1681948\n",
      " -0.17338625 -0.10136058 -0.02318251  0.05043357]\n",
      "learn time: 28438.176079034805\n",
      "376000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.00541155 -0.00936917 -0.07040135 -0.00646138 -0.01365293 -0.09878595\n",
      " -0.02004396  0.03287192  1.20367313 -0.05537234], error = [-0.00541155  0.00936917  0.07040135  0.00646138  0.01365293  0.09878595\n",
      "  0.02004396 -0.03287192 -0.20367313  0.05537234]\n",
      "learn time: 28513.92890548706\n",
      "377000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.06269857 -0.03034971  0.0903221   0.81809859  0.09471576 -0.11646726\n",
      "  0.04187486  0.0415457  -0.05437601  0.07165857], error = [-0.06269857  0.03034971 -0.0903221   0.18190141 -0.09471576  0.11646726\n",
      " -0.04187486 -0.0415457   0.05437601 -0.07165857]\n",
      "learn time: 28589.752546787262\n",
      "378000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.00278432  0.00611064  0.12072418 -0.05082455  0.84307916 -0.10819724\n",
      "  0.02239575 -0.01393025 -0.0795237   0.25696736], error = [-0.00278432 -0.00611064 -0.12072418  0.05082455  0.15692084  0.10819724\n",
      " -0.02239575  0.01393025  0.0795237  -0.25696736]\n",
      "learn time: 28665.69190430641\n",
      "379000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.0022609   0.05212294  0.06679492  0.03365593  0.00529764  0.01028737\n",
      " -0.07089689  0.15409239  0.89040281 -0.09627667], error = [-0.0022609  -0.05212294 -0.06679492 -0.03365593 -0.00529764 -0.01028737\n",
      "  0.07089689 -0.15409239  0.10959719  0.09627667]\n",
      "learn time: 28741.586473464966\n",
      "380000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.02490909  0.0308384  -0.14735492 -0.18850482  0.0794501   1.22533005\n",
      "  0.00859326  0.06289819 -0.05284251 -0.03305374], error = [-0.02490909 -0.0308384   0.14735492  0.18850482 -0.0794501  -0.22533005\n",
      " -0.00859326 -0.06289819  0.05284251  0.03305374]\n",
      "learn time: 28817.329746484756\n",
      "381000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.00484423 -0.0563431   0.01459824  0.02656544  0.03432187 -0.04843251\n",
      " -0.00166665  0.72865247 -0.02065111  0.24355399], error = [-0.00484423  0.0563431  -0.01459824 -0.02656544 -0.03432187  0.04843251\n",
      "  0.00166665  0.27134753  0.02065111 -0.24355399]\n",
      "learn time: 28893.192720651627\n",
      "382000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00117066  0.48140922 -0.09842305  0.86328227 -0.02856525 -0.18265068\n",
      "  0.02002161 -0.02816279 -0.11317433 -0.04262607], error = [-0.00117066 -0.48140922  0.09842305  0.13671773  0.02856525  0.18265068\n",
      " -0.02002161  0.02816279  0.11317433  0.04262607]\n",
      "learn time: 28969.084307909012\n",
      "383000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.01137733  0.04856914 -0.0535321   0.07073727  0.03772408 -0.06819609\n",
      " -0.02701244  0.94927255 -0.02957512 -0.05737456], error = [-0.01137733 -0.04856914  0.0535321  -0.07073727 -0.03772408  0.06819609\n",
      "  0.02701244  0.05072745  0.02957512  0.05737456]\n",
      "learn time: 29044.92508983612\n",
      "384000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.00594933  0.00893264  0.10202071  0.03193035 -0.06054022  0.10282004\n",
      "  0.00558457 -0.06480256  0.88895228 -0.02802663], error = [-0.00594933 -0.00893264 -0.10202071 -0.03193035  0.06054022 -0.10282004\n",
      " -0.00558457  0.06480256  0.11104772  0.02802663]\n",
      "learn time: 29120.73574900627\n",
      "385000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00060327  0.11723991  0.0123508   0.27157156 -0.00755222  0.02102006\n",
      " -0.06757385  0.4535557   0.08165426  0.2016171 ], error = [-6.03273541e-04 -1.17239909e-01 -1.23508005e-02  7.28428443e-01\n",
      "  7.55222272e-03 -2.10200638e-02  6.75738546e-02 -4.53555702e-01\n",
      " -8.16542594e-02 -2.01617099e-01]\n",
      "learn time: 29196.838780879974\n",
      "386000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 6.43950807e-04 -1.41545670e-02 -4.34338138e-03  7.24997051e-03\n",
      "  9.56603167e-01  5.50121804e-02  1.95624189e-02  8.81975798e-02\n",
      " -1.50458211e-01  2.56715538e-02], error = [-0.00064395  0.01415457  0.00434338 -0.00724997  0.04339683 -0.05501218\n",
      " -0.01956242 -0.08819758  0.15045821 -0.02567155]\n",
      "learn time: 29272.577065467834\n",
      "387000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.02581442 -0.05914618 -0.14160433  0.01295583  0.06619213  0.54760029\n",
      "  0.23377676  0.04164479  0.08730677  0.05714736], error = [-0.02581442  0.05914618  0.14160433 -0.01295583 -0.06619213  0.45239971\n",
      " -0.23377676 -0.04164479 -0.08730677 -0.05714736]\n",
      "learn time: 29348.434536218643\n",
      "388000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 4.24541840e-04  9.92322487e-01 -4.91558682e-03  2.88157475e-03\n",
      " -1.37209895e-02  6.35170879e-02 -5.07172728e-02  3.20261695e-02\n",
      "  2.48034465e-02 -1.98626742e-04], error = [-0.00042454  0.00767751  0.00491559 -0.00288157  0.01372099 -0.06351709\n",
      "  0.05071727 -0.03202617 -0.02480345  0.00019863]\n",
      "learn time: 29424.510462522507\n",
      "389000: expected = [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.00136062  0.02383896  0.77577922  0.18849199  0.00767974 -0.01055586\n",
      " -0.1255267   0.03396436  0.09515394  0.02418592], error = [-0.00136062 -0.02383896  0.22422078 -0.18849199 -0.00767974  0.01055586\n",
      "  0.1255267  -0.03396436 -0.09515394 -0.02418592]\n",
      "learn time: 29500.25426030159\n",
      "390000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00335885 -0.01929952 -0.05443814  1.17747094 -0.01817146 -0.05341855\n",
      "  0.05823525 -0.04888004 -0.02551636  0.10428752], error = [-0.00335885  0.01929952  0.05443814 -0.17747094  0.01817146  0.05341855\n",
      " -0.05823525  0.04888004  0.02551636 -0.10428752]\n",
      "learn time: 29576.059171438217\n",
      "391000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.05357647  0.01480018  0.01442176 -0.16243207  0.01237612  0.4539254\n",
      "  0.61405857  0.03725538 -0.04839609  0.04424682], error = [-0.05357647 -0.01480018 -0.01442176  0.16243207 -0.01237612 -0.4539254\n",
      "  0.38594143 -0.03725538  0.04839609 -0.04424682]\n",
      "learn time: 29651.857039928436\n",
      "392000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 0.00176908  0.00279399 -0.01525337  0.08422569  0.04872506 -0.11454102\n",
      "  0.07010671  0.00945084  1.05099076 -0.08671709], error = [-0.00176908 -0.00279399  0.01525337 -0.08422569 -0.04872506  0.11454102\n",
      " -0.07010671 -0.00945084 -0.05099076  0.08671709]\n",
      "learn time: 29727.714728355408\n",
      "393000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.0042137  -0.03433037 -0.00979409  0.39600376  0.04702179  0.64235243\n",
      " -0.01964124  0.04184105  0.04370531 -0.01999962], error = [-0.0042137   0.03433037  0.00979409  0.60399624 -0.04702179 -0.64235243\n",
      "  0.01964124 -0.04184105 -0.04370531  0.01999962]\n",
      "learn time: 29803.37319278717\n",
      "394000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.00171843 -0.01188933 -0.02630354 -0.02941714  0.80170304 -0.05109123\n",
      "  0.03742058  0.03982139  0.01217042  0.34137841], error = [-0.00171843  0.01188933  0.02630354  0.02941714  0.19829696  0.05109123\n",
      " -0.03742058 -0.03982139 -0.01217042 -0.34137841]\n",
      "learn time: 29879.173315048218\n",
      "395000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 2.11674995e-04  1.03701563e+00 -2.97298417e-02 -3.26521132e-02\n",
      "  3.88113305e-02  8.20704972e-02  2.81675430e-02 -2.12107311e-02\n",
      " -1.55626860e-02 -1.08995260e-01], error = [-0.00021167 -0.03701563  0.02972984  0.03265211 -0.03881133 -0.0820705\n",
      " -0.02816754  0.02121073  0.01556269  0.10899526]\n",
      "learn time: 29954.76412296295\n",
      "396000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 2.18177175e-03  6.33625944e-02 -5.84105744e-05 -1.84865098e-02\n",
      "  1.21407713e-01  3.74105362e-02 -8.08354998e-03 -9.93029903e-02\n",
      "  1.84712705e-01  6.95942865e-01], error = [-2.18177175e-03 -6.33625944e-02  5.84105744e-05  1.84865098e-02\n",
      " -1.21407713e-01 -3.74105362e-02  8.08354998e-03  9.93029903e-02\n",
      " -1.84712705e-01  3.04057135e-01]\n",
      "learn time: 30030.73986530304\n",
      "397000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.00109612 -0.0338242  -0.0216997   0.03715338  0.95752657 -0.04272627\n",
      "  0.16517512  0.00982194  0.0454293  -0.13241427], error = [-0.00109612  0.0338242   0.0216997  -0.03715338  0.04247343  0.04272627\n",
      " -0.16517512 -0.00982194 -0.0454293   0.13241427]\n",
      "learn time: 30106.91747736931\n",
      "398000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.], gained = [ 5.84883833e-03 -3.04180264e-02 -2.10817792e-02  6.30502543e-02\n",
      "  3.99107750e-02  6.26247357e-04  4.25431316e-02  6.48505024e-03\n",
      "  7.72339166e-01  1.05154487e-01], error = [-0.00584884  0.03041803  0.02108178 -0.06305025 -0.03991078 -0.00062625\n",
      " -0.04254313 -0.00648505  0.22766083 -0.10515449]\n",
      "learn time: 30183.304982185364\n",
      "399000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 5.27358820e-04 -9.44414241e-03 -1.31600345e-01  1.44372939e-01\n",
      "  5.44916146e-01  6.16957760e-02  2.78638679e-02 -1.41315046e-02\n",
      "  9.12437177e-02  3.18861913e-01], error = [-5.27358820e-04  9.44414241e-03  1.31600345e-01 -1.44372939e-01\n",
      " -5.44916146e-01 -6.16957760e-02 -2.78638679e-02  1.41315046e-02\n",
      " -9.12437177e-02  6.81138087e-01]\n",
      "learn time: 30259.285249233246\n",
      "400000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.0022908   0.01481448 -0.11376891 -0.0646389  -0.00397823  0.18039317\n",
      " -0.01176725  1.05117086 -0.04891428  0.08444148], error = [-0.0022908  -0.01481448  0.11376891  0.0646389   0.00397823 -0.18039317\n",
      "  0.01176725 -0.05117086  0.04891428 -0.08444148]\n",
      "learn time: 30335.430504083633\n",
      "401000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.0078782   0.03206699  0.05546999 -0.07264056  0.25175731  0.69375308\n",
      " -0.08312494  0.08136358 -0.20775893  0.32341391], error = [-0.0078782  -0.03206699 -0.05546999  0.07264056 -0.25175731  0.30624692\n",
      "  0.08312494 -0.08136358  0.20775893 -0.32341391]\n",
      "learn time: 30411.43985915184\n",
      "402000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 2.81267115e-04  1.07518126e+00 -1.32090434e-01  3.91007849e-02\n",
      " -1.10426347e-02 -3.17822350e-02  3.10763782e-02  5.18654890e-02\n",
      "  9.78437142e-02 -8.40745656e-02], error = [-0.00028127 -0.07518126  0.13209043 -0.03910078  0.01104263  0.03178224\n",
      " -0.03107638 -0.05186549 -0.09784371  0.08407457]\n",
      "learn time: 30487.166857004166\n",
      "403000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 0.00142248  0.0435018   0.01409303  0.1557783   0.17658637 -0.02141085\n",
      " -0.08620582  0.03804853 -0.10296004  0.7920394 ], error = [-0.00142248 -0.0435018  -0.01409303 -0.1557783  -0.17658637  0.02141085\n",
      "  0.08620582 -0.03804853  0.10296004  0.2079606 ]\n",
      "learn time: 30562.744310617447\n",
      "404000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.94951295  0.04322937 -0.0616773   0.06018627 -0.00390476  0.05266683\n",
      " -0.10416956  0.0653155  -0.00193435 -0.00356057], error = [ 0.05048705 -0.04322937  0.0616773  -0.06018627  0.00390476 -0.05266683\n",
      "  0.10416956 -0.0653155   0.00193435  0.00356057]\n",
      "learn time: 30638.30504822731\n",
      "405000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00542656 -0.00955403  0.03637983  0.94025094  0.05300635  0.13012129\n",
      " -0.03829644 -0.05393336 -0.0700907   0.08572637], error = [-0.00542656  0.00955403 -0.03637983  0.05974906 -0.05300635 -0.13012129\n",
      "  0.03829644  0.05393336  0.0700907  -0.08572637]\n",
      "learn time: 30714.29544711113\n",
      "406000: expected = [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], gained = [ 1.16510303e-03 -4.40610490e-02  8.39035855e-02 -1.76399728e-03\n",
      " -5.55274129e-02  6.83415272e-03 -6.74193941e-02 -8.43050826e-02\n",
      " -1.38722043e-01  1.19813640e+00], error = [-0.0011651   0.04406105 -0.08390359  0.001764    0.05552741 -0.00683415\n",
      "  0.06741939  0.08430508  0.13872204 -0.1981364 ]\n",
      "learn time: 30790.610235214233\n",
      "407000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.94322062 -0.00243954  0.0340804  -0.0309973  -0.01773926  0.00481652\n",
      "  0.03398213 -0.01066132  0.04015769  0.04446256], error = [ 0.05677938  0.00243954 -0.0340804   0.0309973   0.01773926 -0.00481652\n",
      " -0.03398213  0.01066132 -0.04015769 -0.04446256]\n",
      "learn time: 30866.55157184601\n",
      "408000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 1.00830728e-03 -6.66391794e-02  4.33944038e-02 -7.49366895e-02\n",
      "  1.04693221e+00  5.54198910e-02  1.84057708e-02  5.47099710e-02\n",
      "  7.94747433e-02 -1.29748664e-01], error = [-0.00100831  0.06663918 -0.0433944   0.07493669 -0.04693221 -0.05541989\n",
      " -0.01840577 -0.05470997 -0.07947474  0.12974866]\n",
      "learn time: 30942.685322523117\n",
      "409000: expected = [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.], gained = [ 0.00301641  0.00626716  0.03443133 -0.0571879   1.00257254 -0.02096005\n",
      " -0.04111043 -0.07783134  0.00768949  0.10654733], error = [-0.00301641 -0.00626716 -0.03443133  0.0571879  -0.00257254  0.02096005\n",
      "  0.04111043  0.07783134 -0.00768949 -0.10654733]\n",
      "learn time: 31018.747465133667\n",
      "410000: expected = [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.], gained = [ 0.00166885  0.07640923  0.04455232  0.70195683 -0.05038334  0.10189475\n",
      "  0.00121986 -0.02190726  0.02320457 -0.0057927 ], error = [-0.00166885 -0.07640923 -0.04455232  0.29804317  0.05038334 -0.10189475\n",
      " -0.00121986  0.02190726 -0.02320457  0.0057927 ]\n",
      "learn time: 31094.45331144333\n",
      "411000: expected = [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], gained = [ 0.00620055 -0.02182225  0.08801681 -0.0239308   0.03631109 -0.07994332\n",
      " -0.01578713  1.13884862  0.01541797 -0.04754946], error = [-0.00620055  0.02182225 -0.08801681  0.0239308  -0.03631109  0.07994332\n",
      "  0.01578713 -0.13884862 -0.01541797  0.04754946]\n",
      "learn time: 31170.188720226288\n",
      "412000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.00620377 -0.02277271  0.02513103  0.05207699 -0.05616496  0.05448248\n",
      "  1.02292513 -0.0350053   0.01571373  0.02816135], error = [-0.00620377  0.02277271 -0.02513103 -0.05207699  0.05616496 -0.05448248\n",
      " -0.02292513  0.0350053  -0.01571373 -0.02816135]\n",
      "learn time: 31246.387670516968\n",
      "413000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.03053445  0.01282379  0.14210915  0.00461151 -0.12546988  0.95771484\n",
      "  0.05108275 -0.00304825  0.02215828 -0.01852958], error = [-0.03053445 -0.01282379 -0.14210915 -0.00461151  0.12546988  0.04228516\n",
      " -0.05108275  0.00304825 -0.02215828  0.01852958]\n",
      "learn time: 31322.481023073196\n",
      "414000: expected = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.], gained = [ 0.0039336  -0.01099619 -0.12629169  0.00832657  0.03038101  0.7349655\n",
      " -0.05944806 -0.0537499   0.25856069  0.11912727], error = [-0.0039336   0.01099619  0.12629169 -0.00832657 -0.03038101  0.2650345\n",
      "  0.05944806  0.0537499  -0.25856069 -0.11912727]\n",
      "learn time: 31398.15443301201\n",
      "415000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 1.11248612e-04  9.55291312e-01 -1.42341637e-02  6.86761165e-02\n",
      "  5.15376723e-02 -5.36787642e-03  1.58616614e-02 -4.80081571e-02\n",
      " -5.11254871e-02  1.13441087e-03], error = [-0.00011125  0.04470869  0.01423416 -0.06867612 -0.05153767  0.00536788\n",
      " -0.01586166  0.04800816  0.05112549 -0.00113441]\n",
      "learn time: 31473.948148489\n",
      "416000: expected = [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 1.19328138e-04  1.01360103e+00 -5.62788163e-02  4.94342266e-02\n",
      "  4.28152058e-03 -3.53068465e-02  4.84769609e-02  5.43349113e-02\n",
      " -1.99903177e-03  8.26455950e-02], error = [-0.00011933 -0.01360103  0.05627882 -0.04943423 -0.00428152  0.03530685\n",
      " -0.04847696 -0.05433491  0.00199903 -0.08264559]\n",
      "learn time: 31549.87666296959\n",
      "417000: expected = [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.], gained = [ 9.77062220e-01 -3.26370796e-02 -3.28332395e-02  2.88122009e-02\n",
      " -7.54836258e-02  7.75500275e-02 -5.41980560e-02 -2.85530075e-03\n",
      "  3.53003651e-02 -4.00608084e-04], error = [ 0.02293778  0.03263708  0.03283324 -0.0288122   0.07548363 -0.07755003\n",
      "  0.05419806  0.0028553  -0.03530037  0.00040061]\n",
      "learn time: 31625.634103298187\n",
      "418000: expected = [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.], gained = [ 0.01455072 -0.03014874  0.94388433 -0.12858569  0.09019818 -0.04206876\n",
      " -0.01241525  0.01617894  0.31461302 -0.07350666], error = [-0.01455072  0.03014874  0.05611567  0.12858569 -0.09019818  0.04206876\n",
      "  0.01241525 -0.01617894 -0.31461302  0.07350666]\n",
      "learn time: 31701.331954717636\n",
      "419000: expected = [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.], gained = [ 0.0069506   0.04758658  0.03227352  0.0583277   0.00312323 -0.0694288\n",
      "  0.96946984  0.02740876  0.00223487 -0.05771795], error = [-0.0069506  -0.04758658 -0.03227352 -0.0583277  -0.00312323  0.0694288\n",
      "  0.03053016 -0.02740876 -0.00223487  0.05771795]\n",
      "learn time: 31777.144510507584\n",
      "\n",
      "final learn time: 31852.841015338898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AMats\\NeuralNetworks\\lab_4_convolution\\analysis\\classification_error_analyser.py:117: RuntimeWarning: invalid value encountered in divide\n",
      "  precisions = np.divide(self.__TP.reshape(total), self.__TP.reshape(total) + self.__FP.reshape(total))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learn\n",
      " border = 0.5\n",
      " recall = 0.8242845575471722\n",
      " precision = 0.949431976214154\n",
      " accuracy = 0.9783611904761905\n",
      " F-score = 0.8824432783238964\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 2 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiEAAAGwCAYAAAB/xbX8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAABboklEQVR4nO3dd3hUVf4G8PfOTCaTSkgh9N5CGiEoKrEiiAXpsewKa1nBAvYCFkBFBF0LKFJ22cWy+4MIigURsS+2FSUFSAi9k14nmXp+f0xmkiEhmUlm5k55P8+TBzO5k/nmmJx577nnnCsJIQSIiIiIPEwhdwFEREQUmBhCiIiISBYMIURERCQLhhAiIiKSBUMIERERyYIhhIiIiGTBEEJERESyYAghIiIiWTCEEBERkSxUchdA3u+qq67CyZMnbZ9LkoTIyEikp6fj2WefRbdu3QAA1dXVWLlyJbZt24aSkhJ07doV119/Pe6++26Ehobafc/Tp0/jrbfewvfff4+qqir07dsXf/nLXzBp0iRP/mhE5AFN+xBJkhASEoIhQ4bgvvvuw6WXXgoAuO222/Drr7+2+PwlS5agR48emDFjxnlfY/LkyXjppZdcXzy5lcRt26ktV111FWbOnInrrrsOAGA2m3HgwAEsWLAA3bt3xzvvvIOamhrccsstCAoKwkMPPYR+/frhwIEDePXVV6FSqfDuu+8iLCwMAHDkyBHceuutGDFiBO68807ExMTgp59+wosvvogHHngAd9xxh5w/LhG5WNM+xGw2o7KyEh999BH+9a9/4e9//zsuueQS3HbbbUhKSmrx7z8iIgIKhQKVlZW2xzIyMrBixQqkpaUBADQaDSIiIjz2M5FrcCSEHBIREYG4uDjb5/Hx8Zg7dy4ee+wxVFdXY/ny5dDr9diwYYNt1KNnz55IT0/HhAkT8Oabb+KJJ54AACxatAhDhw7FihUrIEkSAKB3797Q6/V49dVXMW3aNERGRnr+hyQit2nah8THx+Pxxx9HcXExlixZgk8++QQAEBoaatfPnOvcr3Xq1KnV48n7cU4ItZtarbb99+bNmzFjxoxml10iIiIwY8YMbN68GSaTCWfOnMFPP/2Ev/zlL7YAYjVt2jSsXbu22fcgIv900003Yf/+/Th69KjcpZBMOBJC7XLs2DGsWbMGl156KYqKilBTU4Pk5OQWj01PT0dFRQWOHTuGY8eOQQjR4rEhISEYOXKku0snIi8xYMAAAMCBAwdkroTkwhBCDlmwYAGef/55AIDRaERQUBDGjBmD+fPn49ChQwAsQ6MtsV5aqaioQFVVFQDw2i0R2fqB2tpaAMDq1auxbt26Zsf98ccfHq2LPIchhBwyd+5cjBs3DrW1tVixYgVOnjyJRx55BJ07d0ZUVBQAoLi4GH369Gn23KKiIgBAVFQUampqAABVVVWIjo72WP1E5H2s/UF4eDgA4Oabb8Ztt90mZ0nkYZwTQg6JiYlBnz59MGzYMLzxxhsAgHvvvRcGgwF9+vRBVFQU9uzZ0+Jz8/LyEBUVhV69eiExMRGSJCEvL6/ZcVqtFrfffjvy8/Pd+rMQkXcoKCgAAAwaNAiAZTS1T58+zT7IfzGEkNPUajVeeOEF7Nu3D//617+gUqkwZcoU/OMf/7ANq1rV1NTgn//8J6ZMmQKVSoXo6GiMHj0a69evx7mrwzdt2oTffvvNtu8IEfm3TZs2ITExEb169ZK7FJIJQwi1S0pKCqZNm4aVK1fi7NmzuP/++xEbG4vbbrsNO3fuxKlTp7Bz507MmDEDcXFxmDNnju258+bNQ05ODh544AHk5OTg8OHDWLduHV5++WU88sgj551bQkS+q7q6GsXFxSgqKkJBQQEWL16MrVu34sknn7Qdo9VqUVxc3OzDetmG/A83K6M2XXXVVbj//vsxZcoUu8fLysowfvx4XHbZZXjllVdQW1uLNWvWYOvWrTh79izi4+PPu2NqYWEhVqxYgV27dqG2thb9+/fH7bffjgkTJnjyRyMiDzh3x9To6GgMGzYMs2fPtq2Ia23H1GnTpmHx4sV2jw0ZMgTvvPMORo0a5d7iya0YQoiIiEgWvBxDREREsmAIISIiIlkwhBAREZEsGEKIiIhIFgwhREREJAuGECIiIpIFQwgRERHJgiGEiIiIZOH1d9EtLa1Ga9upSRIQExPR5nGBhG1ij+3RnKNtYj3OF7HvcA7bozm2iT139BteH0KEgEP/8x09LpCwTeyxPZrz5zZh39E+bI/m2Cb2XNkevBxDREREsmAIISIiIlkwhBAREZEsGEKIiIhIFgwhREREJAuGECIiIpIFQwgRERHJgiGEiIiIZNHuEKLX63HDDTfgl19+Oe8xe/fuxfTp05GamoqpU6ciLy+vvS9HREREfqZdIUSn0+Hhhx9GYWHheY/RarW4++67MXLkSGzevBlpaWmYNWsWtFptu4slIt/GkxciasrpEHLgwAFkZmbi2LFjrR63detWBAcH4/HHH8eAAQPw1FNPISwsDNu2bWt3sUTku3jyQkTncjqE/Prrrxg1ahQ2bNjQ6nHZ2dlIT0+HJEkAAEmSMGLECOzevdup15Oktj8cPS6QPtgmbA9XtYkr8OSFiFri9A3sbr31VoeOKy4uxsCBA+0ei4mJafUsqCWO3onPV+/06U5sE3s+1x5mM2DSt/BhAIy6xv82Nf1vPWA859hmX9cBYbHARfd5rE2sJy8PPfQQhg8fft7jWjt5mTJlilOv2VaA0uzbAPQYBKnTCKe+r79qGkzJgm1iz9H2cKa93HYX3bq6OqjVarvH1Go19Hq9U9+Ht+N2HtvEXovtIcyA2QCp4Y1ZMukBswEwGSCZdI1fMzd8zWSAZNY3HGt9vOG5ZuvXLW/4lq8bGp6nb3i8re9rsNVge54wubdhel6I0ogUl92SuzWePnmxPK+VuqvPAl89AoTHI+bR/U5/b3/mc2HdA9gm9lzZHm4LIcHBwc0Ch16vh0ajcer78Hbc7SdbmwjR+Ibe0pu42WD/7/ne1E36xsfNDW/itjd7vf2btu3Nu4WwYLY8v7OxSUAwG2RomI4RChWgUEMog2z/CqW6yWNBEMpgQBkEoQgClOqGrwdZvq4Mbng8COaIHgjvdSFEeb1X/d246uQFaP0ERlFThWgAqC1GaUkVBHiqy5OX5tgm9hxtD2dOXtwWQuLj41FSUmL3WElJCbp06eKulwwMQgBmYwtn2pZ/JeubeG0QgsoqLEPztrPv852ltxQIWjpL15//+abG15HMzr9heEJrE6CEpLS8eSuDG9+0Fee+iashFOqGN/mGrzX8N2yB4Jx/leo2A4Hl+Wr712zh+0Jy3bY+kgSEK4MA1Lvse7qCq05egNZDuFkV2nCQGcKog1A6//39FU/ommOb2HNle7gthKSmpmLt2rUQQkCSJAgh8Pvvv2P27Nnuekn/I8xQVh6BqjgPquJcy78leVDUlzv09E5uLs9RAlLjm3DTN+UW35zt3+gbj7MPCC2GhRZHANSQlGpExUShvNoAs9TCayrUgEIpdzMRPHfyIlQhtv+WjPUMIUQycWkIKS4uRkREBDQaDcaPH4+//e1vWLx4MW6++Wb83//9H+rq6nDttde68iX9h9kIZXlhk8CxxxI4DLVtPlU0PatuGJ5XqoNhFMpWhufPeYNXnO+svfFf2xl5QwhoKRy0dCYPhduyrkMkCUBsBEwqDql6O4+dvChUEJICkjBbJu4SkSxc+u6QkZGBJUuWYMqUKQgPD8fq1auxYMECbNy4EUOGDMGaNWsQGhrqypf0TcZ6qErzoSrJawwdpfmWOQ/nEMpgGGOHwRibBGNcEoxxyTBF9rK98UOhajYVWZKA2NgIVJTwTZe8nywnL5IEKIMBY12Lf3dE5BkdCiEFBQWtfp6SkoIPP/ywIy/h8yR9DZQlexFUnGsLHcqy/S2ufDAHhcMYlwhjXLIlcMQmwdR5oOwjCUTuJNfJi1AGQzLWQTIyhBDJhe9uLiTVl1suo9gCRy6UFYchoflwhFkT3RA2EmGMTYYhLgnmTn1cOvmQyBt5y8mLUAUDOvByDJGMGELaSVF7tvFSinWEo/pEi8eawro2jm7EJcMYmwRzeDfugEMkp4bJqJLRu1YIEQUShpC2CAFF9YnG1SnFuVCV7IFSW9Ti4abIPjDYAoflkooIjfVw0UTUFqG07EcimbxzSTlRIGAIaYHqzO8IPvhZ45JYXWWzY4SkgClqYOPoRlwijLGJEMHesjCWiFqlDLL864Mb1xH5C4aQcwQXfICIrx6xmzgqFEEwxgyFMbbJpNGYYUBQSCvfiYi8mVA0hBATQwiRXBhCmgjJ/gfC/7sAAKDrOxb6ftdYAkf0YECpbuPZRORTrJdjvHSHX6JAwBACAEIg9H+vIux/rwEAtKl3oXb0s1ypQuTHOBJCJD+GEGFG2A8LEJr7TwBA7ajHoE2fy5UrRP6uYU6IL97MkMhfBHYIMRkQ8fUj0OzfDACovuwF1Cf/Rd6aiMgjhDLY8h9cHUMkm8ANIcY6RH5xL4KPfAkhKVE95jXohkyRuyoi8hQFR0KI5BaQIUTSVSFy6+1Qn/oFQhmMqvGroe97tdxlEZEHCSXnhBDJLeBCiFRXik6f/BlBxbkwqyNQdf0/Yeh+kdxlEZGnKbg6hkhuARVCFNUn0enjW6GqOAhzSAwqJ7wPY1yS3GURkQw4EkIkv4AJIcryA+j08S1Q1pyGKbwHKif+B6ao/nKXRURysc0J4UgIkVwCIoSoinLQ6ZM/Q1FfBmPngaic8G+YI7rLXRYRycm6ASFHQohk4/chRFF1DJ0+yoTCUANDXAoqJ7wLERIjd1lEJDPrZmUSQwiRbPw+hGgKNjcGkEkbINQRcpdERN7AdgM7Xo4hkovf70uuPvIlAKA+6TYGECKyEdZ7x3AkhEg2fh1CFLVnEFSUDQDQ9RkjczVE5FVs947hSAiRXPw6hKiP7AAAGOLTIMK6yFwNEXkT2w3suGMqkWz8O4QctlyK0fcdJ3MlROR1eAM7Itn5bwgxaKE+8V8AgK7fWJmLISJvIxQN8/LNRnkLIQpgfhtC1Md/gGTSwRTRC6boIXKXQ0TehjewI5Kd/4aQI9sBNIyCSJLM1RCR17GOhHB1DJFs/DOEmE0IbpiUyvkgRNQS2xJdjoQQycYvQ4iqaDcUdaUwqyNh6D5K7nKIyBtxTgiR7PwyhARbV8X0vqJxV0QioiYal+gyhBDJxS9DiHWXVD1XxRDR+djuHcPNyojk4nchRFF5FKqyAghJCX3vK+Uuh4i8FS/HEMnO70JIcMMoiKH7KAhNlLzFEJHXEtysjEh2fhdCGndJ5aUYImqFbSSEIYRILv4VQox1CDr9CwBA1/dqmYshIm8mFJYlujDxcgyRXPwqhChrTkMyGyFUoTBH9ZO7HCLyZg0jIbwcQyQfvwohCm0RAMAUFi9zJUTk7bhEl0h+/hVCas8CAMwMIUTUFqV1JIRLdInkwhBCRIGJIyFEsvPPEBLKEEJErRO2u+gaASFkroYoMPlnCOFICBG1xbpEF+AyXSKZ+FcI0TKEEJFjrHfRBcBLMkQy8a8QUmtZHWMO6yJzJUTk9ZqMhHCZLpE8/CyEWEdCuspcCRF5PUWTO2xzJIRIFv4TQvS1UBhqAADmUI6EEFEbJAmQlJb/5J10iWThNyHENh8kKAxCHS5zNUTkE6zzQjgSQiQL/wkhXBlDRM7inXSJZMUQQkSByzo51cQQQiQHvwkhSttGZZwPQkQOUnLXVCI5+U0IkWzLc7kyhogc1DAnhJdjiOThNyFEUXsGAC/HEJETrJdjGEKIZOE3IUTJOSFE5CxOTCWSld+EEEnbcDmGc0KIyFG8ky6RrPwmhHB1DBE5zToSwtUxRLLwjxCiq4HCUAsAMIUyhBCRg2yrYxhCiOTgHyFEbwkgAhIQFCpzMUTkM3g5hkhW/hFCDFrLv6oQy/0giIgcwYmpRLLykxBSBwAQQSEyF0JEPsV6OYZzQohk4XQI0el0mD9/PkaOHImMjAysW7fuvMd++eWXuPbaa5GWloZbbrkFe/bs6VCx52VsCCFKjXu+PxH5JwVHQojk5HQIWbZsGfLy8rB+/XosWLAAb775JrZt29bsuMLCQjzyyCOYNWsWtmzZgoSEBMyaNQt1dXUuKdwOR0KIvJpXnrwAgNK6WRnnhBDJwakQotVqkZWVhaeeegqJiYkYO3Ys7rrrLrz//vvNjt25cycGDhyISZMmoXfv3nj44YdRXFyMAwcOuKx4G0M9AECoGEKIvJFXnrwA3LadSGZOhZD8/HwYjUakpaXZHktPT0d2djbMZrPdsVFRUThw4AB27doFs9mMzZs3Izw8HL1793ZN5U3ZJqbycgyRt/HakxegcXUM54QQyULlzMHFxcXo3Lkz1Gq17bHY2FjodDpUVFQgOjra9vh1112Hr7/+GrfeeiuUSiUUCgVWr16NTp06OVVgW4tdJAmNl2NUIVwcg8Y2Y1tYsD2ac7RNXNFm5zt5WbVqFcxmMxSKxnOhpicvaWlp7j15AWyXYyRejiGShVMhpK6uzi6AALB9rtfr7R4vLy9HcXExnn32WaSmpuI///kP5s2bhw8//BAxMTEOv2ZMTETbBx2xhBB1aARiYx04PkA41HYBhO3RnCfaRI6TF8DBgGUdCRGGgA+pDOvNsU3suePkxakQEhwc3CxsWD/XaOwvhbzyyisYPHgw/vSnPwEAnn/+eVx77bXYtGkT7r77bodfs7S0GkKc/+uSBMQ0jITozCpUl1Q7/L39lSRZ3lzaartAwfZoztE2sR7XEXKcvAAO1t0wJyQsWEIYT2AAMKy3hG1iz5Xt4VQIiY+PR3l5OYxGI1Qqy1OLi4uh0WgQGRlpd+yePXtw22232T5XKBQYOnQoTp065VSBQqDtN46GOSFmlYZvMk041HYBhO3RnCfaRI6TF8DBE5iGfUK0NbXQBvgJDMN6c2wTe+44eXEqhCQkJEClUmH37t0YOXIkAGDXrl1ITk62u64LAF26dMHBgwftHjt8+DCSk5OdeUnHNKyOAVfHEHkdOU5eAAcDlqJxiS7fZCwY1ptjm9hzZXs4tTomJCQEkyZNwsKFC5GTk4MdO3Zg3bp1mDFjBgBLx1JfbwkEmZmZ2LhxIz766CMcPXoUr7zyCk6dOoXJkye7pvKmGkZCBFfHEHmdpicvVs6evPTs2dM9xVmX6HJ1DJEsnBoJAYB58+Zh4cKFmDlzJsLDwzFnzhyMGzcOAJCRkYElS5ZgypQpuO6661BbW4vVq1fjzJkzSEhIwPr1652+ruuQJqtjiMi7ND15efHFF1FUVIR169ZhyZIlACwnLxEREdBoNMjMzMSTTz6JpKQkpKWlISsry30nL4AthMCsb/04InILp0NISEgIli5diqVLlzb7WkFBgd3n06dPx/Tp09tfnaOM3KyMyJt55ckL0HgDO46EEMnC6RDilXg5hsireeXJC9BkJIQhhEgOfnUXXU5MJSKn2EZCeDmGSA5+FUI4EkJETuFICJGs/CyEcCSEiJzA1TFEsmIIIaLA1XA5hqtjiOThHyHEaJ0TwssxROQEjoQQyco/QghHQoioPTgnhEhWfhJCrEt0GUKIyAlcHUMkKz8JIdbNyng5hoicwJEQIln5fggRwjYnRCiDZS6GiHwK54QQycoPQoi58b+tM92JiBzB1TFEsvKDEGJq/G/J938cIvIgjoQQycr337XNjSFESP5xKxwi8hBVwyVczgkhkoXPhxCp6UiIwud/HCLyJK6OIZKV779r212O4UgIETmBq2OIZOX7IcTcdCREKV8dROR7rHNCzEb7Se5E5BG+H0I4MZWI2qvpijqOhhB5nM+/a0tmIwBAKHgphoicZL0cA66QIZKDz4cQ2xCqxEsxROSkJiGEIyFEnuf7IaRhJITzQYjIaQolRMNlXK6QIfI83w8hDXNCBEdCiKg9FA3zQng5hsjjfD6ESNbVMZyUSkTtIGwrZDgSQuRpvv/ObV0dw4mpRNQeHAkhko0fhBBOTCWi9hPWXVM5MZXI43w/hNiW6DKEEFE7KBpWyHBiKpHH+XwIsd07hiMhRNQOHAkhko/PhxDbtu0cCSGi9uCcECLZ+H4I4RJdIuoIro4hko3PhxDrtu28HENE7SE4EkIkG58PIbbVMbwcQ0TtYb2JHeeEEHmc74cQjoQQUQfYNivj6hgij/P9ENIwEsK76BJRuyg4EkIkF58PIY1LdH3+RyEiGdiW6HJOCJHH+f47Ny/HEFFHWDcr4+oYIo/z/RDCialE1AEcCSGSj++HEOu27RwJIaL24JwQItn4fAiReBddIuoAro4hko/PhxDbtu2cmEpE7cGRECLZ+P47N29gR0Qd0DgnhCMhRJ7m+yGkYSREcGIqEbWHbXUMR0KIPM3nQ4hk+w+ptcOIiFrE1TFE8vH5EGJboguGECJqB+4TQiQb3w8hEA3/MoQQUTtwJIRINr4fQkRDCOHqGCJqB8HVMUSy8f13buvlGM4JIaL24D4hRLLx/RDCyzFE1AEcCSGSj/+EEF6OIaL24JwQItn4/ju3sPuHiMgp1m3buTqGyPP8IIRwTggRdYCCIyFEcvH5ECJxTggRdYB1szLOCSHyPJ8PIVyiS0QdouDqGCK5+ME7N3dMJaL240gIkXx8P4TYRkIYQoioHTgnhEg2vh9CuESXiDqAq2OI5OP779xcm0tEHcGRECLZOB1CdDod5s+fj5EjRyIjIwPr1q0777EFBQW45ZZbkJKSggkTJuDnn3/uULEt4xJdImo/zgkhko/TIWTZsmXIy8vD+vXrsWDBArz55pvYtm1bs+Oqq6txxx13YODAgfjkk08wduxY3H///SgtLXVJ4TbWOSF+MKhD5K+87+SlCa6OIZKNU+/cWq0WWVlZeOqpp5CYmIixY8firrvuwvvvv9/s2A8//BChoaFYuHAh+vTpg7lz56JPnz7Iy8tzWfEWlhAiOBJC5LW87uSlCY6EEMnHqRCSn58Po9GItLQ022Pp6enIzs6G2Wy2O/bXX3/FmDFjoFQqbY9t2rQJl19+eQdLPofgZmVE3sw7T16asI6ECDNgNrnvdYioGadCSHFxMTp37gy1Wm17LDY2FjqdDhUVFXbHHj9+HNHR0XjmmWcwevRoZGZmYteuXU4XKEltfDTMCZEUiraPDaAPh9ougD7YHo0fZ6rq8UV+EYwms8Nt1xFeefLSlHUkBOAKGSIPUzlzcF1dnV0AAWD7XK+3/+PVarVYs2YNZsyYgbVr1+Kzzz7DnXfeic8//xzdunVz+DVjYiJaPyDE8voajRqa2DaODTBttl2AYXsAe09VYcb7f6Bca8DA7lEY1T/G7a/Z1slLdHS07fHjx48jJSUFzzzzDL7++mv06NEDTzzxBNLT051+3bYClO3rqsYQojAbIKQQp1/LHzQN62TBNml0tlqHF77Yj4whXXBzSnyrxzrTXk6FkODg4GZhw/q5RqOxe1ypVCIhIQFz584FAAwbNgw7d+7Eli1bMHv2bIdfs7S0uvGKSwtCtfUIBVBfb0BNSbXD39efSZLlDbettgsUbA+L/LPVuC8rF5X1RiR2jUBqr6g228Tadh0hx8kL4HjdMbGNISgmKhgIC+ywyrDeXKC3yYlyLe7JysWxMi0kpQL3XTnQZd/bqRASHx+P8vJyGI1GqFSWpxYXF0Oj0SAyMtLu2Li4OPTv39/usb59++L06dNOFSgEWn/jaPiigBTQbzAtabPtAkwgt8feM9W4/4NcVOuMSO4WgRXTkqEJUqLGA20ix8kL0PYJjC2cltciWqGCZDairLgM5rpgp17HXzCsN8c2AU5W1GH2xhycrtKhZ5QGL09PdenJi1NzQhISEqBSqbB7927bY7t27UJycjIUCvtvNXz4cBQUFNg9dujQIfTo0cOZl2wbb2BH1Kq801W474McVOuMSOkeieVTkxEe7NT5R4c0PXmxcvfJC9AYOlv7sB5n3bBMmAwOPc9fPxxtt0D6COQ2OV5eh7s3WAJI784hWH1TKnpEhTjcbo5w6p07JCQEkyZNwsKFC5GTk4MdO3Zg3bp1mDFjBgBLx1JfXw8AuPnmm1FQUIAVK1bg6NGjeOONN3D8+HFMnDjRmZd0QMNPy4t2RM1kn6zE/R/kokZnQlqPSCyfmuTRAAJ46cnLOaxbt3PXVCKLo2VazNqQjbPVOvSNDsGqzBTER7h+lNDp4YN58+YhMTERM2fOxKJFizBnzhyMGzcOAJCRkYGtW7cCAHr06IG///3v+Oabb3DDDTfgm2++wZo1axAf3/qEFqfZIhdDCFFTu09UYu6mPNTqTUjv1QlvTE1GmNqzAQTw1pOXcyise4VwdQzRkVItZm/MQVGNHv1iQvF2Ziriwt1zmdLpHikkJARLly7F0qVLm33t3DOY9PR0bN68uf3VOcS6bTsvxxBZ/X6iAg9uzkOdwYwLekfh1UmJ0AQp236im8ybNw8LFy7EzJkzER4e3uzkZcmSJZgyZYrt5GXx4sVYs2YNBgwY4J6Tl3NYNyzjSAgFuoMltbg3KwdlWgMGxobhrenJiA5Vt/3EdvL8aZGrNYyECI6EEAEAfjtWgYc+zEO90YxRfaLwykR5AwjgjScv51BY76TLEEKBq7C4Bvdl5aK8zoDBcWF4a1oKokKD2n5iB/h+CLHNCZG3CiJv8MvRcjzy0R7ojGZc3Lczlt04TPYA4gsa54TwcgwFpoKiGtyXlYPKeiOGdgnHm9OS0SnEvQEE8IMQIlnnhPByDAW4n46U4bEte6EzmpHRPxovTRiGYBX/Lhyi4P1jKHDtO2tZwl9Vb8SwrhFYMTUJkRr3BxDAD0IIJ6YSATsPleHxj/dAbxK4bEAMltyQADUDiMM4J4QC1Z7TVbh/k2UFXXI3z6+g8/0QAoYQCmzfHyzFk5/shcEkcMXAGLx4QwKClAwgTlFa54TwcgwFjpxTVZi7KRe1ehOG94jE61OSPL6Czn9CCC/HUAD6trAE8z7dB6NZYMzgWLxw3VCoGECcJhQcCaHAsvtEJR7YnAetwYQRPTvhtclJCFV7fv6Y74cQYV2iy5EQCixf7y/G/M/yYTILjBsSh0XXDYVKwb+DdlFynxAKHLuOW1bQ1RnMGNmwhD9EpgnsfhBCeDmGAs+OgmI8/dk+mAQwPqELFowfwgDSAULBHVMpMPzvWDke+tCygu6iPp3x8kR5V9D5fgjhEl0KMF/sK8KCz/NhEsD1w7rgmWuGQMkA0jFKro4h//fzkTI82rCC7pJ+nbHsxkTZV9D5QQixYidM/m/r3rNYtK0AZgFMSIzHU+MGM4C4QOOcEF6OIf+083AZHt9iWUF3acMSfm9YQef7IcS6YyonppKf+3TPGTy3bT8EgInJXTF/7CAoOBfKNayrY3g5hvyQN6+g8/0QQhQAtuSexuLthRAApqZ2w+NjBjKAuJBtJISXY8jPfFNYgvlevILOD0KIaPsQIh+2Oec0lnxZCADIHN4dj141ABIDiGtZ54Twcgz5ka/2F+MpL19B5wchxMq7GpbIFbJ2n8Kyrw4AAG4e0QMPX9GfAcQNbKtjOBJCfmJ7fhGe3Zrv9Svo/CiEEPmXDb+fxCvfHAQA/Cm9Jx64vB8DiLvYRkIYQsj3NZ3AfkNiPJ724gnsvh9CBC/HkP/5964TeO3bQwCAGRf0wv2X9mUAcaPGfULqZa6EqGN8bQK774cQKy9uZCJnvPu/41j+/WEAwB2jemH2aAYQdxNBYQAAyaCVuRKi9vPFCex+EEI4EkL+41+/HMNb/z0CALjrot64+5I+DCAeINTWEFIjcyVE7bMp+xRe2mGZP+ZLE9j9IIRYeX9jE7XmHz8fxaqdRwEAd1/SB3+9uI/MFQUOjoSQL9v4x0m8/LVl/tgtI3rgIR+awO5HIYTINwkhsPano1j70zEAwL0ZfXH7qN4yVxVYGkNIrcyVEDmn6fyx20b2xJzLfGsCu8+HEIkTU8mHCSGw6sejWPezJYDMubQfZlzYS+aqAg9DCPmipvPHbh/VC/f44Pwxnw8hNj7W8ERCCLz13yNY/+txAMCDl/fHn0b2lLmqwCSCQgHwcgz5jn/+cgwr/WD+mP+EECIfIoTA8u8P473fTgAAHrlyAG4e0UPmqgKXCAoHAEh6Tkwl77f2p6NY86Nl/tisS/rgLh+eP+YHIYSXY8i3CCHw+neH8O9dJwEAj101EJlp3WWuKrDxcgz5AiEEVv94FP/42X/mj/lBCLHyvWEoCjxCCPztm4PY8McpAMC8qwdiSioDiNxsS3RNOsBsBBR+1DWSXxBCYOV/j+BfDZdv517WD7dd4Pvzx/iXRuQhZiGw7KsD2JR9GhKA+WMHYVJKN7nLIjTOCQEs80JEcKSM1RDZO/fy7UNX9Met6f4xf8x77ufbbrwcQ97PLARe2lFoCyBPXzOYAcSbKIMhFJb7x3DDMvImQgi89u0hWwB57KqBfhNAAL8aCeHlGPJOZiGwePt+fJx3FgoJWDB+CK4bFi93WXQOERQKSVfJFTLkNYQQeOXrg9i4238v3/p+COE+IeTFTGaB57fvx2d7LAFk0bVDMT6hi9xlUQtEUBigq+TkVPIKZiGwdMcBbM6xjJ4+NW4QJib73+ip74cQKw6EkJcxmgUWbSvAtn1FUErAc9cNxbihDCDeiitkyFuYhcCL2wuxJe8MJADPjh+MGxK7yl2WW/hPCCHyIkazwIKt+dheUAylQsKL1w/FVYPj5C6LWsENy8gbnDt6uvDaIbg2wX8v3zKEELmY0WTGM1vzsWN/CVQKCUtuSMAVg2LlLovawA3LSG6BOHrqRyGE12NIfgaTGU99lo9vCksQpJTw0oRhuGxAjNxlkQN4OYbkFKijp34UQojkZTCZMe+TffjuYCmClBJevjERo/tHy10WOYiXY0guxoaTl68LA2/01PdDCFfHkBfQG8144pO9+O+hMqiVEl6emIhL+jGA+BKOhJAcDCYz5n+6D98esJy8LJ0wDJcG0Oip74cQKx+8eyD5B53RjMc/3oMfD5cjWKXA3yYmYlTfznKXRU4S6oY5IdysjDxEbzTjyU/24oeGk5dlExMxOsBOXvwghHAkhORTbzDhsS178fNRSwB5bXIiLujNAOKLeDmGPOnck5dXJg7DRX0DK4AAfhFCrDgSQp5VbzDh4Y/24H/HKhASpMBrk5OQ3itK7rKonXg5hjyl3mDCo1v24JejFQF/8uJHIYTIc+oMJjz0YR52Ha9EaJASr09JQlrPTnKXRR3AEEKeUNdw8vIbT14A+EMI4cRU8jCt3oQHN+fij5NVCFMr8caUJKT2YADxdbYQomcIIffQ6k148MM8/HHCcvLyxpQkDA/wkxffDyE2vBxD7lerN+KBTXnIPmUJIG9OS0ZSN9723R9wJITc6dy+Y/nUZKR0Z9/hRyGEyL1qdEbM3ZSH3NNViAhWYcW0ZCR2jZC7LHIRhhByF0vfkYvc09WWvmNqEhJ58gLAL0IIL8eQ+1XXGzFnUy72nKlGpEaFN6clIyGeAcSfcHUMuUNVvQFzNuVhb0Pf8da0ZAxl32HjByGkAfcJITeprDNgzqZc7Dtbg04aFd6anoIhXcLlLotcjCMh5GqVdQbc/0Eu8ovYd5yP/4QQIjeoaOhECopqEBUShJXTkzEojp2IP2rcrIwhhDquQmvAvR/koLC4Fp1DgrByegoGxoXJXZbX8YMQwssx5B7lWj3u+yAXhcW1iA4NwlvTUzAwlp2Iv7JdjjHpAJMBUAbJXBH5qjKtHvdl5eJAiaXveDszBf1j2He0RCF3Aa4iuDqGXKhMq8c9WZazmJgwNVZlpjKA+Dnr5RgAkIycF0LtU1Krx+yNOThQUovYMDVWZ6YygLTC90MI9wkhFytt6EQOlmgRG6bGqswU9IsJlbsscjelGkJhGf3gJRlqj+IaHWZvyMbhUi26hKux+qZU9GXf0SrfDyFWnJhKLlBSo8Psjed0ItHsRAIFNyyj9jpbrcPsjTk4Wl6H+IhgrL4pFb07h8hdltfznxBC1EFF1TrM2piDI2XsRAIVV8hQe5ypqsesDdk4Vl6H7pHBWH1TCnpGse9wBCemEsHSidyTlYMTFfXoFhmMtzNT0KMTO5FAwxBCzjpVWY97NmbjVJUOPTppsCozBV0jNXKX5TP8IIRY8XIMtc/pqnrM3piDU5X16N5Jg7enp6B7J3YigYgblpEzTlTU4Z6NOThTrUOvKA3ezkxFfESw3GX5FD8KIUTOO1lp6UROV+nQM8oSQHgWE7i4Vwg56lh5He7ZmI2iGj36dA7B25kpiAtnAHGW74cQro6hdmp6FtO7cwhWTk/hWUyAa7wcUyNzJeTNjpRqcU9WDkpq9egXE4qV01MQG6aWuyyf5PshxIpXY8gJPIuhlvByDLXlUGkt7tmYgzKtAQNiLQEkOpQBpL18PoQwe5CzjpRpcW9WDopr9OgXHYqVmTyLIQtOTKXWHCiuxb1ZOSivM2BQXBhWTktBVCh31u0Ip5fo6nQ6zJ8/HyNHjkRGRgbWrVvX5nNOnDiBtLQ0/PLLL+0q0jGMI9S2I6VazN5oCSADYkOx6iYGEE/w3n7DHkMInc/+ohrM3piN8joDhnYJx9vTGUBcwemRkGXLliEvLw/r16/HqVOn8MQTT6B79+4YP378eZ+zcOFCaLUc3iR5HSxpHEYdFBeGt6YlozOHUT3CV/oNblZGLck/W437snJRWW/EsK4RWDE1CZEaBhBXcCqEaLVaZGVlYe3atUhMTERiYiIKCwvx/vvvn7cz+fjjj1Fb684/aE5Mpbbln6nC7A2WYdTBcWF4i8OoHuOd/UbLOBJC58o+XoF7NuaiWmdEcrcILJ+ajPBgn5/J4DWcuhyTn58Po9GItLQ022Pp6enIzs6G2Wxudnx5eTlefvllPPfccx2vtE28HEMt219Ug1vW/GwbRl3JYVSP8u5+wx5DCDWVe6oKf/77L6jWGZHaPZIBxA2cas3i4mJ07twZanXjEHZsbCx0Oh0qKioQHR1td/xLL72EyZMnY9CgQe0u0NFbwkgSbx9jZW0Htof9MGpi1wismMZhVMDx3xFX/A7J0W8Ajv9sdsepG1bHGLUB9/fDfsPe7pOVeGBTHmr1Jozo2QmvT0lCqFopd1mycke/4VQIqaurs+tIANg+1+v1do//+OOP2LVrFz799FNnXqKZmJiI1g9o+KUIC9MgLLaNYwNMm23n53JOVODerFxU1RuR1jsK6++4kAHkHJ74HZGj3wAc/9nsjiuJs9Qn6hEboP1JoPcbAPDzoVLM3ZQHrd6ESwbE4O8zRyJUzREQK1f+jjjVqsHBwc06DevnGk3jLpP19fV49tlnsWDBArvH26O0tLrV/cgidEYEA6it1aGupLpDr+UvJMnyS9JW2/mzvNNVuP+DXNToTEjtHol37rgQ+tp6lNTUy12aV3D0d8R6XEfI0W8AbfcdLbVBUL0CnQAYtVWoCLD+hP2Gxf+OlePBzXugM5oxqk8U/jHzAmirtdAGcJtYuaPfcCqExMfHo7y8HEajESqV5anFxcXQaDSIjIy0HZeTk4Pjx49j7ty5ds//61//ikmTJjl1rVcIxzZFFeDmqedytO38Tc6pKszdlItavQlpPSLx+tQkRGiCUFJTH5Dt0RpP/I7I0W8ATvQdTY4zq6ybldUG7O9KoPYbAPDLkXI8ssUSQC7u2xmvTEpEiFqJ2gBuk5a48nfEqRCSkJAAlUqF3bt3Y+TIkQCAXbt2ITk5GQpF4xzXlJQUbN++3e6548aNwwsvvIDRo0e7oGyilu0+UYkHNudBazAhvVcnvDaZ13Hl5kv9BiemBq6dh8vw+JY90JsEMvpHY+mEYQhWOb2VFjnJqRASEhKCSZMmYeHChXjxxRdRVFSEdevWYcmSJQAsZzcRERHQaDTo06dPs+fHx8cjJibGNZUTneP3ExV4cHMe6gxmjOwdhdcmJUITxAAiN1/qN0SQ9QZ23NcokPxwsBRPfLIXBpPA5QNisGRCAoKUDCCe4HQrz5s3D4mJiZg5cyYWLVqEOXPmYNy4cQCAjIwMbN261eVFErXlt2MVeGCTJYCM6sMA4m18pd+w3TvGpANMBpmrIU/4trAEj39sCSBXDYrFSwwgHuX0dN+QkBAsXboUS5cubfa1goKC8z6vta8RdcQvR8vxyEeN13GX3TiMAcTL+Eq/YQ0hQMO8EGWUR1+fPOvr/cWY/1k+TGaBsUPi8Ny1Q6BiAPEotjb5tJ+OlNkCSEb/aLw8kSMg1AFKNYTCsnyYl2T82/b8Isz/dB9MZoHxCV3w3HVDGUBkwBYnn7XzUBkebQgglw2I4UQycgmh5uRUf/f5vrN4Zms+TAK4PjEeC8cPgUrBXdrk4Ac9NtdNBaLvD5bisY8tM9mvGBiDlyYkQM0AQi7QuEKmRuZKyB0+3XMGC7YWwCyAiUld8ew1g6FkAJGNH20Bx1+iQPFtYQnmfboPRrPAmMGxeIHDqORCjSGEl2P8zce5Z/DC9v0QAKakdMMTVw+EgvvUy8qPQggFgqYTycYNicOi64ZyGJVcyrZChpdj/Mrm7FNYsuMAAGD68O547KoBkBhAZMcQQj5jR0Exnv5sH0wCGJ/QBQt4HZfcgBuW+Z+Nf5zCy19bAsjNI3rg4Sv6M4B4CYYQ8glf7CvCgs8bJpIN64JnrhnC67jkFgwh/uU/v5/Eq98cBAD8eWRPzL2sHwOIF2EIIa+3de9ZLNpmmUg2ITEeT43jRDJyH1sI0TOE+Lp3/3ccy78/DAD4y4W9cG9GXwYQL8MQQl7t0z1n8Nw2y0SyicldMX/sIE4kI7fiSIh/+Ncvx/DWf48AAO68qDdmXdKHAcQLMYSQ19qSexqLtxdCAJia2g2Pj+FMdnI/Tkz1fX//6ShW/3gUAHD3JX3w14ub35OIvIPvhxDeX9kvbc45jSVfFgIAMod3x6OcyU4eItS8iZ2vEkJgzY9H8fefjwEA7s3oi9tH9Za5KmqN74cQK75B+Y2s3aew7CvOZCd5cLMy3ySEwNs7j+CfvxwHAMy9rB9uu6CXzFVRW/wnhJBf2PD7SbzSMJP9T+k98cDlnMlOntV4OYYjIb5CCIEV3x/Gu7+dAAA8dEV/3JreU+aqyBEMIeQ1/r3rBF779hAAYMYFvXD/pZzJTp7Hiam+RQiB1787hH/vOgkAeOyqAchM6yFzVeQohhDyCk2X0t0xqhdmj2YAIXmIIOucEIYQbyeEwN++OYgNf5wCADx59UBMTe0uc1XkDIYQkl3TpXR3XdQbd3MpHcmIl2N8g1kILPvqADZln4YEYP7YQZiU0k3usshJDCEkq3/8fBSrdnIpHXmPxs3KODHVW5mFwItfFmJL7hlIAJ65ZjAmJHWVuyxqB4YQkoUQAmt/Ooq1P3EpHXkXzgnxbiazwAvb9+PTPWehkIAF44fgumHxcpdF7cQQQh4nhMCqH49iXcNa/jmX9sOMC7mUjrxDYwjh5RhvYzQLPLetAJ/vK4JSAp67bijGDe0id1nUAX4QQrhZmS8RQuCt/x7B+l8ta/kfvLw//jSSS+nIewh1Qwgx6wGTHlCqZa6IAEsAWbA1H9sLiqFUSFh8/VCMGRwnd1nUQX4QQqw4kdHbCSGw/PvDeK9hLf8jVw7AzSO4lI68i1CF2v5bMtRCMITIzmgy4+mt+fhqfwlUCgkv3pCAKwfFyl0WuYBC7gIoMFjX8lsDyGNXDWQAIe+kDIJQBgPgJRlvYDCZMe/TffhqfwmClBKW3jiMAcSP+NFICHmrc9fyz7t6IKZwLT95MREUCsmk4+RUmemNZjz5yV78cKgMaqWEZTcmYnT/aLnLIhfiSAi5lVkILP3qADb8cQoSgKfGDmIAIa/HDcvkpzOa8fjHlgASrFLgb5MYQPwRR0LIbcxC4KUdhfgwx7KW/+lrBuNGruUnH8ANy+RVbzDhsS178fPRcgSrFHh1UiIu7NNZ7rLIDRhCyC3MQmDx9v34OI9r+cn3cMMy+dQbTHj4oz3437EKhAQp8NrkJKT3ipK7LHIThhByOZNZ4Pnt+/FZw2ZCi64divEJXMtPvoMblslDqzfhoQ/z8PuJSoQGKfHGlCQM79lJ7rLIjRhCyKWMZoFF2wqwjZsJkQ+z7RXCyzEeU6s34sHNedh9sgphaiWWT01GSvdIucsiN/P9ECK4WZm3OHczoRevH4qruJkQ+SCOhHhWjc6IuZvykHu6CuHBSrw5NRmJ3RhAAoHvhxAr3nVVVkaTGc9szceOhs2EltyQgCu4lp98VGMI4ZwQd6uuN2LOplzsOVONSI0Kb05LRkJ8hNxlkYf4Twgh2RhMZjz1WT6+KbQEkKU3DsNlA2LkLouo3bg6xjMq6wyYsykX+87WoJNGhbemp2BIl3C5yyIPYgihDjGYzJj3yT58d7AUQUoJy24choz+DCDk27hPiPtVaA2474Mc7C+uRVRIEFZOT8agOAaQQMMQQu2mN5rxxCd78d+G3QxfnpiIS/pxMyHyfZwT4l5lWj3uy8rFgZJaRIcGYeX0FAyIDZO7LJIBQwi1i2U3wz348bBlM6G/TUzEqL7cTIj8g+1yjJ4hxNVKa/W4JysHh0u1iA1T4+3pKegbE9r2E8kvMYSQ087dzfC1yYm4oDcDCPkPjoS4R0mNDvdk5eBIWR26hKuxcnoK+kQzgAQyhhByCnczpEDAEOJ6Z6t1uDcrB8fK6xAfEYxVmSnoGRUid1kkM4YQclidwbKb4a7jlt0MX5+ShDTuZkh+iJuVudaZqnrM3piDk5X16BYZjLczU9CjEwMI+UUI4WZlnqDVm/Dgh3n440QlwtSW7ZRTezCAkH/iSIjrnKqsxz0bs3GqSocenTR4OzMF3SI1cpdFXsIPQogVNytzl1q9EQ9sykP2Kct2yiumJiOZ2ymTH+NmZa5xoqIO92zMwZlqHXpFafB2ZiriI4LlLou8iB+FEHKHptspRwSrsGJaMhK7cjdD8m/crKzjjpXX4Z6N2Siq0aN35xCsykxBXDgDCNljCKHz4nbKFKhsm5WZDYBJDyjVMlfkW46UaXHPxhyU1OrRLzoUKzNTEBvGNqTmGEKoRdxOmQKZdSQEsMwLEQwhDjtUWot7NuagTGvAgNhQrJyeguhQth+1TCF3AeR9KuoMuO8DSwCJCgnC25kMIBRgFCoIpeXSATcsc9yBklrM3mAJIIPiwvA2Awi1gSMhZKdcq8d9H+SisNiynfJb01MwkNspUwASQWGQTDqukHHQ/qIa3JuVg8p6I4Z0Cceb05IRFRIkd1nk5TgSQjZlWst2yoXFtYgJU2NVZioDCAUsoeZN7ByVf7baFkAS4sOxcjoDCDnGD0ZCuE+IKzS7n0NmCvpyO2UKYFwh45g9Z6ox54NcVOuMSOoWgeVTkhGh8YO3FvIIv/lNYRRpv3Pv5/B2Zip6d+ZuhhTYbHuF6CtlrsR75Z6qwpxNuajVm5DaPRKvT0lCeLDfvK2QB/ByTIArqtZh1kZLAImPCMbqmxhAiADAFDUAAKAq2SdzJd4p+2SlLYCk9eyE5VOTGUDIaQwhAexMVT1mbczGsfI6dIsMxuqbeEMpIitDl1QAQFDRbnkL8UK/n6iwBZCRvTrhjSlJCFUr5S6LfBBja4A63XBDqVOV9ejeSYO3p6egeyfez4HIyhg/HACgOrsbEAKQeGsIAPjtWAUe+jAP9UYzRvWJwisTE6EJYgCh9uFISAA6WVmHWRuycaqyHj2jNFidyQBCdC5jTAKEQg2FrgKKqqNyl+MVfjlajgcbAsjFfTszgFCHMYQEmBMVdZi9IQenq3QN93NIRVfe0ZKoOaUaxrhEAEDQ2d3y1uIFfjxchoc/zIPOaEZG/2i8zABCLsAQEkCOlVtGQM5U69Cn4YZSvKMl0fkZugwHAKgCfF7Ifw+V4tEte6A3CVw+IAZLJwxDsIpvH9Rx/C0KEEfKtJjdcEfLftGhWHVTKu9oSdQG67yQQB4J+e5ACR7bshcGk8CVg2KxZEIC1Awg5CK+/5vEDULadKRUi9kbc1Bco8eA2FCsuol3tCRyhDE+DQCgKs4FTAaZq/G8r/cX44lP9sFoFrh6cBxevH4ogpS+/7ZB3sPp3yadTof58+dj5MiRyMjIwLp168577LfffouJEyciLS0NEyZMwFdffdWhYlvFmestOlhSi1kbs1Faq+cNpUg2XttvtMHUqS/M6khIJh2UZftlq0MOXxYUY/6n+2AyC1wzNA7PXz8UKgYQcjGnf6OWLVuGvLw8rF+/HgsWLMCbb76Jbdu2NTsuPz8f999/P6ZOnYqPPvoIN998Mx544AHk5+e7pHBq24HixltqD44Lw8ppKejMAEIy8Nl+Q1LAaNsv5A95apDBtn1FePqzfTAJ4PphXbDo2qFQKXiiR67n1D4hWq0WWVlZWLt2LRITE5GYmIjCwkK8//77GD9+vN2xn376KS666CLMmDEDANCnTx98/fXX+PzzzzF06FDX/QTUor2nqjBrYzYq64wY2nBHy068oRTJwNf7DUP8cKhP/GDZLyTxz7LU4Emf7jmL57YVwCyAG5PiMX/sYCgZQMhNnAoh+fn5MBqNSEtLsz2Wnp6OVatWwWw2Q6FoHFiZPHkyDIbm11Crq6s7UC45Iv9sNe7flIfKOiOGdY3AiqlJiNQwgJA8fL3fMDaskAmEnVM3/u84Fn1eAAFgckpXPHn1ICh4qZvcyKkQUlxcjM6dO0OtbhzSj42NhU6nQ0VFBaKjo22PDxgwwO65hYWF+Omnn3DzzTc7VWBbv/9Sk3/5twLsPVON+7Isd7RM7haBFdN4Pwfr7wV/Pxo52iauaDM5+g3A8Z+treOMXYcDAJRl+yEZagF1mNO1+IIPc05j8fZCAMD04d3w2JiBAR9A2HfYc0e/4dS7U11dnV1HAsD2uV6vP+/zysrKMGfOHIwYMQJjxoxx5iURExPR+gFqy48QHq5BeGwbx/q5P46V476GW2qn9+mMf91+ASI4AmLT5u9SAPJEm8jRbwCO/2xtHhcbAUT2gFR1ErH6g0D30U7X4u3e/emILYDcProvnr1hGCS+89qw77DnyvZwKoQEBwc36zSsn2s0Le+6WVJSgttvvx1CCCxfvtxu6NURpaXVEK0sw43UG6EGUFNTj/qSwL3Uk3OqCnM+sN7RMhLr77gQupo6lNTUy12a7CTJ8kfT1u9SIHG0TazHdYQc/QbQdt/hzO9FRGwKgqtOonb/j6gLT3G6Fm/2f7+fxCtfHwQA/PXSfpg1qidKS2tkrso7sO+w545+w6kQEh8fj/LychiNRqhUlqcWFxdDo9EgMjKy2fFnz561TTB755137IZdHSUEWv1hRZN/A/WXZPeJSjywOQ9agwnpvTrh9clJCA9Wob46cNukJW39LgUiT7SJHP0G4PjP5shxhi6pCD70OZRnd/vV79B7v53AG98dAgD85cJemH9dAkpLa/zqZ3QF9h32XNkeTp1eJCQkQKVSYffu3bbHdu3aheTk5GZnKlqtFnfddRcUCgXee+89xMfHu6Tg5gL7N+P3ExWYuzkXWoMJI3tH4fXJSQjhLbXJi3hnv+Ec66ZlQUXZMlfiOut/PW4LIHdc1Bv3XdqXl2DI45wKISEhIZg0aRIWLlyInJwc7NixA+vWrbOdtRQXF6O+3jL8v3r1ahw7dgxLly61fa24uNiNs9wD74/nt2MVeGBTHuoMlltqvzaJN5Qi7+Pd/YZjjHHJEJCgrD4OSVsiay2u8I+fj+LNHw4DAO6+pA/uGc0AQvJw+kLrvHnzkJiYiJkzZ2LRokWYM2cOxo0bBwDIyMjA1q1bAQBffPEF6uvrMX36dGRkZNg+Fi9e7NqfIEDxltrkS3y93xDBkTB1HgjAt0dDhBBY8+MRrNp5FABwb0Zf/PXiPjJXRYFMEsK7r3SVlLQxMXXrHQg+vB3VVy5D/bBbPVeYjH46UobHtuy13VL7pXPuaClJQGxsRJttFyjYHs052ibW43yRoz+bo78XEV89BE1+FmpHPgjtqEddV6iHCCGwaucRrPvlOABgzqX9MOPCXrav8++kObaJPXf0G7wRgI/ZebgMj360BzqjGZfxltpEHmPw4U3LhBB484fDtgDy0BX97QIIkVwCexcrH/P9wVI8+YnlltpXDIzBizck8I6WRB5ijB8OAJbt24XwmR2shBB4/btD+PeukwCAR68cgJtG9JC5KiILvoP5iO8OlOCJjy0BZMzgWCxhACHyKGNMAoRCDYWuAoqqo3KX4xAhBP72zUFbAHlizEAGEPIqfBfzAV8XluCJT/bBaBYYNyQOL1yfwFtqE3maUg1j7DAAQNDZ3fLW4gCzEFj61QFs+OMUJADzxw7CtOHd5S6LyA7fybzcjoJizP9kL0xmgWuGxmHRdbylNpFcbJdkvHyFjFkILPmyEJuyT0MC8Mw1gzE5pZvcZRE1wxDixb7YV4SnP9sHkwCuG9YFi65lACGSk6EhhHjz5FSTWeCFL/bjo9wzUEjAwmuHYEJSV7nLImoRJ6Z6qa17z2LRtgKYBTAhMR5PjRsMJQMIkayMXSw7p6qKcwGTAVB61w0iTWaB574owNa9RVBKwKJrh+KahC5yl0V0XhwJ8UKf7jmDhZ9bAsjE5K54+hoGECJvYIrqB7M6EpKxHsqy/XKXY8doFljweb4tgLxwfQIDCHk9hhAvsyX3NJ7bth8CwNTUbpg/dhAUPrIUkMjvSQoYu6QCAIKK/pC5mEZGkxnPfLYPX+QXQ6mQ8OKEYbh6SJzcZRG1iSHEi2zOOY0XthdCAMgc3h1PjBnIAELkZawhROUlK2QMJjPmfboPO/aXQKWQsHTCMFw1KFbusogcwjkhXiJr9yks++oAAODmET3w8BX9eUMpIi/kTZNT9UYznvxkL344VAa1UsKyGxMxun+03GUROYwhxAts+P0kXvnmIADgT+k98cDl/RhAiLyUdZmusmw/YNACQaGy1KEzmvHEx3ux83AZglUKvDxxGC7uywBCvoWXY2T2710nbAFkxgW9GECIvJw5rCtMYV0hCTOCinNlqaHeYMKjW/bYAsirkxIZQMgnMYTI6N3/Hcdr3x4CANwxqhfuv7QvAwiRD7C7j4yH1RtMeOSjPfj5SDk0KgXemJKEC/t09ngdRK7AECKTf/1yDMu/PwwAuOui3pg9mgGEyFdY76ir8vC8EK3ehAc/zMOvxyoQGqTE8qnJSO8V5dEaiFyJc0Jk8I+fj2LVTssNsO6+pA/+enEfmSsiImcY4y2blnnyHjK1eiMe2pyHP05WIUytxBtTkpDao5PHXp/IHTgS4kFCCKz58YgtgNyb0ZcBhMgHGeOSAQDK6uOQtCVuf70anRFzN1kCSHiwEm9OS2YAIb/AEOIhQgis+vEo1v50DAAw59J+uH1Ub5mrIqL2EMGRMHYeCAAIcvPN7Krrjbj/g1zknKpCpEaFt6alIKlbpFtfk8hTGEI8QAiBt/57BOt+tgSQBy/vjxkX9pK5KiLqCKN1XshZ9+2cWllnwH0f5GDPmWp00qiwcloKhnWNcNvrEXma74cQIeSuoFVCCCz//jDW/3ocAPDIlQPwp5E9Za6KiDrK3ZuWVdQZcG9WDvadrUFUSBBWTk/BkPhwt7wWkVz8Z2KqF64sEULg9e8O4d+7TgIAHrtqIDLTustcFRG5QuNIyG7LyZAL+6ByrR73ZuXiQEktokOD8Nb0FAyMDXPZ9yfyFr4/EuKlhBD42zcHbQFk3tUMIET+xBibAKFQQ6GrgKLqmMu+b2mtHrM35uBASS1iwtRYlZnKAEJ+iyHEDcxCYOlXB7Dhj1OQADw1dhCmpDKAEPkVZTCMscMAuO6STEmNDrM3ZuNQqRZx4WqsykxBvxh5toUn8gSGEBczC4GXdhRiU/ZpSACevmYwJqV0k7ssInIDY7zr7qhbVK3DrI05OFJWh/iIYKzOTEXfaAYQ8m8MIS5kFgKLt+/HhzlnoJCAhdcOwY1JXeUui4jcxNClYdOyDo6EnKmqx6yN2ThWXodukcFYfVMKenUOcUGFRN7NfyamysxkFnh++358tucsFBKw6NqhGJ/QRe6yiMiNbPeQKc4FTAZAGeT09zhVWY97snJwqrIe3TtpsCozBd0iNS6ulMg7cSTEBYxmgYXbCvDZnrNQSsDz1zGAEAUCU1R/mNURkIz1UJbtd/r5JyrqMGtDNk5V1qNnlAarGUAowDCEdJDRLLBgaz627SuCUiHhxRsSMG4oAwhRQJAUMHaxzAsJKnJu07Lj5ZYAcqZah96dQ7A6MxVdGUAowDCEdIDRZMYzn+3D9oJiqBQSXrohAVcNjpO7LCLyILv9Qhx0pEyLWRuzUVSjR7/oUKzOTEGXiGD3FEjkxTgnpJ0MJjOe+iwf3xSWIEgp4aUJw3DZgBi5yyIiD2vcOdWxe8gcLtXinqwclNbq0T8mFCunpyAmTO3GCom8F0dC2sFgMmPeJ/tsAeTlGxMZQIgClPVyjLKsADBoWz32QEktZm/MRmmtHoPiwrAqkwGEAhtDiJP0RjMe/3gvvjtYCrVSwisTEzG6f7TcZRGRTMzh3WAKi4ckzAgqzj3vcYXFNbhnYw7KtAYM6RKOldNT0DmUAYQCG0OIE3RGMx77eA/+e6gMwSoFXp2UhEv6MYAQBbq25oUUnLUEkIo6AxLiw7FyejKiQpxfzkvkbxhCHFRvMOHRj/bgx8PlCFYp8NrkRIzq21nusojICxjiLZuWqVrYtGzvmWrck5WDynojkrpF4K1pKYjUMIAQAZyY6pB6gwkPf7QH/ztWgZAgBV6bnIT0XlFyl0VEXsI6EhJ0zkhI3ukqzNmUixqdCSndI/HGlCSEB7PbJbLiSEgb6gwmPPhhHv53rAKhQUq8MSWZAYSI7Bi7pAAAlNXHIWlLAADZJytx/weWAJLWIxLLpzKAEJ2LIaQVtXojHtiUi13HKxGmVmL51CSk9ewkd1lE5GVEcCSMnQcCsCzV/eNEJeZuykOt3oT0Xp3wxtRkhKkZQIjOxb+K86jRGfHg5jxkn6pCmFqJN6clI6lbpNxlEZGXMnZJhar8ACr3bMWDB4KgNQIX9o7C3yYlQhOklLs8Iq/EkZAW1OiMmLvJEkAiglV4a3oKAwgRtUrf4xIAQO8jG7BF8Rgeid+Nv904lAGEqBUMIeeorjfi/g9ykXu6CpEaFVZOT0Zi1wi5yyIiL/edZgzeME1HpQjFIMVJzKlchm5ZYxCcnwWYjXKXR+SVGEKaqKwz4L4PcrDnTDU6aVRYOT0FQ+MZQIiodTsPleHhLfvwmmEyHu/+HioveAzm4CioKg8j8quHEP3+5dDs/Tdg0stdKpFXYQhpUFFnwH0f5GLf2RpEhQTh7cwUDOkSLndZROTlvjtQike37IHBJHDloFgsmDgS+gsfQNmMn1Fz8XyYQ2KgrDqKiG8eR/R7l0KT9y5g0sldNpFX8PkQIjSdG/5t/86l5Vo97s3KQUFRDaJDLQFkUBwDCBG17uvCEjzxyV4YzQJXD47Fi9cPRZDS0q0KdTjqRtyL0tt+Qs3oZ2EK7QJlzUlEfDcP0e+OhiZnHWCsk/knIJKXz4eQ2kvmAze9B33fMe16fplWj3uyclBYXIuYMDVWZaZiYGyYi6skIn+zo6AY8z/ZC5NZ4JqhcXj++gSolC10qUGhqBt+N8pu24nqS5+DKawrlLVnEPHDs4h+dzRCdq9p88Z3RP7K50OICI0FEiYACudXG5fU6jF7Yw4OlmgRG6bGqswU9IsJdUOVRORPvthXhKc/2weTAK4b1gWLrh0KlUJq/UmqENSn3GEJI5cvgSm8B5TaIoTvfA4x716MkN9XAvpaz/wARF7C50NIexXX6DB7QzYOl2rRJVyN1Telom80AwgRtW7r3rN49vN8mAQwITEez14zBMq2AkhTymDUJ92Gsj//gOorX4YpsjcUdaUI/+lFxLx7EUJ/Ww5JV+W+H4DIiwRkCCmq1mH2xhwcLa9DfEQwVt+Uit6dQ+Qui4i83Md5Z7Dw8wKYBTApuSuevmawcwGkKaUa9cNuQdmt36FqzGswduoHRX05wn5Zhuh3L0bor69Cqq9waf1E3ibgQsiZqnrM2piNY+V16BYZjNU3paBnFAMIEbXuw5zTeP6L/RAApqZ2w7yxg6CQ2hlAmlIGQTd0Ospv/RZVY1fA2HkQFLpKhP3vVUsY+XkZpPryjr8OkRcKqBByuqoeszbm4ERFPbp30mD1Tano0YkBhIhal7X7FF78shAAcFNadzwxZqBrAkhTCiV0gyej/OYdqBr3NowxQ6HQVyNs13LErB+FsB8X226OR+QvAiaEnKysw6wN2ThVWY+eURqszkxBt0iN3GURkZf7v99PYtlXBwAAt6b3wCNXDoDk6gDSlEIJ3aAJKL9pOyqvXQtDbBIkoxahf7yNmHcvQth/n4Oi9qz7Xp/IgwIihJyoqMOsDTk4XaVD784hWJ2Ziq4MIETUhvd/O4G/fXMQADDjgp548PL+7g0gTUkK6Ptfi4rMz1F5/b9g6JIKyViP0Ow1iH73EoR9/wwUNac8UwuRm/h9CDlWbhkBOVutQ9/oEKzOTEGXiGC5yyIiL/fOr8fx+neHAAB3jOqF+y/t57kA0pQkQd/3alRM+xQVE96DoetISCYdQnP/ieh3MxD+3Xwoqk54vi4iF/DrEHKkTItZG7JRVKNHv5hQvJ2ZithwBhAiat26n49hxQ+HAQB/vbg3Zo/uK08AaUqSYOh9BSqmfIiKiRug734RJLMeIXnvIPr9DIR/8xgUlUflrZHISX4bQg6XajF7Yw5KavUYEBuKVZkpiA1Ty10WEXkxIQTW/ngUb+88AgCYPboP7r7ECwJIU5IEQ8/RqJz8ASomZUHfMwOS2YiQvf9B9PuXIeKrh6CsOCR3lUQO8csQcrCkFrM3ZqO0Vo9BcWFYNT0V0aEMIER0fkIIrPrxKNb8ZBlNuP/Sfrjzoj4yV9U6Q4+LUTnx/1A+5SPoe18BSZigyc9C539fgYjt90NZtl/uEola5XchpLC4BrM35qBMa8DguDCsnJ6CqNAgucsiIi8mhMCbPxzBup+PAQAevLw/Zl7YS+aqHGfsNhKVE95D+bRPoOs7FpIwQ1P4ETr/ZwwivrgHytJ9cpdI1CKnb7ii0+mwaNEibN++HRqNBnfccQfuuOOOFo/du3cvFixYgP3792PgwIFYtGgRkpKSOlz0+RQU1eC+rBxU1huREB+OFVOT0SmEAYRIbt7cbwgh8MZ3h/H+LsvkzkeuHICbR/Rw2+u5kzE+DVXX/xOq4jyE/vYGgg99Ds2BT6A58Al0/a6BMWYooFBBKIIs99s657/RKQJqrRFCUgGKIAiF5V8oVQ2PNTleaf36ud+v4TmSEvCmy1jklZwOIcuWLUNeXh7Wr1+PU6dO4YknnkD37t0xfvx4u+O0Wi3uvvtuTJgwAS+99BL+85//YNasWfjyyy8RGur6e7Tkn63GvVm5qKo3YljXCLw5NRkRGudvakdEruet/YYQAq9+cwj/+f0kAODxMQMxfXh3l7+OpxnjklB17VooS/ch9LflCD7wKYIPf4Hgw1+0+dxIF9ZhDSeWf5VNwkrrAcYu/Fi/R0MIsnvM7vsp7Z+raHqc6vyvaw1TkqrZ60pKFaBXACY9IKkAye8uHsjOqXdprVaLrKwsrF27FomJiUhMTERhYSHef//9Zp3J1q1bERwcjMcffxySJOGpp57C999/j23btmHKlCku/SFyTlTgno25qNYZkdwtAsunJiM8mAGEyBt4a79hFgLPbtljCyDzxg7ClJRuLn0NuZliElB9zdvQXvAwNAWbIBmqAbMJMBsgmY2A2QjJbABMRkjCALVSQK/TWb5mMjR+3WxsON4AmE1NHmvybwssXzdAQp2Hf3LXim34V0gKuyDTNPRYQ5B9+AlqDE7nHUlqDEEtB6zWQlzT0KVs4XWbhC7J8rm3jVY59U6dn58Po9GItLQ022Pp6elYtWoVzGYzFIrGlJidnY309HTbrHJJkjBixAjs3r3bqc6krfYpKLLMAanWGZHSPRLLpyYFfACxthlHQi3YHs052iauaDM5+g1Hal+24wCydp+GBODpawZjYnJXp76/LzHHDIL2kidbPUaSgJiYCFSXVkMIJ19ACEBYw42pIaw0hhSYzg0zxiZByNAk4Jzn66YmjwnL92sWgmyh6jzf02QARJMAZTI0ex27gCVMzdtImAGTDpJJ52QDea/zjxY1CVvKIAhJCSjVwAUzIfWZ1Or3dKbfcOrduri4GJ07d4Za3bjSJDY2FjqdDhUVFYiOjrY7duDAgXbPj4mJQWFhoTMviZiYiFa//uoPR1Bdb8SFfaOx7vYLAj6ANNVW2wUatkdznmgTOfoNy/PO/7NV1RssAUQCXpmWiqnpPZ3+/v6KfycNzGZbSIGpMeRY/tvQEISafK1JqGn5a42BrPFr5x5rauVrRvvXsR1z7uufr8a2Rqsc9IcSMSNuc1kzO/WOXVdXZ9eRALB9rtfrHTr23OPaUtpGKr8ltSuGdYvEFX06ob66DvXVTn17v2Q9o2mr7QIF26M5R9vEelxHyNFvAG33HUsmJKB/t04Y2EmNkhJ2HPw7ac6+TZQAmmx2KQFQNnz4AiEAYW5hlOjcUaRWRomEEZGJV7q033AqhAQHBzfrDKyfazQah44997i2CIFWf9hukRok949DSQn/cM7VVtsFGrZHc55oEzn6DaDtn23skDjExkaw7zgH/06a8482kSxzQJRKiHYGJ0kCEBYBUee6vxmnpvrGx8ejvLwcRqPR9lhxcTE0Gg0iIyObHVtSYn/b6ZKSEnTp0qUD5RKRr2G/QUTn41QISUhIgEqlwu7du22P7dq1C8nJyXaTywAgNTUVf/zxB0RDXBJC4Pfff0dqamrHqyYin8F+g4jOx6kQEhISgkmTJmHhwoXIycnBjh07sG7dOsyYMQOA5eymvr4eADB+/HhUVVVh8eLFOHDgABYvXoy6ujpce+21rv8piMhrsd8govNxeueVefPmITExETNnzsSiRYswZ84cjBs3DgCQkZGBrVu3AgDCw8OxevVq7Nq1C1OmTEF2djbWrFnjlg2HiMi7sd8gopZIQnj3dJu2Jo1JEji57BxsE3tsj+YcbRPrcb6IfYdz2B7NsU3suaPf4B60REREJAuGECIiIpIFQwgRERHJgiGEiIiIZMEQQkRERLJgCCEiIiJZMIQQERGRLBhCiIiISBZO3UVXDpLk2NfbOi6QsE3ssT2ac7RNfLnN2Hc4h+3RHNvEnjv6Da/fMZWIiIj8Ey/HEBERkSwYQoiIiEgWDCFEREQkC4YQIiIikgVDCBEREcmCIYSIiIhkwRBCREREsmAIISIiIlkwhBAREZEsGEKIiIhIFj4RQnQ6HebPn4+RI0ciIyMD69atO++xe/fuxfTp05GamoqpU6ciLy/Pg5V6jjNt8u2332LixIlIS0vDhAkT8NVXX3mwUs9wpj2sTpw4gbS0NPzyyy8eqNDznGmTgoIC3HLLLUhJScGECRPw888/e7BS92HfYY/9RnPsO+x5vN8QPuC5554TEyZMEHl5eWL79u0iLS1NfP75582Oq62tFaNHjxYvvfSSOHDggHj++efFJZdcImpra2Wo2r0cbZN9+/aJxMREsX79enHkyBHx3nvvicTERLFv3z4ZqnYfR9ujqTvvvFMMHjxY/Pzzzx6q0rMcbZOqqipxySWXiKefflocOXJEvPHGGyI9PV2UlJTIULVrse+wx36jOfYd9jzdb3h9CKmtrRXJycl2/7Pfeust8ec//7nZsVlZWeKqq64SZrNZCCGE2WwWY8eOFZs2bfJYvZ7gTJu8/PLL4s4777R77I477hCvvvqq2+v0FGfaw2rLli3i5ptv9tuOxJk2Wb9+vbj66quF0Wi0PTZlyhTx7bffeqRWd2HfYY/9RnPsO+zJ0W94/eWY/Px8GI1GpKWl2R5LT09HdnY2zGaz3bHZ2dlIT0+H1HAfYUmSMGLECOzevduTJbudM20yefJkPProo82+R3V1tdvr9BRn2gMAysvL8fLLL+O5557zZJke5Uyb/PrrrxgzZgyUSqXtsU2bNuHyyy/3WL3uwL7DHvuN5th32JOj3/D6EFJcXIzOnTtDrVbbHouNjYVOp0NFRUWzY7t06WL3WExMDM6cOeOJUj3GmTYZMGAAhg4davu8sLAQP/30Ey6++GJPlet2zrQHALz00kuYPHkyBg0a5MEqPcuZNjl+/Diio6PxzDPPYPTo0cjMzMSuXbs8XLHrse+wx36jOfYd9uToN7w+hNTV1dk1CADb53q93qFjzz3O1znTJk2VlZVhzpw5GDFiBMaMGePWGj3Jmfb48ccfsWvXLtx7770eq08OzrSJVqvFmjVrEBcXh7Vr1+KCCy7AnXfeidOnT3usXndg32GP/UZz7DvsydFveH0ICQ4ObvbDWz/XaDQOHXvucb7OmTaxKikpwcyZMyGEwPLly6FQeP3/eoc52h719fV49tlnsWDBAr/7nTiXM78jSqUSCQkJmDt3LoYNG4bHHnsMffv2xZYtWzxWrzuw77DHfqM59h325Og3VB0r2f3i4+NRXl4Oo9EIlcpSbnFxMTQaDSIjI5sdW1JSYvdYSUlJs2FWX+dMmwDA2bNnMWPGDADAO++8g+joaI/W626OtkdOTg6OHz+OuXPn2j3/r3/9KyZNmuRX13md+R2Ji4tD//797R7r27evz4+EsO+wx36jOfYd9uToN7w+1iYkJEClUtlNENu1axeSk5ObpfLU1FT88ccfEEIAAIQQ+P3335GamurJkt3OmTbRarW46667oFAo8N577yE+Pt7D1bqfo+2RkpKC7du346OPPrJ9AMALL7yABx54wMNVu5czvyPDhw9HQUGB3WOHDh1Cjx49PFGq27DvsMd+ozn2HfZk6Tfas4zH05555hlx/fXXi+zsbPHll1+KESNGiC+++EIIIURRUZGoq6sTQghRXV0tLrroIvH888+LwsJC8fzzz4vRo0f73Vp/IRxvk1dffVWkpKSI7OxsUVRUZPuoqqqSs3yXc7Q9zuWPy+ysHG2TEydOiOHDh4vly5eLI0eOiNdff10MHz5cnDlzRs7yXYJ9hz32G82x77Dn6X7DJ0KIVqsVjz/+uBg+fLjIyMgQ//znP21fGzx4sN1a/uzsbDFp0iSRnJwspk2bJvbs2SNDxe7naJtcc801YvDgwc0+nnjiCZkqdw9nfkea8teORAjn2uS3334TkydPFklJSWLixIni119/laFi12PfYY/9RnPsO+x5ut+QhGgYfyQiIiLyIK+fE0JERET+iSGEiIiIZMEQQkRERLJgCCEiIiJZMIQQERGRLBhCiIiISBYMIURERCQLhhAiIiKSBUMIERERyYIhhIiIiGTBEEJERESy+H+k7+RXq4QJnAAAAABJRU5ErkJggg==\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\"\"\"\n",
    "expected_gained_learn = nn.learn(learn, num_epochs=7, categorical=True, need_preparations=False)\n",
    "learn_analyser = ClassificationErrorAnalyser(expected_gained_learn, 10, border_step=0.1)\n",
    "show_plots_classification(learn_analyser, \"learn\")\n",
    "#\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test\n",
      " border = 0.5\n",
      " recall = 0.897772721459386\n",
      " precision = 0.9568712318616057\n",
      " accuracy = 0.9858699999999999\n",
      " F-score = 0.9263803851691729\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "expected_gained_test = nn.test(test, categorical=True, need_preparation=False)\n",
    "test_analyser = ClassificationErrorAnalyser(expected_gained_test, 10, border_step=0.1)\n",
    "show_plots_classification(test_analyser, \"test\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving weights...\n"
     ]
    }
   ],
   "source": [
    "nn.save_net(\"./classification_weights\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
